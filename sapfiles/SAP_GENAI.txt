<!-- missing-text -->

Service Guide | PUBLIC 2024-12-19

SAP AI Core

<!-- missing-text -->

Content

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

1 What Is SAP AI Core?

Learn more about the SAP AI Core service on SAP Business Technology Platform (SAP BTP).

SAP AI Core is a service in the SAP Business Technology Platform that is designed to handle the execution and operations of your AI assets in a standardized, scalable, and hyperscaler-agnostic way. It provides seamless integration with your SAP solutions. Any AI function can be easily realized using open-source frameworks. SAP AI Core supports full lifecycle management of AI scenarios. Access generative AI capabilities and prompt lifecycle management via the generative AI hub.

SAP AI Core allows you to make data driven decisions confidently and efficiently and is tailored to business problems. It handles large volumes of data and provides scalable machine learning capabilities to automate tasks such as triage services for customer feedback or tickets and classification tasks. SAP AI Core comes with preconfigured SAP solutions, can be configured for open-source machine learning frameworks, can be used with Argo Workflow and KServe, and can be embedded into other applications.

SAP AI Core allows you to experiment with and utilize natural language prompts with a variety of generative AI models in the generative AI hub.

 Tip

The English version of this guide is open for contributions and feedback using GitHub. This allows you to get in contact with responsible authors of SAP Help Portal pages and the development team to discuss documentation-related issues. To contribute to this guide, or to provide feedback, choose the corresponding option on SAP Help Portal:

 · Feedback Create issue : Provide feedback about a documentation page. This option opens an issue on GitHub.

 · Feedback Edit page : Contribute to a documentation page. This option opens a pull request on GitHub.

You need a GitHub account to use these options.

More information:

 · Contribution Guidelines

 · Introduction Video

 · Introduction Blog Post

Environment

This service runs in the following environments:

 · Cloud Foundry

 · Kyma

 · Kubernetes

SAP AI Core

What Is SAP AI Core?

Multitenancy

This service supports multitenancy. It can be used in tenant-aware applications.

Features

Execute pipelines

Execute pipelines as a batch job, for example, to preprocess or train your models, or perform batch inference.

Serve inference requests

Deploy а trained machine learning model as a Web service to serve inference requests of trained models with high performance.

Manage the AI scenario lifecycle

Manage your ML artifacts and workflows, such as model training, metrics tracking, data, models, and model deployments via a uniform API lifecycle.

Benefit from multitenancy support

Use this service in tenant-aware applications. Implement multi-tenant services to segregate your AI assets and executions to isolate your tenants within SAP AI Core.

Integrate your cloud infrastructure

Register your Docker registry, synchronize your AI content from your git repository, and register your object store for training data and trained models. Productize your AI content and expose it as a service to consumers in the SAP BTP marketplace.

Generative AI hub

Choose from a selection of generative AI models for prompt experimentation, and prompt lifecycle management.

Process Flow Between SAP AI Core and SAP AI Launchpad

<!-- missing-text -->

Related Information

AI API Overview [page 36] SAP AI Launchpad

2 What's New for SAP AI Core

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

Mod

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

Mod

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

2.1 2021 What's New for SAP AI Core (Archive)

2021

<!-- missing-text -->

<!-- missing-text -->

3 Concepts

In this section, we'll explore some of the concepts surrounding both the SAP AI Core and SAP AI Launchpad services.

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

You can see how these concepts interact from the following diagram:

<!-- missing-text -->

3.1 SAP AI Core Overview

SAP AI Core is the key to integrating artificial intelligence capabilities in your SAP solutions.

Specifically, SAP AI Core helps you to:

 · Seamlessly and easily embed AI capabilities into other applications

 · Leverage high-volumes of data from applications to create robust AI learning models

 · Execute AI training on accelerated hardware

 · Serve AI inference with low latency and high-throughput in a cost-efficient manner

 · Adhere to a compliant, explainable, and maintainable process

 · Manage all stages of the AI lifecycle using a comprehensive set of tools and services

 · Focus on the productization and operationalization of AI scenarios

Management and operations of your AI content (such as versioning, deployment, and monitoring) is unified across your SAP solutions. Authoring of your AI content is, however, open to different toolsets (such as JupyterLab).

The SAP AI Core service is intended to be used together with SAP AI Launchpad and the AI API . The key components are SAP AI Core, SAP AI Launchpad, and the AI API .

 · SAP AI Core provides an engine that lets you run AI workflows and model serving workloads.

 · SAP AI Launchpad manages a number of AI runtimes. It allows various user groups to access and manage their AI scenarios.

 · AI API provides a standard way of managing the AI scenario lifecycle on different runtimes, regardless of whether they are provided on SAP technology (such as SAP S/4HANA) or on partner technology (such as Amazon Web Services). When the AI API is deployed on runtimes other than SAP AI Core, the runtimes have to provide a runtime adapter.

Overview Architecture [page 33] shows these three main components in an overview architecture diagram.

<!-- missing-text -->

3.1.1  SAP AI Core Systems Overview

Your SAP AI Core system connects internal and external tools.

Users interact with the various repositories, systems, and objects when working with SAP AI Core; some of these objects are provided by SAP. In other cases, customers provide these components to enable enhanced control (authorizations) and continuous integration/continuous deployment (CI/CD).

Key Repositories, Systems, and Objects

<!-- missing-text -->

<!-- missing-text -->

Synchronization

 · Your GitOps implementation is integrated with SAP AI Core to enable CI/CD.

 · Templates are synchronized from your Git repository to a Kubernetes cluster.

 · Your training and serving templates are regularly synchronized (every few minutes).

 · SAP AI Core checks the template syntax, and if synchronization fails, then error messages are displayed.

<!-- missing-text -->

<!-- missing-text -->

Process Summary

 · An SAP BTP token is used in AI API for authentication of the API calls.

 · Users run templates via the AI API.

 · Training templates are executed using the Argo workflow. The training pipeline consumes data input artifacts and outputs a model artifact.

 · An execution is created using the training template and AI API configuration. The result is a training job.

 · Serving templates are deployed using KServing.

 · A deployment is created using the serving template and AI API configuration. The result is an inference server.

 · Artifacts (such as datasets, models) are copied to and from the hyperscaler storage.

 · Required images are pulled from the registered Docker repository.

 · Details about scenarios and executables are retrieved from the Kubernetes cluster by the AI API .

<!-- missing-text -->

3.1.2  AI API Overview

The AI API lets you manage your AI assets (such as training scripts, data, models, and model servers) across multiple runtimes.

Argo workflows and serving templates, as well as their execution and deployment, are managed using the SAP AI Core implementation of the AI API. In SAP AI Core, the Argo workflow and serving templates are mapped under the concept of Executable . For the mapping mechanism to work, the Argo workflows and serving templates require certain attributes in the metadata section of the YAML file. These attributes are shared by both template types.

SAP AI Core provides additional APIs that are runtime-specific. These are available in the AI Core API specification, which is an extension of the AI API specification.

Related Information

<!-- missing-text -->

AI Core API AI API

AI API Runtime Implementations

The AI API specification is a general specification for the lifecycle management of machine learning artifacts. SAP AI Core is one specific runtime implementation of the AI API specification. It is also possible to provide other runtime implementations of the AI API specification, independent of SAP AI Core. This section describes the necessary boundary conditions and implementation requirements.

The benefit of using AI API is that clients can integrate with all AI API-enabled runtime implementations. For example, SAP AI Launchpad can interact with custom runtime implementations as long as the same APIs are provided. Intelligent Scenario Lifecycle Management can also integrate with AI API-enabled runtimes. The AI API Client SDK (Python) can also be used (for more information, see SAP AI Core SDK).

AI API Specification

The AI API specification comprises the following parts:

 · A main specification

 · Extensions:

 · Analytics extension

 · Resource group extension

 · Dataset management extension

 · Metrics extension

 Recommendation

Implement at least the main specification, and then implement the extension specifications based on your use case.

AI API Runtime Capabilities Endpoint

Meta API is part of the AI API specification (endpoint /lm/meta ). The implementation must return a configuration response that specifies the capabilities of the AI API runtime implementation.

Meta API allows AI API clients to query the capabilities of an AI API implementation so that they can select which commands or user interfaces are available. For example, some AI API runtimes may offer executions but not deployments. They may also offer logs for executions and not for deployments. As an example, if a client of SAP AI Core such as SAP AI Launchpad queries the Meta API endpoint of SAP AI Core, the response will be for example:

```
json { "aiApi": { "capabilities": { "logs": { "deployments": true, "executions": true
```

```
}, "multitenant": true, "shareable": true, "staticDeployments": true, "timeToLiveDeployments": true, "userDeployments": true, "userExecutions": true "executionSchedules": true }, "limits": { "deployments": { "maxRunningCount": -1 }, "executions": { "maxRunningCount": -1 }, "minimumFrequencyHour": 1, "timeToLiveDeployments": { "minimum": "10m", "maximum": -1 } }, "version": "2.18.0" }, "extensions": { "analytics": { "version": "1.0.0" }, "metrics": { "capabilities": { "extendedResults": true }, "version": "1.0.0" }, "resourceGroups": { "version": "1.2.0" } }, "runtimeApiVersion": "2.21.0", "runtimeIdentifier": "aicore" }
```

SAP AI Launchpad and other clients can then react accordingly and hide the deployments on the user interface for this runtime implementation of AI API.

Capabilities include the following:

<!-- missing-text -->

<!-- missing-text -->

Limits include the following:

<!-- missing-text -->

In addition to the general AI API specification, there is also a number of extensions that cover additional use cases. These might not be implemented in all runtime engines.

The extensions are:

<!-- missing-text -->

Related Information

Serving Templates [page 268]

Workflow Templates [page 221] AI API Specification

Custom Runtime Capabilities Using the Meta API

Analytics Extension

Resource Groups Extension

Intelligent Scenario Lifecycle Management

3.1.3  Resource Groups

SAP AI Core tenants use resource groups to isolate related ML resources and workloads. Scenarios, executables, and Docker registry secrets are shared across all resource groups.

Resource groups represent a virtual collection of related resources within the scope of one SAP AI Core tenant. When your tenant is onboarded, a default resource group is created immediately. Further resource groups can be created, or deleted by your tenant administrator with the AI API. Tenants can map the resource groups based on the corresponding usage scenarios.

If your SAP AI Core tenant uses resource groups to isolate the scenario consumer tenant and the resource groups are subsequently deleted, the scenario consumers will be deprovisioned. SAP AI Core is not aware of the scenario consumer of the tenant. The standard XUSAA multitenancy model is followed.

3.1.3.1 Scope of Resources

Resources that are available for tenants and resource groups differ based on the available scope.

<!-- missing-text -->

Tenant-Level Resources

Tenant-level resources include:

 · Workflow templates

 · Serving templates

 · Docker registry (containing the Docker images)

 · User authentication and authorization (UAA)

User authentication and authorization is based on the SAP AI Core tenant. The tenant is the holder of the access token obtained using the SAP AI Core service key. The SAP AI Core tenant can set the resource group

in the request header at runtime, or during lifecycle management, using the AI API. If the resource group is not set, the default resource group is used.

Resource Group-Level Resources

Executables at tenant level are shared across all of the resource groups. At resource group level, the object store is registered by setting the resource group header.

SAP AI Core tenants must consider security aspects in the design of AI functions.

 Recommendation

Do not use the same object store bucket with the same AWS IAM user for multiple resource groups.

Runtime entities such as executions, deployments, configurations, and artifacts belong to specific resource groups and cannot be shared across resource groups.

Examples of Resource Group Mapping

<!-- missing-text -->

3.2 Generative AI Hub in SAP AI Core Overview

The generative AI hub incorporates generative AI into your AI activities in SAP AI Core and SAP AI Launchpad.

LLMs are self-supervised, deep learning models that have been trained on vast amounts of unlabeled data. They leverage AI technology and industrial-scale computational resources to learn complex language patterns and semantic knowledge bases for natural language processing (NLP) tasks. They parse input, such as

prompts, and by predicting a target word, can return contextually relevant responses written in natural language. A single LLM can perform multiple NLP tasks by using different input formats and output modes.

LLMs are general models but can be fine-tuned with additional embeddings for specialized or domain-specific use cases.

SAP AI Core and the generative AI hub help you to integrate LLMs and AI into new business processes in a cost-efficient manner.

<!-- missing-text -->

4 Service Plans

The SAP AI Core service plan you choose determines pricing, conditions of use, resources, available services, and hosts.

Your use case will determine whether you choose a free tier option for SAP AI Core or a paid service plan.

The following service plans are available:

 · Free

 · Standard

 · Extended

<!-- missing-text -->

 Restriction

SAP AI Core free tier is only available in SAP BTP free tier, not SAP BTP trial.

<!-- missing-text -->

<!-- missing-text -->

Deployment Quotas

Each tenant is assigned a default quota that limits the number of deployments and replicas per deployment. If you reach this quota, your deployment will not be created and you will be notified accordingly. You can free up your quota by deleting existing deployments.

Alternatively, you can request an increase to your quota by creating a ticket on component CA-ML-AIC . Enter the description Request to Increase Quota and include details about the size of your increase, whether you want to include deployments, replicas, or both, and your subaccount ID.

Resource Group Quotas

<!-- missing-text -->

Restriction

The maximum number of resource groups is limited at tenant level to 50. If you reach this limit, you will receive an error message. To free up space, delete some resource groups. Alternatively, raise a ticket to increase your quota.

For more information, see Delete a Resource Group [page 86].

Related Information

<!-- missing-text -->

Free Tier [page 45] SAP Discovery Center SAP BTP Service Description Guide Choose a Resource Plan [page 219]

4.1 Free Tier

Enable the free tier option to get to know SAP AI Core with usage limits, to familiarize yourself with the service. Note that usage is limited with this option.

You can use the free tier option and benefit from the following advantages:

 · Explore SAP AI Core features and capabilities without cost.

 · Migrate from the free tier to a standard service plan while retaining your work.

Free Tier Scope

<!-- missing-text -->

 · For region information, see SAP Discovery center

 · Only community support is available for free tier service plans and these are not subject to SLAs.

 · The free tier option is subject to usage limits. For more information, see Service Plans [page 43].

 Note

Customers can create one SAP AI Core free plan instance, if and only if there are no active standard plan instances within the subaccount. Tenants can update their plan from free to standard, but not from standard to free. A standard plan SAP AI Core instance cannot be created if there is an active free plan instance in the subaccount.

Related Information

Service Plans [page 43]

Enabling Free Tier

<!-- missing-text -->

Enable free tier to experience SAP AI Core for trial and testing purposes. Alternatively, follow the tutorial here.

Prerequisites

 Restriction

 · SAP AI Core free tier is only available in SAP BTP free tier, not SAP BTP trial.

 · Generative AI hub is not available in free tier. For more information, see Service Plans [page 43].

Procedure

 1. Open your global account in the SAP BTP cockpit.

 2. Go to your subaccount.

 3. In the navigation area, choose Instances and Subscriptions .

 4. Choose New Instance or Subscription

 5. Search for SAP AI Core in the Service field.

 6. Choose Free in the Plan field.

<!-- missing-text -->

Related Information

Service Plans [page 43] Using Free Service Plans Getting a Global Account

4.2 Metering and Pricing for SAP AI Core

SAP AI Core is metered based on various nonbillable units of measure (UoM), depending on which resources of SAP AI Core are consumed.

The following resources and nonbillable UoMs are available:

<!-- missing-text -->

 Note

Infrastructure specifications depend on the chosen hyperscaler. For more information, see Choose a Resource Plan [page 219].

Each metered record is translated to the billable metric 'capacity units'. Every resource has a specific conversion rate for translating the UoM to capacity units. For the applicable conversion rates, see the SAP BTP Service Description Guide .

All 'Compute' and 'Storage' resource consumption is based on the actual metered usage.

Baseline charges are automatically calculated based on the following: For each active tenant hour where one of the 'Compute' or 'Storage' resources is being consumed within a tenant, one 'Baseline' tenant hour is

automatically charged. If multiple resources are used within the same hour, only one 'Baseline' tenant hour will be charged.

 Example

This example uses fictitious values. For the current conversion rates, see the SAP BTP Service Description Guide .

Let's assume the following parameters:

 · 1 node hour of a Starter Instance equals 0.5 capacity units.

 · 1 node hour of an Infer-M Instance equals 2 capacity units.

 · 1 tenant hour of Baseline equals 1.5 capacity units.

 · 1 capacity unit costs EUR 1

If you consume 100 hours of a Starter Instance and 300 hours of an Infer-M Instance, and you are active on the cluster for 300 hours in a month, the following costs will accrue:

100 node hours of Starter Instance = 100 * 0.5 capacity units = 50 capacity units 300 node hours of Infer-M Instance = 300 * 2 capacity units = 600 capacity units 300 tenant hours of Baseline = 300 * 1.5 capacity units = 450 capacity units

Total = 1,100 capacity units = EUR 1,100

 Note

It is possible to process data in parallel, using more than one node. Costs are calculated in node hours, meaning that 1 hour of data processing, on 2 nodes is equal to 2 node hours.

 Recommendation

For an estimate of projected costs, use the SAP AI Core Cost Calculator.

Related Information

SAP Discovery Center SAP BTP Service Description Guide Choose a Resource Plan [page 219]

4.3 Metering and Pricing for the Generative AI Hub

The use of models in the generative AI hub is metered using GenAI tokens and capacity units.

 Note

The generative AI hub is available only as part of the Extended service plan.

GenAI tokens correspond to blocks of 1,000 tokens from each model. The number of GenAI tokens you need changes based on the model you use and whether the token is for input or output. There are conversion rates that help you figure out how many GenAI tokens you get from the model provider's tokens. The conversion rates apply to blocks of 1,000 input and output model tokens. By referring to these rates, you can work out how many GenAI tokens you're consuming in total. For more information about the supported models and the conversion rates between model tokens and GenAI tokens, see SAP Note 3437766 .

Each record measured in GenAI tokens is converted into billable metric units, known as 'capacity units'. For the conversion rate between GenAI tokens and capacity units (capacity unit values), see the SAP BTP Service Description Guide .

 Example

This example uses fictitious values.

For a given request, x input model tokens and y output model tokens are consumed. The corresponding metrics are:

<!-- missing-text -->

Charges associated with the use of other SAP AI Core components may also apply. For more information, see SAP AI Core Metering and Pricing.

 Recommendation

For an estimate of projected costs, use the SAP AI Core cost calculator. For more information, see Cost Calculator.

4.4 Choose a Resource Plan

You can configure SAP AI Core to use different infrastructure resources for different tasks, based on demand. SAP AI Core provides several preconfigured infrastructure bundles called 'resource plans' for this purpose.

Context

Resource plans are used to select resources in workflow and serving templates. Different steps of a workflow can have different resource plans.

In general, if your workload needs GPU acceleration, you should use one of the GPU-enabled resource plans. Otherwise, choose a resource plan based on the anticipated CPU and memory need of your workloads.

Within SAP AI Core, the resource plan is selected via the ai.sap.com/resourcePlan label at pod level. It maps the selected resource plan and takes a string value, which can be any of the following resource plan IDs:

Resource Plan Specifications for AWS

Code to Allocate Resources

<!-- missing-text -->

<!-- missing-text -->

Restriction

For the Free Tier service plan, only the Starter resource plan is available. Specifying other plans will result in error. For the Standard service plan, all resource plans are available. For more information, see Free Tier [page 45] and Service Plans [page 43].

 Note

There are limits to the default disk storage size for all of these nodes. Datasets that are loaded to the nodes will consume disk space. If you have large data sets (larger than 30 GB), or have large models, you may have to increase the disk size. To do so, use the persistent volume claim in Argo Workflows to specify the required disk size (see Volumes ).

Service Usage Reporting

Usage consumption of services is reported in the SAP BTP cockpit on the Overview page for your global account and on the Overview and Usage Analytics pages of your subaccount. The usage report lists usage in billable measures and non-billable measures. Your final monthly bill is based on the billable measure only. Non-billable measures are displayed for reporting purposes only.

4.5 Update a Service Plan

Learn how to update to the SAP AI Core standard or extended plan, after exploring the product in Free Tier.

Procedure

 1. Open your global account in the SAP BTP cockpit.

 2. Go to your subaccount.

 3. In the navigation area, choose Instances and Subscriptions .

A list of the applications to which your subaccount is subscribed in the Cloud Foundry environment is displayed.

 4. Search for SAP AI Core.

 5. Select the ellipsis at the end of the subscription row, and from the menu, select Update .

 6. In the wizard that opens, select default and click Update Subscription .

<!-- missing-text -->

 Note

After you have updated your service plan, free tier restrictions are no longer applied.

Data that you have defined in the free tier plan is automatically migrated to your new plan.

Updating the service plan does not change user permissions.

Changing Service Plans

If you first subscribe to the free tier option, you can migrate the same service instance to a standard service plan (for enterprise accounts).

If you migrate from the free tier option to a standard plan or extended plan, or from a standard plan to an extended plan, your metadata and transaction data, including trained models, are also migrated (for enterprise accounts).

<!-- missing-text -->

Restriction

It is not possible to migrate from a standard or extended service plan to the free tier option, or from an extended plan to a standard plan.

If an instance with an extended or standard plan is deleted, it is not possible to create a new instance with a standard plan.

5 Initial Setup

You provision SAP AI Core from the SAP BTP cockpit in SAP Business Technology Platform. After provisioning, you will have your service key, which provides URLs and credentials for accessing the SAP AI Core instance

Prerequisites

 · Your SAP BTP administrator has access to a global account on SAP Business Technology Platform. For more information, see Getting a Global Account.

 · Your SAP BTP administrator has set the entitlement to a subaccount.

Context

The SAP AI Core service is a tenant-aware reuse service. It isolates tenants based on the ID of the zone (representing the subaccount). The SAP AI Core service instance is created within a subaccount. Each subaccount represents an SAP AI Core tenant.

 Note

The SAP AI Core service does not isolate tenants based on the service instance ID. If you create multiple service instances within the same subaccount, all of them will reference the same SAP AI Core tenant.

<!-- missing-text -->

The steps below guide you through the provisioning procedure, Alternatively, a booster is available for both SAP AI Core and SAP AI Launchpad. For more information, see AI Boosters Tutorial . If you choose to use the booster, you can skip the remaining steps to SAP AI Core Starter Tutorials [page 72].

5.1 Enabling the Service in Cloud Foundry

Enable SAP AI Core using the standard procedures for the SAP BTP Cloud Foundry environment.

You provision SAP AI Core from the SAP BTP cockpit in SAP Business Technology Platform. After you have provisioned the service, you will have your service key, which provides URLs and credentials for accessing the SAP AI Core instance.

The SAP AI Core service is a tenant-aware reuse service. It isolates tenants based on the ID of the zone (representing the subaccount). The SAP AI Core service instance is created within a subaccount. Each subaccount represents an SAP AI Core tenant.

 Note

The SAP AI Core service does not isolate tenants based on the service instance ID. If you create multiple service instances within the same subaccount, all of them will reference the same SAP AI Core tenant.

<!-- missing-text -->

The steps below guide you through the provisioning procedure. Alternatively, a booster is available for both SAP AI Core and SAP AI Launchpad. For more information, see AI Boosters Tutorial . If you choose to use the booster, you can skip the remaining steps to SAP AI Core Starter Tutorials [page 72].

5.1.1  Create a Subaccount

Procedure

<!-- missing-text -->

 3. Optional: If your subaccount is used for production purposes, under Advanced select the Used for production checkbox.

This setting does not change the configuration of your subaccount. It is intended to help you manage the production subaccounts in your global account. For example, your cloud operator can refer to it when handling incidents related to mission-critical accounts.

Advanced

<!-- missing-text -->

 4. Click Create .

 5. Return to the Account Explorer to view your subaccount.

<!-- missing-text -->

Related Information

SAP BTP Cockpit

5.1.2  Enable Cloud Foundry

Procedure

 1. Click your subaccount and on the Overview page, choose Enable Cloud Foundry .

<!-- missing-text -->

 2. Enter the basic information for your Cloud Foundry environment instance and click Create .

<!-- missing-text -->

5.1.3  Create a Space

Procedure

 1. On the overview page for your subaccount, choose Create Space .

<!-- missing-text -->

<!-- missing-text -->

 2. Enter a name for your space, assign the required roles, and click Create .

<!-- missing-text -->

Related Information

About Roles in the Cloud Foundry Environment

5.1.4  Add a Service Plan

&gt;

Procedure

 1. In SAP BTP cockpit, navigate to your global account and choose Entitlements and then Entity Assignments .

 2. From the Select Entities box, select your subaccount and click Select .

 3. Choose Edit .

<!-- missing-text -->

 Tip

To use generative AI capabilities, choose the extended plan. For more information, see Service Plans [page 43].

 6. Save your changes.

<!-- missing-text -->

Related Information

Service Plans [page 43]

5.1.5  Create a Service Instance

Procedure

 1. In SAP BTP cockpit, navigate to your subaccount within your global account and choose Service Marketplace .

You will see a tile for SAP AI Core.

<!-- missing-text -->

 2. Open the tile and click Create .

<!-- missing-text -->

 3. Enter a name for your service instance and choose Next (all other details will be filled by default).

New Instance or Subscription

<!-- missing-text -->

 4. At present, the JSON file upload feature is not supported. Choose Next to proceed.

New Instance or Subscription

<!-- missing-text -->

 5. Check the data and choose Create .

New Instance or Subscription

<!-- missing-text -->

Results

When your service instance is created, you can view it on the Instances and Subscriptions page of your subaccount.

<!-- missing-text -->

5.1.6  Create a Service Key

Procedure

 1. On the Instances and Subscriptions page, find your new instance and choose Create Service Key from the dropdown.

<!-- missing-text -->

 2. Enter a name for your service key and click Create .

New Service Key

<!-- missing-text -->

Optional: To use an x.509 certificate instead of client secret credentials, specify the credentials by updating and uploading the following as a JSON:

```
{ "xsuaa": {"credential-type":"x509", "x509": { "key-length": 2048, "validity":7, "validity-type":"DAYS"}
```

 · key-length : The byte length of the generated private key. Default: 2048.

 · validity : The validity time unit. DAYS, MONTHS and YEARS are supported. Default: DAYS.

 · validity-type : The number of time units. Default: 7.

The default combination is a key of length 2048, valid for 7 DAYS.

 3. Download your service key to save it.

Results

You now have your service key, which provides URLs and credentials for accessing the SAP AI Core instance through SAP AI Launchpad, SAP AI Core toolkit Postman, or curl.

Credentials

<!-- missing-text -->

If you have generated a client secret, your key will include:

 · clientid clientsecret , , and url can be used to generate your authentication token.

 · identityzone and identityzoneid represent your tenant ID.

 · appname provides the service instance details if service instance isolation is implemented.

 · serviceurls allow you to interact with SAP AI Core once your authentication token has been generated.

 · AI_API_URL : Unified AI API to handle ML artifacts (such as training, data, models, and deployments) across multiple hyperscalers.

If you have generated an x.509 certificate, your key will include:

 · certificate

 · certurl can be used to generate your authentication token.

 · key your RSA private key.

 · identityzone and identityzoneid represent your tenant ID.

 · appname provides the service instance details if service instance isolation is implemented.

 · serviceurls allow you to interact with SAP AI Core once your authentication token has been generated.

 · AI_API_URL : Unified AI API to handle ML artifacts (such as training, data, models, and deployments) across multiple hyperscalers.

5.1.7  Use a Service Key

After you have configured your service key, it can be used by local clients, apps in other spaces, or entities outside your deployment to access SAP AI Core through one of the available interfaces.

Using Postman

Prerequisites

 · You have downloaded and installed the Postman client from https:/ /www.postman.com/ .

 · You have familiarized yourself with the Postman documentation and interface.

Procedure

 1. Download the JSON collection from https:/ /api.sap.com/api/AI_CORE_API/overview .

 2. In Postman, click Import , select the JSON file, and choose Import to start the import.

 3. After the import is complete, highlight the collection and select the Authorization tab.

 4. Navigate to Configure New Token , enter the credentials from your service key, and save your changes.

 Note

 · The Token Name field is your choice of descriptive identifier.

 · The Access Token URL is labeled url in your service key. Add /oauth/token to the end of the URL.

 · The Grant Type should be Client Credentials .

If you see an alert relating to the characters in your credentials, ignore it.

 Note

If you have generated a x.509 certificate instead of client secret credentials, you'll need to use your certificate key , and certUrl to create your token.

For example: curl --cert &lt;cert.pem&gt; --key &lt;key.pem&gt; -XPOST &lt;certUrl&gt;/oauth/token -d 'grant_type=client_credentials&amp;client_id=&lt;client id&gt;'

 5. Select the Variables tab, and set your baseUrl from your credentials.

The baseUrl is labeled AI_API_URL in your service key.

 6. Choose Save .

 7. In the Authorization tab, choose Get New Access Token . and wait for the authentication process. When the authentication process is complete, select Use Token to finish. Check that the token is stored in your environment variables. If it is not stored automatically, you can copy and paste it to the Token field manually.

Next Steps

To train and deploy your own AI models, follow the procedure in Administration [page 73].

To use generative AI models provided in the generative AI hub, see Models and Scenarios in the Generative AI Hub [page 110].

Using Curl

Prerequisites

curl is likely to be installed on your operating system by default. To check, open a command prompt and enter curl -V . If curl isn't installed, download and install it from https:/ /curl.se/ .

 Note On macOS, you may need to install jq so that you can follow the curl commands. 1. Install brew from https:/ /brew.sh/ . 2. In a T erminal session, run brew install jq to install jq in your shell environment.

Procedure

 1. Set up your environment as follows:

For Linux:

```
# XSUAA details URLs should be without trailing slash '/' # xport CLIENTID= e <clientid> from service key xport CLIENTSECRET= e <clientsecret> from service key xport XSUAA_URL= e <url> from service key xport AI_API_URL= e <AI_API_URL> from service key
```

For Windows PowerShell:

```
$env:CLIENTID = <clientid> from service key env:CLIENTSECRET = $ <clientsecret> from service key env:XSUAA_URL = $ <url> from service key env:AI_API_URL = $ <AI_API_URL> from service key
```

 Note

The export command sets the values of your keys to your environment variable, meaning that they will be retained after you close your terminal session. It is possible to set the environment variables without the export , for the current session only.

 2. Get the XSUAA OAuth Token using clientid and clientsecret from the service key to call the APIs.

The XSUAA OAuth token is required for authentication when making AI API calls.

For Linux:

```
SECRET='echo -n '$CLIENTID:$CLIENTSECRET' | base64 -i - ' OKEN='curl --location --request POST "$XSUAA_URL/oauth/token? T grant_type=client_credentials" \ --header "Authorization: Basic $SECRET" | jq -r '.access_token''
```

For Windows PowerShell:

```
$SECRET = $env:CLIENTID + ":" + $env:CLIENTSECRET base64SECRET = $ [Convert]::ToBase64String([System.Text.Encoding]::UTF8.GetBytes("$SECRET")) TOKENRESPONSE = curl --location --request POST "$XSUAA_URL/oauth/token? $ grant_type=client_credentials" --header "Authorization: Basic $base64SECRET" TOKENRESPONSE = $TOKENRESPONSE | ConvertFrom-Json $ TOKEN = $TOKENRESPONSE.access_token $
```

 Note

The token is valid for a limited time. Once it has expired, create a new token, using the same code snippet.

 Note

```
If you have generated a x.509 certificate instead of client secret credentials, you'll need to use your certificate key , and certUrl to create your token. For example: curl --cert <cert.pem> --key <key.pem> -XPOST <certUrl>/oauth/token
```

```
-d 'grant_type=client_credentials&client_id=<client id>'
```

 3. Verify that the token has been fetched properly:

```
echo $TOKEN
```

You should see a long string of alphanumeric characters:

```
eyJhbGciOiJSUzI1NiIsImprdSI6Imh0dHBzOi8vYWktYWxwaGEtdmFsaWRhdGlvbi0yLmF1dGhlbn RpY2F0aW9uLnNhcC5oYW5hLm9uZGVtYW5kLmNvbS90b2tlbl9rZXlzIiwia2lkIjoiZGVmYXVsdC1q d3Qta2V5LTMyODMxMjg2NCIsInR5cCI6IkpXVCJ94ZGU5YjAxNmQ0MDk5YjlmM... ............................................ . ...ALdfbMsHoYTtF6fNFbf3ZQ
```

Next Steps

To train and deploy your own AI models, follow the procedure in Administration [page 73].

To use generative AI models provided in the generative AI hub, see Models and Scenarios in the Generative AI Hub [page 110].

5.2 Enabling the Service in the Kyma Environment

Enable SAP AI Core using the standard procedures for the SAP BTP Kyma environment.

Procedure

 1. Create a service instance in the Kyma environment.

 2. You can then bind the service instance to your application, or you can create a service key to communicate directly with the service instance. See Using SAP BTP Services in the Kyma Environment.

Related Information

Using SAP BTP Services in the Kyma Environment

6 SAP AI Core Starter Tutorials

You can use a booster to get started with the Free Tier plans for SAP AI Core and SAP AI Launchpad using the tutorial: AI Boosters .

Alternatively, to complete the booster tutorial, then learn the basics of SAP AI Core from beginning to end in a simple use case, follow the tutorial: Quick Start for SAP AI Core . If you have already provisioned SAP AI Core, you can skip through to the second tutorial in this guide.

To see the complete library of tutorials available for SAP AI Core, see Tutorials [page 334].

 Note

Once you have provisioned SAP AI Core, you can access the service in multiple ways. Where the tutorial below offers more than one option tab, you need only complete the tab for your chosen method of access. For customers who prefer a GUI to code, SAP AI Launchpad is recommended. For more information, see SAP AI Launchpad.

7 Administration

Creating secrets for external programs and tools, that are used with SAP AI Core means that you can connect them without compromising your credentials.

Using SAP AI Core alongside external tools such as GitHub, Docker and Amazon Web Services S3 storage leverages the benefits of version control, containerization, and cloud storage. Your content is made available remotely, if you have a stable internet connection.

Administration is a one-time procedure, but steps can be repeated, if necessary, for example to remove or add a tool.

 Note

You must have completed the initial setup tasks before configuring your SAP AI Core instance. For more information, see Initial Setup [page 53].

Manage Resource Groups [page 82]

A resource group is a unique dedicated namespace or workspace environment, where users can create or add configurations, executions, deployments, and artifacts. They are used for running training jobs or model servers.

7.1 Manage Your Git Repository

7.1.1  Add a Git Repository

You can use your own git repository to version control your SAP AI Core templates. The GitOps onboarding to SAP AI Core instances involves setting up your git repository and synchronizing your content.

Using Curl

Prerequisites

 · You've access to a git repository over the Internet.

 · You've generated a personal access token for your git repository. For more information, see Create a Personal Access Token .

 · If you want to onboard a git repository hosted on GitLab, make sure that the repository URL contains the .git suffix.

 · Secrets aren't permitted in your repository. If secrets are used, it isn't possible to synchronize content.

 · You have completed the Initial Setup. For more information, see Initial Setup [page 53].

 Note

When you synchronize resources, make sure that there are no naming collisions. This is important if you use multiple repositories or applications in one tenant. If you experience difficulties during synchronization, we recommend that you use only one repository or application per tenant.

Context

Git repositories are managed by creating personal access tokens registering them in SAP AI Core. Personal access tokens are a means of allowing and controlling connections to GitHub repositories without compromising your credentials.

Procedure

Submit a POST request to the endpoint {{apiurl}}/v2/admin/repositories and include your credentials:

```
curl --location --request POST "$AI_API_URL/v2/admin/repositories" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ --data-raw '{ "url": "https://github.com/john/examplerepo", "username": "john", "password": "<GIT_PAT_USER_TOKEN>" }'
```

You specify your unique git repository details as follows:

 · url : URL of the git repository

 · username : (Service) user that is accessing the git repository

 · password : git personal access token. For more information, see Create a Personal Access Token .

Next Steps

Create an application to sync your folders. For more information, see Create an Application [page 78].

Using Postman

Prerequisites

 · You've access to a git repository over the Internet.

 · You've generated a personal access token for your git repository. For more information, see Create a Personal Access Token .

 · If you want to onboard a git repository hosted on GitLab, make sure that the repository URL contains the .git suffix.

 · Secrets aren't permitted in your repository. If secrets are used, it isn't possible to synchronize content.

 · You have completed the Initial Setup. For more information, see Initial Setup [page 53].

 Note

When you synchronize resources, make sure that there are no naming collisions. This is important if you use multiple repositories or applications in one tenant. If you experience difficulties during synchronization, we recommend that you use only one repository or application per tenant.

Context

Git repositories are managed by creating personal access tokens registering them in SAP AI Core. Personal access tokens are a means of allowing and controlling connections to GitHub repositories without compromising your credentials.

Procedure

Send a POST request to the endpoint {{apiurl}}/v2/admin/repositories and include your credentials in JSON format in the raw body:

You specify your unique git repository details as follows:

 · url : URL of the git repository

 · username : (Service) user that is accessing the git repository

 · password : git personal access token. For more information, see Create a Personal Access Token .

Next Steps

Create an application to sync your folders. For more information, see Create an Application [page 78].

7.1.2  Edit a Git Repository

Using Curl

Procedure

Submit a PATCH request to the endpoint {{apiurl}}/v2/admin/repositories and include your changes:

```
curl --location --request PATCH "$AI_API_URL/v2/admin/repositories" \
```

```
-header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ --data-raw '{ "url": "https://github.com/john/examplerepo", "username": "john", "password": "<GIT_PAT_USER_TOKEN>" }'
```

You specify your unique git repository details as follows:

 · url : URL of the git repository

 · username : (Service) user that is accessing the git repository

 · password : git personal access token. For more information, see Create a Personal Access Token .

Using Postman

Procedure

Send a PATCH request to the endpoint {{apiurl}}/v2/admin/repositories/{{repositoryName}} and include your changes in the body.

You specify your unique git repository details as follows:

 · url : URL of the git repository

 · username : (Service) user that is accessing the git repository

 · password : git personal access token. For more information, see Create a Personal Access Token .

7.1.3  Delete a Git Repository

You remove a Git repository from a connection if its URL is invalid or contains errors, or if the repo is no longer required. Once a Git repository is removed, it can no longer be selected as a source repository for an application.

Using Postman

Send a DELETE request to the endpoint {{apiurl}}/v2/admin/repositories/{{repositoryName}} and include your repository name.

Using curl

```
curl --location --request DELETE "{{apiurl}}/v2/admin/repositories/
```

```
{{repositoryName}}" \
```

Using Curl

Context

You remove a Git repository from a connection if its URL is invalid or contains errors, or if the repo is no longer required. Once a Git repository is removed, it can no longer be selected as a source repository for an application.

Procedure

Run the following code:

```
curl --location --request DELETE "{{apiurl}}/v2/admin/repositories/ {{repositoryName}}" \
```

Using Postman

Context

You remove a Git repository from a connection if its URL is invalid or contains errors, or if the repo is no longer required. Once a Git repository is removed, it can no longer be selected as a source repository for an application.

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/admin/repositories/{{repositoryName}} and include your repository name.

7.2 Manage Applications

7.2.1  Create an Application

Using Curl

Context

After you have registered your git repository, you need to create an application to sync the templates in your repository. The first sync takes some time, but you can check the status of the application to see when it is complete. After the first sync, syncing takes place automatically, every ~3 minutes, or can be requested manually.

 · applicationName : Set a name for your application. The name must be between 3 and 64 characters long and match [A-Za-z0-9\-\_]+ .

 · repositoryUrl : The URL of a registered git repository.

 · revision : The revision to target. &lt;HEAD&gt; refers to the most recent.

 · path : The path to the target folder that contains the templates to be synced.

Procedure

Submit a POST request to the endpoint {{apiurl}}/v2/admin/applications including details of your application:

```
curl --location --request POST "$AI_API_URL/v2/admin/applications" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ --data-raw '{ "applicationName": "my-app", "repositoryUrl": "https://github.com/john/examplerepo", "revision": "HEAD", "path": "workflows" }'
```

 · url : URL of the git repository

 · username : (Service) user that is accessing the git repository

 · password : git personal access token. For more information, see Create a Personal Access Token .

Since each application points to a particular path and revision in the repository, multiple applications may be created for the same repositoryUrl .

Results

After the GitOps setup is completed, the templates in your git repository are automatically synced to SAP AI Core approximately every three minutes.

Next Steps

Check the synchronization status of your application by submitting a GET request to {{apiurl}}/v2/ admin/applications/{{appName}}/status :

```
curl --location --request GET "$AI_API_URL/v2/admin/applications/{{appName}}/ status" \ -header "Authorization: Bearer $TOKEN" \ --header 'Content-Type: application/json' -
```

As applicationName , enter the name of your application that you specified when you created the application.

Using Postman

Context

After you have registered your git repository, you need to create an application to sync the templates in your repository. The first sync takes some time, but you can check the status of the application to see when it is complete. After the first sync, syncing takes place automatically, every ~3 minutes, or can be requested manually.

 · applicationName : Set a name for your application. The name must be between 3 and 64 characters long and match [A-Za-z0-9\-\_]+ .

 · repositoryUrl : The URL of a registered git repository.

 · revision : The revision to target. &lt;HEAD&gt; refers to the most recent.

 · path : The path to the target folder that contains the templates to be synced.

Procedure

Send a POST request to the endpoint {{apiurl}}/v2/admin/applications including details of your application:

 · url : URL of the git repository

 · username : (Service) user that is accessing the git repository

 · password : git personal access token. For more information, see Create a Personal Access Token .

Since each application points to a particular path and revision in the repository, multiple applications may be created for the same repositoryUrl .

Results

After the GitOps setup is completed, the templates in your git repository are automatically synced to SAP AI Core approximately every three minutes.

Next Steps

Check the synchronization status of your application by sending a GET request to {{apiurl}}/v2/admin/ applications/{{appName}}/status . As applicationName , enter the name of your application that you specified when you created the application.

```
 Output Code
```

```
{ "healthStatus": "Healthy", "message": "successfully synced (all tasks run)", "reconciledAt": "2021-11-23T10:27:49Z", "source": { "path": "workflows", "repoURL": "https://github.com/username/examplerepo", "revision": "db611bb28be3c853d08867c08b52b8f733b4f7bf", }, "syncFinishedAt": "2021-11-23T10:27:49Z", "syncResourceStatus": [ { "kind": "ServingTemaplate", "message": "servingtemplate.ai.sap.com/text-clf-infer-tutorial configured", "name": "text-clf-infer-tutorial", "status": "Synced", } ], "syncedStartedAt": ""2021-11-23T10:27:48Z",", "syncStatus": "Synced", }
```

Sync an Application Manually

Applications sync with your GitHub repository automatically at intervals of ~3 minutes. Use the endpoint below to manually request a sync: {{apiurl}}/admin/applications/{{appName}}/refresh

7.2.2  List Applications

Using Postman

Send a GET request to the endpoint {{apiurl}}/v2/admin/applications .

Using curl

Send a GET request to the endpoint {{apiurl}}/v2/admin/applications .

7.2.3  Edit an Application

Using Postman

Send a PATCH request to the endpoint {{apiurl}}/v2/admin/applications/{{appName}} and include your changes in the body.

Using curl

Send a PATCH request to the endpoint {{apiurl}}/v2/admin/applications/{{appName}} and include your changes in the body.

7.2.4  Delete an Application

Using Curl

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/admin/applications/{{appName}} .

Using Postman

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/admin/applications/{{appName}} .

7.3 Manage Resource Groups

A resource group is a unique dedicated namespace or workspace environment, where users can create or add configurations, executions, deployments, and artifacts. They are used for running training jobs or model servers.

Resource groups are used to physically isolate machine learning workloads, and to logically isolate related resources for a usage scenario.

When your tenant is onboarded, a default resource group is automatically created. Default resource groups can't be deleted.

As an administrator, you create, edit, or delete resource groups, based on your service consumers and usage scenarios.

Runtime entities such as executions, deployments, configurations, and artifacts belong to a specific resource group and are not shared across resource groups. Scenarios, executables, and Docker registry secrets are shared by all resource groups within a tenant.

A resource group may also be referred to as an instance.

 Remember

Your SAP global account may consist of several accounts. Each account can be associated with a tenant. A tenant can contain multiple resource groups. A tenant always contains a default resource group, as well as the resource groups defined for your usage scenarios.

<!-- missing-text -->

Restriction

The maximum number of resource groups is limited at tenant level to 50. If you reach this limit, you will receive an error message. To free up space, delete some resource groups. Alternatively, raise a ticket to increase your quota.

For more information, see Delete a Resource Group [page 86].

Create a Resource Group [page 83]

Edit a Resource Group [page 85]

Delete a Resource Group [page 86]

Parent topic: Administration [page 73]

Resource Group Level Resources

Executables at tenant level are shared across all resource groups. In contrast, runtime entities such as executions, deployments, configurations, and artifacts belong to a specific resource group and cannot be shared across resource groups. Similarly, generic secrets created within a resource group and be used only for workloads within that group.

You can register an object store at resource-group level by setting the resource group header. We recommend that you do not use the same object store bucket with the same IAM user for multiple resource groups.

Example resource group mappings are outlined in the figure below:

<!-- missing-text -->

7.3.1  Create a Resource Group

Parent topic: Manage Resource Groups [page 82]

Related Information

Edit a Resource Group [page 85] Delete a Resource Group [page 86]

Using Curl

Prerequisites

You have completed the Initial Setup. For more information, see Initial Setup [page 53].

Context

 Note

Resource group Ids must be of length minimum: 3, maximum: 253. The first and last characters must be either a lowercase letter, an uppercase letter, or a number. Character entries from the second to penultimate, may include a lower case letter, an upper case letter, a number, a period (.), or a hyphen (-). No other special characters are permitted.

Procedure

Create a resource group by sending the following:

```
curl --location --request POST "$AI_API_URL/v2/admin/resourceGroups" --header "Authorization: Bearer $TOKEN" --header 'Content-Type: application/json' --dataraw '{ "resourceGroupId": "<ID of your resource group>"}'
```

Using Postman

Prerequisites

You have completed the Initial Setup. For more information, see Initial Setup [page 53].

Context

 Note

Resource group Ids must be of length minimum: 3, maximum: 253. The first and last characters must be either a lowercase letter, an uppercase letter, or a number. Character entries from the second to penultimate, may include a lower case letter, an upper case letter, a number, a period (.), or a hyphen (-). No other special characters are permitted.

Procedure

 1. As the request body, select the raw radio button and enter the following:

```
{ "resourceGroupId": "<ID of your resource group>" }
```

 2. Send the request.

Results

You'll receive a 202 response to confirm that the request to create the resource group has been accepted.

7.3.2  Edit a Resource Group

 Note

Resource group Ids must be of length minimum: 3, maximum: 253. The first and last characters must be either a lower case letter, an upper case letter or a number. Character entries from the second to penultimate, may include a lower case letter, an upper case letter, a number, a full stop or a hyphen. No other special characters are permitted.

Using Postman

```
1. Send a PATCH request to the endpoint {{apiurl}}/v2/admin/resourceGroups/
```

 {{resource_group_name}} with the body:

```
{ "resourceGroupId": "<ID of your resource group>" }
```

Using curl

 1. Create a resource group by sending the following:

```
curl --location --request PATCH "$AI_API_URL/v2/admin/resourceGroups/ {{resource_group_name}}" --header "Authorization: Bearer $TOKEN" --header 'Content-Type: application/json' --data-raw '{ "resourceGroupId": "<ID of your resource group>"}'
```

Parent topic: Manage Resource Groups [page 82]

Related Information

```
Create a Resource Group [page 83] Delete a Resource Group [page 86]
```

7.3.3  Delete a Resource Group

Parent topic: Manage Resource Groups [page 82]

Related Information

Create a Resource Group [page 83] Edit a Resource Group [page 85]

Using Curl

Procedure

Run the following code:

```
curl --location --request POST "$AI_API_URL/v2/admin/resourceGroups/ {{resource_group_name}}" --header "Authorization: Bearer $TOKEN" --header 'Content-Type: application/json' --data-raw '{ "resourceGroupId": "<ID of your resource group>"}'
```

Using Postman

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/admin/resourceGroups/ {{resource_group_name}} .

7.4 Manage Object Store Secrets

7.4.1  Register an Object Store Secret

Connect SAP AI Core to a cloud object store and manage access using an object store secret. The connected storage stores your dataset, models, and other cache files of the Metaflow Library for SAP AI Core.

Your cloud storage credentials are managed using secrets. Secrets are a means of allowing and controlling connections across directories and tools, without compromising your credentials.

Prerequisites

You have completed the Initial Setup. For more information, see Initial Setup [page 53].

Context

SAP AI Core supports multiple hyperscaler object stores, such as Amazon S3, GCS, OSS (Alicloud Object Storage Service), SAP HANA Cloud, Data Lake, and Azure Blob Storage.

Using Postman

 1. Send a POST request to the endpoint {{apiurl}}/v2/admin/objectStoreSecrets .

 2. As the request body, select the raw radio button and enter your object store secret details.

 Note

For all storage types except Azure Blob Storage, all &lt;data&gt; fields are required. For Azure, required fields are specified.

 · For Amazon S3:

{

```
"name": "<Your identifier>", "data": { "AWS_ACCESS_KEY_ID": "<AWS access key ID>", "AWS_SECRET_ACCESS_KEY": "<AWS secret access key>" }, "type": "S3", "bucket": "<S3 bucket name>", "endpoint": "<S3 end point>", "region": "<S3 region>", "pathPrefix": "<A path prefix that follows the bucket name>" }
```

 · For GCS

```
{ "name": "<Your identifier>", "type": "gcs", "pathPrefix": "<A path prefix that follows the bucket name>", "data": { "BUCKET": "<GCS bucket name>", "PRIVATE_KEY": "<GCS private key>", // optional "PROJECT_ID": "<GCS project Id>",   // optional "REGION": "<GCS region>",           // optional } }
```

 · For OSS (Alicloud Object Storage Service):

```
{ "name": "default", "type": "oss", "pathPrefix": "<path prefix to be appended with bucketname>", "data": { "BUCKET": "<bucket-name>", "ENDPOINT": "oss-cn-shanghai.aliyuncs.com",{ "name": "default", "type": "S3", "bucket": "{ "name": "default", "type": "oss", "pathPrefix": "<path prefix to be appended with bucketname>", "data": { "REGION": "", "ACCESS_KEY_ID": "xxxxx", "SECRET_ACCESS_KEY": "xxxxx" } }
```

 · For SAP HANA Cloud, Data Lake:

```
{ "name": "default", "type": "webhdfs", "pathPrefix": "<path prefix to be appended>", "data": { // e.g. https://c32727c8-4260-4c37-b97f-ede322dcfa8f.files.hdl.canaryeu10.hanacloud.ondemand.com "HDFS_NAMENODE": "https://<file-container-name>.files.hdl.canaryeu10.hanacloud.ondemand.com", "TLS_CERT": "-----BEGIN CERTIFICATE----\nMIICmjCCAYIxxxxxxxxxxxxR4wtC32bGO66D+Jc8RhaIA==\n-----END CERTIFICATE-----\n", "TLS_KEY": "-----BEGIN PRIVATE KEY----\nMIIEvQIBADANBgkqxxxxxxxxxxxxnor+rtZHhhzEfX5dYLCS5Pww=\n-----END PRIVATE KEY-----\n", "HEADERS": "{x-sap-filecontainer\": \"<file-container-name>\", Content-Type\": application/octet-stream\"}"
```

```
} }
```

 · For Azure Blob Storage:

```
{ "name": "default", "type": "azure", "pathPrefix": "<path prefix to be appended>", "data": { "CONTAINER_URI": "https://account_name.blob.core.windows.net/ container_name",  //required "REGION": "<region name>",                  //optional "CLIENT_ID": "<azure client id>",           //optional "CLIENT_SECRET": "<azure client secret>",   //optional "STORAGE_ACCESS_KEY": "sas_token",          //required "TENANT_ID": "azure tenant id",             //optional "SUBSCRIPTION_ID": "subscription id",       //optional } }
```

 3. Send the request.

The following shows a request using S3.

<!-- missing-text -->

Using curl

 1. Register your object store secret details using the endpoint /v2/admin/objectStoreSecrets .

 Note

For all storage types except Azure Blob Storage, all &lt;data&gt; fields are required. For Azure, required fields are specified.

 · For Amazon S3:

```
curl --location --request POST "$AI_API_URL/v2/admin/objectStoreSecrets" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default",
```

```
"type": "S3", "bucket": "<S3 bucket name>", "endpoint": "<S3 end point>", "pathPrefix": "<A path prefix that follows the bucket name>", "region": "<S3 region>", "data": { "AWS_ACCESS_KEY_ID": "<AWS access key ID>", "AWS_SECRET_ACCESS_KEY": "<AWS secret access key>" } }'
```

 · For GCS

```
{ "name": "<Your identifier>", "type": "gcs", "pathPrefix": "<A path prefix that follows the bucket name>", "data": { "BUCKET": "<GCS bucket name>", "PRIVATE_KEY": "<GCS private key>", // optional "PROJECT_ID": "<GCS project Id>",   // optional "REGION": "<GCS region>",           // optional } }
```

 · For OSS (Alicloud Object Storage Service):

```
curl --location --request POST "$AI_API_URL/v2/admin/objectStoreSecrets" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default", "type": "oss", "pathPrefix": "<path prefix to be appended with bucketname>", "data": { "BUCKET": "<bucket-name>", "ENDPOINT": "oss-cn-shanghai.aliyuncs.com", "REGION": "", "ACCESS_KEY_ID": "xxxxx", "SECRET_ACCESS_KEY": "xxxxx" } }
```

 · For SAP HANA Cloud, Data Lake:

```
curl --location --request POST "$AI_API_URL/v2/admin/objectStoreSecrets" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default", "type": "webhdfs", "pathPrefix": "<path prefix to be appended>", "data": { // e.g. https://c32727c8-4260-4c37-b97fede322dcfa8f.files.hdl.canary-eu10.hanacloud.ondemand.com "HDFS_NAMENODE": "https://<file-container-name>.files.hdl.canaryeu10.hanacloud.ondemand.com", "TLS_CERT": "-----BEGIN CERTIFICATE----\nMIICmjCCAYIxxxxxxxxxxxxR4wtC32bGO66D+Jc8RhaIA==\n-----END CERTIFICATE-----\n", "TLS_KEY": "-----BEGIN PRIVATE KEY----\nMIIEvQIBADANBgkqxxxxxxxxxxxxnor+rtZHhhzEfX5dYLCS5Pww=\n-----END PRIVATE KEY-----\n", "HEADERS": "{x-sap-filecontainer\": \"<file-container-name>\", Content-Type\": application/octet-stream\"}"
```

```
} }
```

 · For Azure Blob Storage:

```
curl --location --request POST "$AI_API_URL/v2/admin/objectStoreSecrets" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default", "type": "azure", "pathPrefix": "<path prefix to be appended>", "data": { "CONTAINER_URI": "https://account_name.blob.core.windows.net/ container_name",  //required "REGION": "<region name>",                  //optional "CLIENT_ID": "<azure client id>",           //optional "CLIENT_SECRET": "<azure client secret>",   //optional "STORAGE_ACCESS_KEY": "sas_token",          //required "TENANT_ID": "azure tenant id",             //optional "SUBSCRIPTION_ID": "subscription id",       //optional } }
```

 Note

For input artifacts only

You can create multiple secrets using different values for name , but you must create a default first.

 Tip

The pathPrefix is useful if you share the same bucket for different projects. You can set the name of your project folder to my-ml-project1 , for example. All data is then stored in that folder.

 Note

If the AI-Resource-Group header is not specified, the &lt;Resource Group&gt; is assigned the value "default" automatically.

<!-- missing-text -->

 Restriction

When using an SAP HANA Data Lake object store with output artifacts pointing to a directory, you cannot use archive: none: {} in your workflow templates to disable artifact archiving. For more information, see Workflow Templates [page 221].

7.4.2  Edit an Object Store Secret

Using Postman

```
1. Send a PATCH request to the endpoint {{apiurl}}/v2/admin/objectStoreSecrets/{{objectStoreName}} . 2. As the request body, select the raw radio button and enter your object store secret details.
```

 Note

For all storage types except Azure Blob Storage, all &lt;data&gt; fields are required. For Azure, required fields are specified.

 · For Amazon S3:

```
{ "name": "default", "type": "S3", "bucket": "<S3 bucket name>", "endpoint": "<S3 end point>", "pathPrefix": "<A path prefix that follows the bucket name>", "region": "<S3 region>", "data": { "AWS_ACCESS_KEY_ID": "<AWS access key ID>", "AWS_SECRET_ACCESS_KEY": "<AWS secret access key>" } }
```

 · For OSS (Alicloud Object Storage Service):

```
{ "name": "default", "type": "oss", "pathPrefix": "<path prefix to be appended with bucketname>", "data": { "BUCKET": "<bucket-name>", "ENDPOINT": "oss-cn-shanghai.aliyuncs.com", "REGION": "", "ACCESS_KEY_ID": "xxxxx", "SECRET_ACCESS_KEY": "xxxxx" } }
```

 · For SAP HANA Cloud, Data Lake:

```
{ "name": "default", "type": "webhdfs", "pathPrefix": "<path prefix to be appended>", "data": { // e.g. https://c32727c8-4260-4c37-b97f-ede322dcfa8f.files.hdl.canaryeu10.hanacloud.ondemand.com "HDFS_NAMENODE": "https://<file-container-name>.files.hdl.canaryeu10.hanacloud.ondemand.com",
```

 ·

```
"TLS_CERT": "-----BEGIN CERTIFICATE----For Azure Blob Storage:
```

```
{ "name": "default", "type": "azure", "pathPrefix": "<path prefix to be appended>", "data": { "CONTAINER_URI": "https://account_name.blob.core.windows.net/ container_name",  //required "REGION": "<region name>",                  //optional "CLIENT_ID": "<azure client id>",           //optional "CLIENT_SECRET": "<azure client secret>",   //optional "STORAGE_ACCESS_KEY": "sas_token",          //required "TENANT_ID": "azure tenant id",             //optional "SUBSCRIPTION_ID": "subscription id",       //optional } }
```

 3. Send the request.

Using curl

 1. Register your object store secret details using the endpoint $AI_API_URL/v2/admin/ objectStoreSecrets/{{objectStoreName}} .

 Note For all storage types except Azure Blob Storage, all &lt;data&gt; fields are required. For Azure, required fields are specified.

 · For Amazon S3:

```
curl --location --request PATCH "$AI_API_URL/v2/admin/objectStoreSecrets/ {{objectStoreName}}" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default", "type": "S3", "bucket": "<S3 bucket name>", "endpoint": "<S3 end point>", "pathPrefix": "<A path prefix that follows the bucket name>", "region": "<S3 region>", "data": { "AWS_ACCESS_KEY_ID": "<AWS access key ID>", "AWS_SECRET_ACCESS_KEY": "<AWS secret access key>" } }'
```

```
\nMIICmjCCAYIxxxxxxxxxxxxR4wtC32bGO66D+Jc8RhaIA==\n-----END CERTIFICATE-----\n", "TLS_KEY": "-----BEGIN PRIVATE KEY----\nMIIEvQIBADANBgkqxxxxxxxxxxxxnor+rtZHhhzEfX5dYLCS5Pww=\n-----END PRIVATE KEY-----\n", "HEADERS": "{x-sap-filecontainer\": \"<file-container-name>\", Content-Type\": application/octet-stream\"}" } }
```

 · For OSS (Alicloud Object Storage Service):

```
curl --location --request PATCH "$AI_API_URL/v2/admin/objectStoreSecrets/ {{objectStoreName}}" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default", "type": "oss", "pathPrefix": "<path prefix to be appended with bucketname>", "data": { "BUCKET": "<bucket-name>", "ENDPOINT": "oss-cn-shanghai.aliyuncs.com", "REGION": "", "ACCESS_KEY_ID": "xxxxx", "SECRET_ACCESS_KEY": "xxxxx" } }
```

 · For SAP HANA Cloud, Data Lake:

```
curl --location --request PATCH "$AI_API_URL/v2/admin/objectStoreSecrets/ {{objectStoreName}}" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default", "type": "webhdfs", "pathPrefix": "<path prefix to be appended>", "data": { // e.g. https://c32727c8-4260-4c37-b97fede322dcfa8f.files.hdl.canary-eu10.hanacloud.ondemand.com "HDFS_NAMENODE": "https://<file-container-name>.files.hdl.canaryeu10.hanacloud.ondemand.com", "TLS_CERT": "-----BEGIN CERTIFICATE----\nMIICmjCCAYIxxxxxxxxxxxxR4wtC32bGO66D+Jc8RhaIA==\n-----END CERTIFICATE-----\n", "TLS_KEY": "-----BEGIN PRIVATE KEY----\nMIIEvQIBADANBgkqxxxxxxxxxxxxnor+rtZHhhzEfX5dYLCS5Pww=\n-----END PRIVATE KEY-----\n", "HEADERS": "{x-sap-filecontainer\": \"<file-container-name>\", Content-Type\": application/octet-stream\"}" } }
```

 · For Azure Blob Storage:

```
curl --location --request PATCH "$AI_API_URL/v2/admin/objectStoreSecrets/ {{objectStoreName}}" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ -header 'AI-Resource-Group: <Resource group>' \ --data-raw '{ -"name": "default", "type": "azure", "pathPrefix": "<path prefix to be appended>", "data": { "CONTAINER_URI": "https://account_name.blob.core.windows.net/ container_name",  //required "REGION": "<region name>",                  //optional "CLIENT_ID": "<azure client id>",           //optional "CLIENT_SECRET": "<azure client secret>",   //optional "STORAGE_ACCESS_KEY": "sas_token",          //required "TENANT_ID": "azure tenant id",             //optional "SUBSCRIPTION_ID": "subscription id",       //optional
```

```
} }
```

 Note For input artifacts only You can create multiple secrets using different values for name , but you must create a default first.  Tip The pathPrefix is useful if you share the same bucket for different projects. You can set the name of your project folder to my-ml-project1 , for example. All data will then be stored in that folder.  Note If the AI-Resource-Group header is not specified, the &lt;Resource Group&gt; is assigned the value "default" automatically.

7.4.3  Delete an Object Store Secret

Using Curl

Context

Deleting an object store secret stops access to the object store.

Procedure

Run the following code:

```
curl --location --request DELETE "$AI_API_URL/v2/admin/objectStoreSecrets/ {{objectStoreName}}" \
```

Using Postman

Context

Deleting an object store secret stops access to the object store.

Procedure

Send a DELETE request to the endpoint

7.5 Manage Docker Registry Secrets

7.5.1  Register Your Docker Registry Secret

Docker packages and runs applications in remote containers. Connect SAP AI Core to a Docker repository and manage access using a Docker registry secret.

Prerequisites

You have completed the Initial Setup. For more information, see Initial Setup [page 53].

Context

Your Docker credentials are managed using secrets. Secrets allow and control connections across directories and tools without compromising your credentials.

Your Docker registry secret lets you authorize SAP AI Core to pull your private Docker images from your Docker repository. You specify the name of the secret in your workflows to authenticate the Docker image pull. For more information, see Workflow Templates [page 221] and Serving Templates [page 268].

Using Postman

 1. Send a POST request to the endpoint {{apiurl}}/v2/admin/dockerRegistrySecrets

 2. As the request body, select the raw radio button and enter the following:

```
{ name": "mydockerregistry", " "data": { ".dockerconfigjson": "{auths\":{your.private.registry\": {username\":john\",password\":docker-accesstoken-or-password\"}}}" } }
```

 · name : Set the name of your Docker registry secret. This is your choice of identifier for your secret. In the example, the name is "mydockerregistry" .

 · data : Enter a JSON string that represents your Docker registry secret.

 3. Send the request:

<!-- missing-text -->

 4. After your Docker registry secret has been created, reference it in your template as an image pull secret.

```
 Source Code spec: imagePullSecrets: - name: <Name of your Docker registry secret>
```

<!-- missing-text -->

Restriction

The maximum number of Docker registry secrets is limited at tenant level to 50. If you reach this limit, you will receive an error message. To free up space, delete some Docker registry secrets. Alternatively, raise a ticket to increase your quota.

For more information, see Delete a Docker Registry Secret [page 100].

Using curl

Submit a POST request to the endpoint {{apiurl}}/v2/admin/dockerRegistrySecrets , Include the following parameters in your request body:

 · name : Set the name of your Docker registry secret.

 · data : Enter a JSON string that represents your Docker registry secret.

```
 Sample Code { name": "mydockerregistry", " "data": { ".dockerconfigjson": "{auths\":{your.private.registry\": {username\":john\",password\":docker-accesstoken-or-password\"}}}" } }
```

 Note

If you are using a public Docker registry from http:/ /hub.docker.com , you must provide your Docker URL in the format https://index.docker.io , in the &lt;auths\"&gt; variable input.

 1. Submit a POST request to the endpoint {{apiurl}}/v2/admin/dockerRegistrySecrets and include the name of your Docker registry secret along with the credentials for your repository. For example

```
 Sample Code $ curl --location --request POST "$AI_API_URL/v2/admin/ dockerRegistrySecrets" --header "Authorization: Bearer $TOKEN" --header 'Content-Type: application/json' --data-raw '{ "name": "mydockerregistry", "data": { ".dockerconfigjson": "{auths\": {my.docker.repositories.io\": {username\":\"$USERNAME\", password\": \"$PWD\"}}}" } } { "message": "secret has been created" }
```

 2. After your Docker registry secret has been created, reference it in your template as an image pull secret. For example

```
 Source Code spec: imagePullSecrets: - name: <Name of your Docker registry secret>
```

<!-- missing-text -->

Restriction

The maximum number of Docker registry secrets is limited at tenant level to 50. If you reach this limit, you will receive an error message. To free up space, delete some Docker registry secrets. Alternatively, raise a ticket to increase your quota.

For more information, see Delete a Docker Registry Secret [page 100].

7.5.2  Edit a Docker Registry Secret

Docker packages and runs applications in remote containers. Connect SAP AI Core to a Docker repository and manage access using a Docker registry secret.

Using Curl

Context

Your Docker credentials are managed using secrets. Secrets allow and control connections across directories and tools without compromising your credentials.

Your Docker registry secret lets you authorize SAP AI Core to pull your private Docker images from your Docker repository. You specify the name of the secret in your workflows to authenticate the Docker image pull. For more information, see Workflow Templates [page 221] and Serving Templates [page 268].

Procedure

Submit a PATCH request to the endpoint $AI_API_URL/v2/admin/dockerRegistrySecrets/ {{dockerRegistryName}} , Include the following parameters in your request body:

 · name : Set the name of your Docker registry secret.

 · data : Enter a JSON string that represents your Docker registry secret.

```
 Sample Code { name": "mydockerregistry", " "data": { ".dockerconfigjson": "{auths\":{your.private.registry\": {username\":john\",password\":docker-accesstoken-or-password\"}}}" } }
```

 Note If you are using a public Docker registry from http:/ /hub.docker.com , you must provide your Docker URL in the format https://index.docker.io , in the &lt;auths\"&gt; variable input.

```
$ curl --location --request PATCH "$AI_API_URL/v2/admin/dockerRegistrySecrets/ {{dockerRegistryName}}" --header "Authorization: Bearer $TOKEN" --header 'Content-Type: application/json' --data-raw '{ "name": "mydockerregistry", "data": { ".dockerconfigjson": "{auths\": {my.docker.repositories.io\": {username\":\"$USERNAME\", password\": \"$PWD\"}}}" } }
```

Using Postman

Procedure

Send a PATCH request to the endpoint {{apiurl}}/v2/admin/repositories/{{repositoryName}} and include your changes in the body.

You specify your unique git repository details as follows:

 · url : URL of the git repository

 · username : (Service) user that is accessing the git repository

 · password : git personal access token. For more information, see Create a Personal Access Token .

7.5.3  Delete a Docker Registry Secret

Deleting a docker registry secret removes access to the docker registry.

Using Curl

Procedure

Submit a DELETE request to the endpoint $AI_API_URL/v2/admin/dockerRegistrySecrets/ {{dockerRegistryName}} .

Using Postman

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/admin/dockerRegistrySecrets/ {{dockerRegistryName}}

7.6 Manage Generic Secrets

7.6.1  Create a Generic Secret

A generic secret gives SAP AI Core authorization to utilize your resource group without exposing your credentials.

Using Curl

Prerequisites

You have completed the Initial Setup. For more information, see Initial Setup [page 53].

Context

Generic Secrets are used in addition to store sensitive information when system secrets are not applicable, for example in integration use cases where SAP AI Core is an orchestration layer.

SAP AI Core lets you optionally use generic secrets at the following levels:

 · Main-tenant scope

 · Tenant wide level

 · Resource-group level

Generic secrets are different to system secrets (such as object store, Docker registry, and so on) and can be used to store sensitive information, either for the main tenant, for all of its resource groups, or for each resource group via an API. The latter can be attached to containers in executions or deployments as environment variables or volume mounts.

Tenant-wide secrets are only automatically propagated to the appropriate resource group when a new execution or deployment is created. They are not propagated to running deployments.

Procedure

Send a POST request and enter the URL {{apiurl}}/v2/admin/secrets .

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 : and : . The operation will be performed at the tenant-

 · AI-Tenant-Scope true AI-Resource-Group * wide level.

```
curl --location --request POST "$AI_API_URL/v2/admin/secrets" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ --header 'AI-Resource-Group: default' \ --data-raw '{ "name": "MY_GENERIC_SECRET", "data": { "some-credential": "bXktc2Vuc2l0aXZlLWRhdGE=" } ' }
```

 Note

The secret name is written without hyphens to make it simple to consume as a Unix environment variable later. It is written in capitals as is convention.

Using Postman

Prerequisites

You have completed the Initial Setup. For more information, see Initial Setup [page 53].

Context

Generic Secrets are used in addition to store sensitive information when system secrets are not applicable, for example in integration use cases where SAP AI Core is an orchestration layer.

SAP AI Core lets you optionally use generic secrets at the following levels:

 · Main-tenant scope

 · Tenant wide level

 · Resource-group level

Generic secrets are different to system secrets (such as object store, Docker registry, and so on) and can be used to store sensitive information, either for the main tenant, for all of its resource groups, or for each resource group via an API. The latter can be attached to containers in executions or deployments as environment variables or volume mounts.

Tenant-wide secrets are only automatically propagated to the appropriate resource group when a new execution or deployment is created. They are not propagated to running deployments.

Procedure

 1. Send a POST request and enter the URL {{apiurl}}/v2/admin/secrets .

 2. As the request body, select the raw radio button and enter your credentials in JSON format:

```
{ name": "MY_GENERIC_SECRET", " "data": { "some-credential": "bXktc2VjcmV0LWNyZWRlbnRpYWw=", "other-credentials": "bXktc2VjcmV0LW90aGVyLWNyZWRlbnRpYWw=" } }
```

 · name : Set the name of your generic secret.

 · data : Enter a JSON string that represents your generic secret.

 Note

The secret name is written without hyphens to make it simple to consume as a Unix environment variable later. It is written in capitals as is convention.

 3. Specify the scope of the request via the header AI-Tenant-Scope and AI-Resource-Group :

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 · AI-Tenant-Scope : true and AI-Resource-Group * : . The operation will be performed at the tenant-wide level.

In this example, we are using the resource-group level.

Key

Value

<!-- missing-text -->

 4. Send the request.

Results

```
 Output Code { message": "secret has been created", " "name": "MY_GENERIC_SECRET" }
```

7.6.2  List All Generic Secrets

Using Curl

Procedure

Submit a GET request to the endpoint /v2/admin/secrets , and include the scope via the headers:

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 · AI-Tenant-Scope : true and AI-Resource-Group * : . The operation will be performed at the tenantwide level.

```
curl --location --request GET "$AI_API_URL/v2/admin/secrets" \
```

```
-header "Authorization: Bearer $TOKEN" \ ---header 'AI-Resource-Group: default'
```

Results

The response includes a list of generic secrets, their name, and their creation timestamp. No sensitive information is revealed in the response.

Using Postman

Procedure

Send a GET request to the endpoint {{apiurl}}/v2/admin/secrets .

 a. As the request body, select the none radio button.

 b. Specify the scope of the request via the header AI-Tenant-Scope or AI-Resource-Group :

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 · AI-Tenant-Scope : true and AI-Resource-Group * : . The operation will be performed at the tenant-wide level.

Results

The response includes a list of generic secrets, their name, and their creation timestamp. No sensitive information is revealed in the response.

7.6.3  Update a Generic Secret

To update a generic secret, use the PATCH endpoint as shown below. The PATCH operation replaces the secret with the data provided. This can be used for rotating secret credentials.

Using Curl

Context

Generic Secrets are used in addition to store sensitive information when system secrets are not applicable, for example in integration use cases where SAP AI Core is an orchestration layer.

SAP AI Core lets you optionally use generic secrets at the following levels:

 · Main-tenant scope

 · Tenant wide level

 · Resource-group level

Generic secrets are different to system secrets (such as object store, Docker registry, and so on) and can be used to store sensitive information, either for the main tenant, for all of its resource groups, or for each resource group via an API. The latter can be attached to containers in executions or deployments as environment variables or volume mounts.

Tenant-wide secrets are only automatically propagated to the appropriate resource group when a new execution or deployment is created. They are not propagated to running deployments.

Procedure

Submit a PATCH request to the endpoint /v2/admin/secrets/"$SECRET_NAME" . Specify the scope via the headers:

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 · AI-Tenant-Scope : true and AI-Resource-Group * : . The operation will be performed at the tenantwide level.

```
curl --location --request PATCH "$AI_API_URL/v2/admin/secrets/$SECRET_NAME" \
```

```
-header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ --header 'AI-Resource-Group: default' \ --data-raw '{ "data": { "some-credential": "bXktc2Vuc2l0aXZlLWRhdGE=" } ' }
```

Using Postman

Context

Generic Secrets are used in addition to store sensitive information when system secrets are not applicable, for example in integration use cases where SAP AI Core is an orchestration layer.

SAP AI Core lets you optionally use generic secrets at the following levels:

 · Main-tenant scope

 · Tenant wide level

 · Resource-group level

Generic secrets are different to system secrets (such as object store, Docker registry, and so on) and can be used to store sensitive information, either for the main tenant, for all of its resource groups, or for each resource group via an API. The latter can be attached to containers in executions or deployments as environment variables or volume mounts.

Tenant-wide secrets are only automatically propagated to the appropriate resource group when a new execution or deployment is created. They are not propagated to running deployments.

Procedure

 1. Send a PATCH request to the endpoint {{apiurl}}/v2/admin/secrets/{{secretName}} As the request body, select the raw radio button and enter the following code:

```
 Source Code { "data": { "updated-credentials": "bXktc2VjcmV0LW90aGVyLWNyZWRlbnRpYWw=" } }
```

Specify the scope of the request via the header AI-Tenant-Scope and specify the scope via the or AI-Resource-Group :

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 · AI-Tenant-Scope : true and AI-Resource-Group * : . The operation will be performed at the tenant-wide level.

 2. Send the request.

Results

```
 Output Code { message": "The secret has been modified", " "name": "my-generic-secret" }
```

7.6.4  Delete a Generic Secret

To get a secret name, see List All Generic Secrets [page 104].

Using Curl

Procedure

Submit a DELETE request to the endpoint /v2/admin/secrets/"$SECRET_NAME" . Specify the scope of the request via the header AI-Tenant-Scope and AI-Resource-Group :

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 · AI-Tenant-Scope : true and AI-Resource-Group * : . The operation will be performed at the tenantwide level.

In this example we use the resource-group scope:

```
curl --location --request DELETE "$AI_API_URL/v2/admin/secrets/
```

```
$SECRET_NAME$AI_API_URL/v2/admin/secrets/$SECRET_NAME" \ -header "Authorization: Bearer $TOKEN" \ ---header 'AI-Resource-Group: default'
```

Using Postman

Procedure

 1. Send a DELETE request to the endpoint {{apiurl}}/v2/admin/secrets/{{secretName}}

As the request body, select the none radio button. Specify the scope of the request via the header AI-Tenant-Scope or AI-Resource-Group :

 · AI-Tenant-Scope : true . The operation will be performed at the main-tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

 · AI-Tenant-Scope : true and AI-Resource-Group * : . The operation will be performed at the tenant-wide level.

 2. Send the request.

Results

```
 Output Code 200
```

7.6.5  Consume Generic Secrets in Executions or Deployments

Generic secrets at resource-group level can be attached to containers in executions or deployments. They can either be mounted as a volume or attached as an environment variable. The following examples illustrate how to consume a generic secret in a container by declaring it in the template. Note that only generic secrets can be attached to containers in this way. System secrets cannot be consumed in a template.

Consume a Generic Secret as an Environment Variable

Generic secrets can be attached to containers using either envFrom.secretRef or env.valueFrom.secretKeyRef , as shown below.

 · Using envFrom.secretRef :

```
spec: containers: image: centaur
```

```
- name: my-kserve-container envFrom:
```

```
- secretRef: name: MY_GENERIC_SECRET
```

If your secret contains invalid characters, such as hyphens (-), this method will result in error. You will need to map your secret to a valid variable name using env.valueFrom.secretKeyRef .

 · Using env.valueFrom.secretKeyRef :

```
spec: containers: - name: kserve-container image: centaur env: - name: MY_GENERIC_SECRET valueFrom: secretKeyRef: name: my-generic-secret key: some-credential
```

Consume a Generic Secret as a Volume Mount

Generic secrets can also be mounted to containers as volumes, as shown below.

```
spec: containers: - name: kserve-container image: centaur volumeMounts: - name: my-generic-secret mountPath: "/etc/my-generic-secret" readOnly: true volumes: - name: my-generic-secret secret: secretName: my-generic-secret
```

Additional Information

Secret names can be included as parameters in the templates and supplied via AI API configurations, as shown below.

```
envFrom: secretRef: -name: "{{inputs.parameters.secretName}}"
```

8 Generative AI Hub in SAP AI Core

The generative AI hub incorporates generative AI into your AI activities in SAP AI Core and SAP AI Launchpad.

LLMs are self-supervised, deep learning models that have been trained on vast amounts of unlabeled data. They leverage AI technology and industrial-scale computational resources to learn complex language patterns and semantic knowledge bases for natural language processing (NLP) tasks. They parse input, such as prompts, and by predicting a target word, can return contextually relevant responses written in natural language. A single LLM can perform multiple NLP tasks by using different input formats and output modes.

LLMs are general models but can be fine-tuned with additional embeddings for specialized or domain-specific use cases.

SAP AI Core and the generative AI hub help you to integrate LLMs and AI into new business processes in a cost-efficient manner.

<!-- missing-text -->

8.1 Models and Scenarios in the Generative AI Hub

The generative AI hub is available in SAP AI Core only with the extended service plan. For information about how to add a service plan or update an existing service plan, see Add a Service Plan in SAP AI Core [page 60] and Update a Service Plan in SAP AI Core [page 51].

Scenarios

Access to generative AI models is provided under the global AI scenario foundation-models , which is managed by SAP AI Core. Individual models are provided as executables in the form of serving templates, and accessed by choosing the corresponding template for the desired model.

The following scenarios are available:

<!-- missing-text -->

Models

For more information about available models, including conversion rates for tokens, rate limits and deprecation dates, see SAP Note 3437766 .

For more information about models and their parameters, see the documentation from the model provider.

Models from Azure OpenAI are accessed through a private instance of the chat-completions API. The access points are not publicly accessible, and are accessed through SAP AI Core. For more information, see Azure OpenAI Chat Completions API .

Open Source models are hosted by SAP AI Core and can be accessed via OpenAI compatible API schema.

8.2 Create a Deployment for a Generative AI Model

You make a generative AI model available for use by creating a deployment. You can do so one time for each model and model version, and for each resource group that you want to use with generative AI hub. The deployment URL that is generated can be reused.

Prerequisites

 · You have an SAP AI Core service instance and service key. For more information, see SAP AI Core Initial Setup Documentation.

 · You're using the extended service plan. For more information, see Service Plans [page 43] and Update a Service Plan [page 51].

Context

You make a model available for use by creating a deployment. You can do so one time for each model and model version. The model deployment includes the modelName and version of the model you want to access. After the deployment is complete, you have a deploymentUrl , which can be used across your organization to access the model version.

Using the API

Procedure

 1. Decide which LLM you want to deploy and note the following information:

 · Executable ID

 · Model name

 · Model version

 Note

 · Instead of specifying a model version, using 'latest' will use the latest version of the model available in SAP AI Core.

 · Where model version is not listed, it is not applicable.

For more information about available models, including conversion rates for tokens, rate limits and deprecation dates, see SAP Note 3437766 .

 2. Check that you have access to the scenario containing generative AI by sending a GET request to {{apiurl}}/v2/lm/scenarios .

Set the Authorization header with Bearer $TOKEN and set your resource group. For more information, see Create a Resource Group [page 83].

 Note

You must use the same resource group for all of your generative AI activities. To use a different resource group, these steps must be repeated for each resource group. For more information, see Manage Resource Groups [page 82].

<!-- missing-text -->

The scenarios listed contain a scenario with the id foundation-models .

 3. Create a configuration by sending a POST request to the endpoint {{apiurl}}/v2/lm/ configurations .

Include details of the model to which you want to provide access by passing in the following parameters:

 · name is your free choice of identifier.

 · executableId modelName , , and modelVersion are provided in the table above.

 · scenarioId must be foundation-models .

 · versionId is your own version reference.

<!-- missing-text -->

<!-- missing-text -->

 Tip

You can specify the value latest for the modelVersion to use the most recent model version available in SAP AI Core.

You receive a unique configurationId in the response.

 4. Create a deployment by sending a POST request to the endpoint {{apiurl}}/v2/lm/deployments .

Include the configurationId from the previous step in your request.

<!-- missing-text -->

<!-- missing-text -->

 5. Retrieve the details of your deployment by sending a GET request to the endpoint {{apiurl}}/v2/lm/ deployments .

<!-- missing-text -->

Next Steps

When the deployment is running, the model can be accessed using the deploymentUrl provided in the response. For more information, see Consume Generative AI Models [page 116].

Model Lifecycle

Model versions have deprecation dates. Where a model version is specified in a deployment, the deployment will stop working on the deprecation date of that model version.

Implement one of the following model upgrade options:

 · Auto Upgrade: Create a new generative AI configuration and deployment or patch a deployment with a new configuration, specifying modelVersion latest . When a new modelVersion is supported by SAP AI Core, existing generative AI deployments will automatically use the latest version of the given model.

 · Manual Upgrade: Create a new generative AI configuration with your chosen replacement modelVersion and use it to patch your deployment. This model version will be used in generative AI deployments irrespective of updates to the models supported by SAP AI Core.

 Note

If modelVersion isn't specified, it will be latest by default. To upgrade manually, you must specify a modelVersion .

8.3 Consume Generative AI Models

The generative AI hub helps you to complete tasks like summarizing text, inferencing, and transforming content. To do so, you consume a generative AI model by sending a request to the model's endpoint.

Prerequisites

You have the deployment URL for your generative AI model. For more information, see Create a Deployment for a Generative AI Model [page 112].

Context

Ensure that you have the following headers set:

<!-- missing-text -->

 Caution

SAP does not take any responsibility for quality of the content in the input to or output of the underlying generative AI models. This includes but is not limited to bias, hallucinations, or inaccuracies. The user is responsible for verifying the content.

 Caution

Do not store personally identifiable information in prompts when using the generative AI hub. Personally identifiable information is any data that can be used alone, or in combination, to identify the person that the data refers to.

Example Payloads for Inferencing

llama2-70b-chat-hfThe following examples show how you can consume various generative AI models using curl. For more information about prompts, see the tutorial Prompt LLMs in the Generative AI Hub in SAP AI Core &amp; Launchpad .

```
 Tip If you use a Windows device, use Windows Powershell, and replace curl with curl.exe
```

```
.
```

Open AI

For information about the supported API versions, see Chat completions and Embeddings in the Microsoft documentation.

GPT-4-32k | GPT-4 | GPT-3.5-Turbo-16k | GPT-3.5-Turbo

Text Input

```
curl --location '$DEPLOYMENT_URL/chat/completions?api-version=2023-05-15' \ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "messages": [ { "role": "user", "content": "sample input prompt" } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" ' }
```

GPT-4o | GPT-4-Turbo | GPT-4o Mini

Image input

```
# Request ---# --header 'AI-Resource-Group: <Resource Group Id>' \
```

```
curl --location '$DEPLOYMENT_URL/chat/completions?api-version=2023-05-15' \ --header 'Content-Type: application/json' \
```

```
--header "Authorization: Bearer $AUTH_TOKEN" \ "url": "https://path/images/image.png"
```

```
--data '{ "messages": [ { "role": "user", "content": [ { "type": "text", "text": "Describe this picture:" }, { "type": "image_url", "image_url": { } } ] } ], "max_tokens": 10 ' }
```

text-embedding-ada-002 | text-embedding-3-small | text-embedding-3-large

```
curl --location '$DEPLOYMENT_URL/embeddings?api-version=2023-05-15' \
```

```
-header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "input": "sample input prompt" ' }
```

Vertex AI

Gemini 1.0 Pro

```
curl --location '$DEPLOYMENT_URL/models/gemini-1.0-pro:generateContent' \ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "contents": [ { "role": "user", "parts": { "text": "Hello!" } }, { "role": "model", "parts": { "text": "Argh! What brings ye to my ship?" } }, { "role": "user", "parts": { "text": "Wow! You are a real-life pirate!" } } ], "safety_settings": { "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_LOW_AND_ABOVE" }, "generation_config": { "temperature": 0.9, "topP": 1, "candidateCount": 1,
```

```
"maxOutputTokens": 2048
```

```
} ' }
```

Gemini 1.5 Pro

Text Input

```
curl --location '$DEPLOYMENT_URL/models/gemini-1.5-pro:generateContent' \ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "generation_config":{ "maxOutputTokens":100 }, "contents": { "role": "user", "parts": { "text": "Give me a recipe for banana bread." } }, "safety_settings": { "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_LOW_AND_ABOVE" } ' }
```

Image input

```
curl --request POST --location "$DEPLOYMENT_URL/models/gemini-1.5pro:generateContent" \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "contents": { "role": "user", "parts": [ { "fileData": { "mimeType": "image/png", "fileUri": "filepath/images/scones.jpg" } }, { "text": "Describe this picture." } ] } ' }
```

Gemini 1.5 Flash

Text Input

```
curl --request POST --location "$DEPLOYMENT_URL/models/gemini-1.5flash:generateContent" \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "generation_config":{ "maxOutputTokens":100 },
```

```
"contents": { "role": "user", "parts": { "text": "Give me a recipe for banana bread." } }, "safety_settings": { "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_LOW_AND_ABOVE" } ' }
```

Image input

```
curl --request POST --location "$DEPLOYMENT_URL/models/gemini-1.5flash:generateContent" \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "contents": { "role": "user", "parts": [ { "fileData": { "mimeType": "image/png", "fileUri": "filepath/images/scones.jpg" } }, { "text": "Describe this picture." } ] } ' }
```

Text Bison

```
curl --location '$DEPLOYMENT_URL/models/text-bison:predict'  \ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "instances": [ { "prompt": "Sample prompt" } ], "parameters": { "temperature": 0.8, "maxOutputTokens": 50 } ' }
```

Chat Bison

```
curl --location '$DEPLOYMENT_URL/models/chat-bison:predict'  \ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "instances": [ { "context": "Your conversation context here", "messages": [
```

```
{ "author": "user", "content": "User message 1" }, { "author": "assistant", "content": "Assistant response 1" }, { "author": "user", "content": "User message 2" } ] } ], "parameters": { "temperature": 0.8 } ' }
```

Textembedding Gecko

```
curl --location '$DEPLOYMENT_URL/models/textembedding-gecko:predict'  \ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "instances": [ { "task_type": "RETRIEVAL_DOCUMENT", "title": "Document title", "content": "I would like embeddings for this text" } ] ' }
```

Textembedding Gecko Multilingual

```
curl --location '$DEPLOYMENT_URL/models/textembedding-gecko-
```

```
multilingual:predict'  \ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "instances": [ { "task_type": "RETRIEVAL_DOCUMENT", "title": "Document title", "content": "I would like embeddings for this text" } ] ' }
```

AWS Bedrock

Claude 3 Sonnet | Claude 3.5 Sonnet | Claude 3 Opus | Claude 3 Haiku

```
curl --location '$DEPLOYMENT_URL/invoke' \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \
```

```
--header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "anthropic_version": "bedrock-2023-05-31", "max_tokens": 100, "messages": [ { "role": "user", "content": "Hello, Claude" } ] }'
```

Titan Text Express | Titan Text Lite

```
curl --location '$DEPLOYMENT_URL/invoke' \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "inputText": "Who am AI?", "textGenerationConfig": { "maxTokenCount": 10, "stopSequences": [], "temperature": 0, "topP": 1 } }'
```

Titan Embed Text

```
curl --location '$DEPLOYMENT_URL/invoke' \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "inputText": "Who im I?" }'
```

Nova Pro | Nova Lite | Nova Micro

```
curl --location '$DEPLOYMENT_URL/converse' \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "inferenceConfig": { "maxTokens": 100, "stopSequences": [ "blab" ], "temperature": 0.7 }, "messages": [ { "content": [ { "text": "Perplexity means?" }], "role": "user" } ] ' }
```

aicore-opensource

Llama-3-70b-instruct | Llama-3.1-70b-instruct | mistralai--mixtral-8x7b-instruct-v01

```
curl --location '$DEPLOYMENT_URL/chat/completions' \ -header 'AI-Resource-Group: default' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "model": "<ModelName>", "messages": [ { "role": "user", "content": "Sample prompt" } ], "max_tokens": 100 }'
```

aicore-mistral

mistralai--mistral-large-instruct

```
url --location '$DEPLOYMENT_URL/chat/completions' \ c --header 'AI-Resource-Group: default' \ --header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "model": "<modelName>", "messages": [ { "role": "user", "content": "Whats the difference between Accountability vs Responsibility, answer in 200 words?" } ], "max_tokens": 100 }' # Here's an example response: { "choices": [ { "finish_reason": "length", "index": 0, "logprobs": null, "message": { "content": " Accountability and responsibility are related but distinct concepts.\n\n**Responsibility** is about the tasks and duties assigned to you. It's what you're expected to do as part of your role or agreement. For example, a project manager is responsible for planning and overseeing a project. Responsibility is often proactive and focuses on what you should do.\n\n**Accountability**, on the other hand, is about answering for the outcomes of your responsibilities. It", "role": "assistant", "tool_calls": [] }, "stop_reason": null } ], "created": 1730096376, "id": "chat-8f06fb250621420a9aa1baafadb04da1",
```

```
"model": "mistralai--mistral-large-instruct", "object": "chat.completion", "usage": { "completion_tokens": 100, "prompt_tokens": 23, "total_tokens": 123 }
```

aicore-ibm

ibm--granite-13b-chat

```
url --location '$DEPLOYMENT_URL/chat/completions' \ c --header 'AI-Resource-Group: default' \ --header 'Content-Type: application/json' \ --header "Authorization: Bearer $AUTH_TOKEN" \ --data '{ "model": "<modelName>", "messages": [ { "role": "user", "content": "Whats the difference between Accountability vs Responsibility, answer in 200 words?" } ], "max_tokens": 100 }' # Here's an example response: { "choices": [ { "finish_reason": "length", "index": 0, "logprobs": null, "message": { "content": "Accountability and responsibility are two related but distinct concepts that are often used interchangeably, but they have different nuances. Accountability refers to being answerable or responsible for something, typically for an outcome, decision, or action. It implies that there is a level of ownership and control over the result, and one is expected to take responsibility for the consequences of their actions. Accountability can be held by individuals, teams, or organizations, and it often involves being transparent, communicative, and responsible", "role": "assistant", "tool_calls": [] }, "stop_reason": null } ], "created": 1730097489, "id": "chat-9acb599c079248f59281238c6d5c1a30", "model": "ibm--granite-13b-chat", "object": "chat.completion", "usage": { "completion_tokens": 100, "prompt_tokens": 24, "total_tokens": 124 }
```

If you want to remove a model, delete its deployment. For more information, see Delete a Single Deployment [page 297] and Delete Multiple Deployments [page 298].

8.3.1  Prompt Examples

Summarizing

You can provide the LLM with a text and ask for a summary of it.

Procedure

Send a POST request to the relevant endpoint.

Include your query in the body. Mark the text to be summarized with triple back ticks (').

<!-- missing-text -->

 Example

This example generates a summary of a product review. Summaries can include topics that aren't related to the main topic.

```
{ messages": [ " { "role": "user", "content": "Your task is to generate a short summary of a product review from an ecommerce site. Summarize the review below, delimited by triple backticks, in at most 30 words. Review:'''Got this panda plush toy for my daughter's birthday, who loves it and takes it everywhere. It's soft and super cute, and its face has a friendly look. It's a bit small for what I paid though. I think there might be other options that are bigger for the same price. It arrived a day earlier than expected, so I got to play with it myself before I gave it to her. '''" } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" }
```

<!-- missing-text -->

 Example

In this example, the prompt is similar but has been refined by adding the intended recipient of the feedback (the purchasing department) and the reason for requesting it (to determine the price of the product).

```
{ messages": [ " { "role": "user", "content": "Your task is to generate a short summary of a product review from an ecommerce site to give feedback to the pricing department, responsible for determining the price of the product. Summarize the review below, delimited by triple backticks, in at most 30 words, and focusing on any aspects that are relevant to the price and perceived value. Review: '''Got this panda plush toy for my daughter's birthday,who loves it and takes it everywhere. It's soft and super cute, and its face has a friendly look. It's a bit small for what I paid though. I think there might be other options that are bigger for the same price. It arrived a day earlier than expected, so I got to play with it myself before I gave it to her.''' " } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" }
```

<!-- missing-text -->

<!-- missing-text -->

Inferencing

Inferencing uses the information in a given text to draw a conclusion.

Procedure

Send a POST request to the relevant endpoint.

Include your query in the body. Mark the text to be inferenced with triple back ticks (').

 Example

This example performs a sentiment analysis on a product review.

```
{ messages":[ " { "role":"user", "content":"What is the sentiment of the following product review, which is delimited with triple backticks? Review text: '''Needed a nice lamp for my bedroom, and this one had additional storage and not too high of a price point. Got it fast.  The string to our lamp broke during the transit and the company happily sent over a new one. Came within a few days as well. It was easy to put together.  I had a missing part, so I contacted their support and they very quickly got me the missing piece! Lumina seems to me to be a great company that cares about their customers and products!!''' " } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" } 00
```

 Example

This example generates the sentiment as a one word response.

{

messages":[ "

<!-- missing-text -->

 Example

This example analyzes the emotions expressed in the review.

```
{ messages":[ " { "role":"user", "content": "Identify a list of emotions that the writer of the following review is expressing. Include no more than five items in the list. Format your answer as a list of lower-case words separated by commas.Review text: '''Needed a nice lamp for my bedroom, and this one had additional storage and not too high of a price point. Got it fast.  The string to our lamp broke during the transit and the company happily sent over a new one. Came within a few days as well. It was easy to put together.  I had a missing part, so I contacted their support and they very quickly got me the missing piece! Lumina seems to me to be a great company that cares about their customers and products!!'''" } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" }
```

<!-- missing-text -->

<!-- missing-text -->

 Example

This example detects product and company names from the customer review.

<!-- missing-text -->

 Example

This example performs multiple tasks in a single query.

```
{ messages":[ " { "role":"user", "content": "Identify the following items from the review text: Sentiment (positive or negative) - Is the reviewer expressing anger? (true or false) - Item purchased by reviewer - Company that made the item The review is delimited with triple backticks. Format your response as a JSON object with 'Sentiment', 'Anger', 'Item'and 'Brand' as the keys. If the information isn't present, use 'unknown' as the value. Make your response as short as possible. Review text: '''Needed a nice lamp for my bedroom, and this one had additional storage and not too high of a price point. Got it fast.  The string to our lamp broke during the transit and the company happily sent over a new one. Came within a few days as well. It was easy to put together.  I
```

<!-- missing-text -->

 Example

This example identifies the five topics discussed in a story.

{ messages":[ " { "role":"user", "content": "Determine five topics that are being discussed in the following text, which is delimited by triple backticks. Make each item one or two words long. Format your response as a list of items separated by commas. Text sample: '''In a recent survey conducted by the government, public sector employees were asked to rate their level of satisfaction with the department they work at. The results revealed that NASA was the most popular department with a satisfaction rating of 95%. One NASA employee, John Smith, commented on the findings, stating, 'I'm not surprised that NASA came out on top. It's a great place to work with amazing people and incredible opportunities. I'm proud to be a part of such an innovative organization.' The results were also welcomed by NASA's management team, with Director Tom Johnson stating, 'We are thrilled to hear that our employees are satisfied with their work at NASA. We have a talented and dedicated team who work tirelessly to achieve our goals, and it's fantastic to see that their hard work is paying off.' The survey also revealed that the Social Security Administration had the lowest satisfaction rating, with only 45% of employees ndicating they were satisfied with their job. The government has pledged to address the concerns raised by employees in the survey and work towards improving job satisfaction across all departments. '''" } "frequency_penalty": 0,

```
], "max_tokens": 100, "temperature": 0.0, "presence_penalty": 0, "stop": "null" }
```

<!-- missing-text -->

Transformations

Transformations transform a given text into another language or register.

Procedure

Send a POST request to the relevant endpoint.

Include your query in the body. Mark the text to be transformed with triple back ticks (').

<!-- missing-text -->

```
 Example This example translates text from English to Spanish. { messages":[ " { "role":"user", "content": "Translate the following English text to Spanish: '''Hi, I would like to order a blender'''" } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" }
```

<!-- missing-text -->

 Example

This example detects the language that the text is written in.

<!-- missing-text -->

 Example

This example translates the given text into multiple languages.

```
{ messages":[ " { "role":"user", "content": "Translate the following  text to French and Spanish and English pirate: '''I want to order a basketball'''" }
```

<!-- missing-text -->

 Example

This example translates both the language and register of the text.

<!-- missing-text -->

 Example

<!-- missing-text -->

 Example

This example translates between output formats. The prompt describes both the input and output format.

```
{ messages":[ " { "role":"user", "content": "Translate the following python dictionary from JSON to an HTML table with column headers and title: { resturant employees\" : [     {name\":Shyam\", email\":shyamjaiswal@gmail.com\"}, {name\":Bob\", email\":bob32@gmail.com\"},    {name\":Jai\", email\":jai87@gmail.com\"}]}" } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" }
```

<!-- missing-text -->

 Example

In these examples, a text is proofread. The text can be proofread and corrected, or simply proofread.

{

messages":[ "

{

"role":"user",

"content": "Proofread and correct the following text and rewrite

the corrected version. If you don't find and errors, just say No errors

found\". Don't use any punctuation around the text:  The girl with the black

and white puppies have a ball."

}

],

"max_tokens": 100,

"temperature": 0.0,

"frequency_penalty": 0,

"presence_penalty": 0,

"stop": "null"

}

{

"

messages":[

{

"role":"user",

"content": "Proofread and correct the following text and rewrite

the corrected version. If you don't find and errors, just say No errors

found\". Don't use any punctuation around the text:  Yolanda has her

notebook."

}

],

l

<!-- missing-text -->

Expansions

Expansions generate text based on a prompt.

Procedure

Send a POST request to the relevant endpoint.

Include your query in the body.

 Example

This example generates an automated reply to a customer email.

{ messages":[ " { "role":"user", "content": "You are a customer service AI assistant. Your task is to send an email reply to a valued customer. Given the customer email delimited by ''', Generate a reply to thank the customer for their review. If the sentiment is positive or neutral, thank them for their review. If the sentiment is negative, apologize and suggest that they can reach out to customer service. Make sure to use specific details from the review. Write in a concise and professional tone. Sign the email as 'AI customer agent'. Customer review: '''So, they still had the 17 piece system on seasonal sale for around $49 in the month of November, about half off, but for some reason (call it price gouging) around the second week of December the prices all went up to about anywhere from between $70-$89 for the same system. And the 11 piece system went up around $10 or so in price also from the earlier sale price of $29. So it looks okay, but if you look at the base, the part where the blade locks into place doesn't look as good as in previous editions from a few years ago, but I plan to be very gentle with it (example, I crush very hard items like beans, ice, rice, etc. in the blender first then pulverize them in the serving size I want in the blender then switch to the whipping blade for a finer flour, and use the cross cutting blade first when making smoothies, then use the flat blade if I need them finer/less pulpy). Special tip when making smoothies, finely cut and freeze the fruits and vegetables (if using spinach-lightly stew soften the spinach then freeze until ready for use-and if making sorbet, use a small to medium sized food processor) that you plan to use that way you can avoid adding so much ice if at allwhen making your smoothie. After about a year, the motor was making a funny noise. I called customer service but the warranty expired already, so I had to buy another one. FYI: The overall quality has gone done in these types of products, so they are kind of counting on brand recognition and consumer loyalty to maintain sales. Got it in about two days.''' Review sentiment: negative" } ], "max_tokens": 100, "temperature": 0.0, "frequency_penalty": 0, "presence_penalty": 0, "stop": "null" }

<!-- missing-text -->

Chatbot

Context

Chatbots use input and give output in the form of conversations.

Procedure

Send a POST request to the relevant endpoint.

Include your query in the body. To provide more context or set a precedent, include examples of the desired outputs in your prompt.

<!-- missing-text -->

```
 Example The following examples include few shot prompts for a chatbot. { messages":[ " { "role": "system", "content": "You are an assistant that speaks like Shakespeare." }, { "role": "user", "content": "tell me a joke" }, { "role": "assistant", "content": "Why did the chicken cross the road" }, { "role": "user", "content": "I dont know" } ],
```

<!-- missing-text -->

<!-- missing-text -->

8.4 Prompt Registry

Manage the life cycle of your prompts, from design to runtime.

Prompt registry integrates prompt templates into SAP AI Core, making them discoverable across your applications and orchestration.

It reduces the complexity of dealing with prompt templates and leveraging integration capabilities.

The Prompt Registry consists of the following interfaces:

 · The imperative API supports full CRUD operations and is recommended for refining prompt templates in design time use cases. Iterations are tracked and can be viewed in the history endpoint. For more information, see Create a Prompt Template (Imperative) [page 143].

 · The declarative API utilizes SAP AI Core applications and is recommended for runtime application use cases and CI/CD pipelines.. You can manage your prompt templates in a git repository declaratively. Your commits are automatically synchronized with the Prompt Registry For more information, see Create a Prompt Template (Declarative) [page 145].

The interfaces can all be consumed either directly via the API or through Orchestration.

8.4.1  Create a Prompt Template (Imperative)

Prerequisites

Context

 Recommendation

The imperative API supports full CRUD operations and is recommended for refining prompt templates in design time use cases. Iterations are tracked and can be viewed in the history endpoint.

You can create a reusable prompt for a specific use case, including placeholders that are filled later.

Procedure

Create a prompt template by sending a POST request to endpoint {{apiurl}}/lm/promptTemplates .

```
 Sample Code curl -X POST "{{apiurl}}/lm/promptTemplates"\ -header 'AI-Resource-Group: <Resource Group Id>' \ ---header 'Content-Type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --data '{ "name": "example-prompt-template", "version": "0.0.1", "scenario": "categorization", "spec": { "template": [ {
```

```
"role": "system", "content": "You classify input text into the two following categories: {{?categories}}" }, { "role": "user", "content": "{{?inputExample}}" } ], "defaults": { "categories": "Finance, Tech, Sports" }, "additionalFields": { "modelParams": { "temperature": 0.7, "max_tokens": 100 }, "modelGroup": "chat" } } }'
```

 · The defaults and additionalFields fields are optional.

 · The additionalFields field is unstructured and can be used to store metadata or configuration objects.



```
Output Code { "id": "9y02-ha9x-92b1-4255", "name": "example-prompt-template", "versionId": "0.0.1", "message": "Prompt created successfully." }
```

Next Steps

You can iterate over the prompt template, and make changes to it.

You can save your changes as a new version. The latest iteration is always the head version and is marked with isVersionHead: true .

```
 Tip You'll need to update the version number when you want to create a new version, and the version entry must be semver compliant.
```

You can check the change history for the template. For more information, see Get Prompt Template History [page 147].

8.4.2  Create a Prompt Template (Declarative)

Prerequisites

 · You have created and synced a git repository. For more information, see Create an Application to Sync Your Folders.

Context

 Recommendation

The declarative API utilizes SAP AI Core applications and is recommended for runtime application use cases and CI/CD pipelines. You can manage your prompt templates in a git repository declaratively. Your commits are automatically synchronized with the prompt registry.

Procedure

Create a prompt template and push it to your git repository. The file name must be in the format &lt;name&gt;.prompttemplate.ai.sap.yaml to be recognised.

The following formats are supported:

 · yaml

```
 Sample Code name: simple ersion: 0.0.1 v scenario: my-scenario spec: template: - role: "system" content: "{{ ?instruction }}" - role: "user" content: "Some more {{ ?user_input }}" defaults: instruction: "default instruction" additionalFields: isDev: true validations: required: true blockedModels: - name: "gpt-4" versions: "gpt-4-vision" - name: "gpt-4o" versions: "*"
```

 · The defaults and additionalFields fields are optional.

 · The additionalFields field is unstructured and can be used to store metadata or configuration objects.

Your template will sync automatically. After a few minutes, you will be able to verify your template by sending a GET request to the endpoint {{apiurl}}/lm/promptTemplates .

```
 Sample Code { "count": 3, "resources": [ { "id": "a460d210-df38-4867-9535-7a556701a4b0", "name": "simple", "version": "0.0.1", "scenario": "my-scenario", "creationTimestamp": "2024-08-18T14:50:17.157000", "managedBy": "declarative", "isVersionHead": true }, {...} ] }
```

The prompt template is marked as managedBy &lt;declarative&gt; : . Declarative managed prompt templates are always the head version and cannot be edited with the imperative API.

Next Steps

You use your prompt template at run time. For more information, see Use a Prompt Template [page 149].

You can make changes to your template locally, and push them to git.

```
 Tip For declaratively managed templates, use history capabilities in git to track your changes.
```

8.4.3  Get a Prompt Template

Prerequisites

 · You have created a prompt template. For more information, see Create a Prompt Template (Imperative) [page 143] and Create a Prompt Template (Declarative) [page 145].

Context

You can retrieve a prompt template by ID, or by the combination of name, scenario, and version.

Prompt templates can also be retrieved and consumed in orchestration. For more information, see Templating [page 172].

 Note

Retrieval by ID offers immutability; guaranteeing that the prompt template behind the ID will not change.

Retrieval by name, scenario, and version are not immutable, and the latest iteration of the prompt template is retrieved.

Procedure

Send a GET request to the endpoint {{apiurl}}/lm/promptTemplates , and include the information for your chosen retrieval method.

```
 Sample Code curl -X GET "{{apiurl}}/lm/promptTemplates?name=example-prompttemplate&scenario=categorization&version=0.0.1" \ -Header "Authorization: Bearer <your_auth_token>"
```

```
 Sample Code curl -X GET "{{apiurl}}/lm/promptTemplates/{{promptTemplateId}}" \ Header "Authorization: Bearer <your_auth_token>" -
```

8.4.4  Get Prompt Template History

Context

You can list the history of edits to prompt templates, for imperatively managed prompt templates only.

Procedure

Send a GET request to endpoint {{apiurl}}/lm/scenarios/{{scenarioId}}/promptTemplates/ {{promptTemplateName}}/versions/{{versionId}}/history and include the name, scenario, and version of your prompt template.



```
Sample Code curl -X GET "{{apiurl}}/lm/scenarios/{{scenarioId}}/promptTemplates/
```

```
{{promptTemplateName}}/versions/{{versionId}}/history" \ -Header "Authorization: Bearer <your_auth_token>"
```

 Output Code

```
{ "count": 10, "resources": [ { "id": "8y02-ha9x-92b1-4255", "name": "example-prompt-template", "version": "0.0.1", "scenario": "categorization", "managedBy": "imperative", "isVersionHead": true, "creationTimestamp": "2021-09-29T14:00:00Z", "spec": { "template": [ { "role": "system", "content": "You classify input text into the two following categories: {{?categories}}" }, { "role": "user", "content": "{{?inputExample}}" } ], "defaults": { "categories": "Finance, Tech, Sports" }, "additionalFields": { "modelParams": { "temperature": 0.7, "max_tokens": 100 }, "modelGroup": "chat" } } } ] }
```

8.4.5  Use a Prompt Template

Prerequisites

 · You have created a prompt template. For more information, see Create a Prompt Template (Imperative) [page 143].

Procedure

You can fill a prompt template by ID, or by the combination of name, scenario, and version.

 · To fill a prompt template by ID, send a POST request to the endpoint {{apiurl}}/lm/scenarios/{{scenarioId}}/promptTemplates/ {{promptTemplateName}}/versions/{{versionId}}/substitution and add your variable values in the body.

```
·  Sample Code curl -X POST "{{apiurl}}/lm/promptTemplates/{{promptTemplateId}}/ substitution" \ -H "Authorization: Bearer <your_auth_token>" \ -H "Content-Type: application/json" \ -d '{ "inputParams": { "inputExample": "Sample text" } }'
```

 · To fill a prompt template by name, scenario, and version send a POST request to the endpoint {{apiurl}}/lm/promptTemplates/{{promptTemplateId}}/substitution and add your variable values in the body.

```
 Sample Code curl -X POST "{{apiurl}}/lm/scenarios/{{scenarioId}}/promptTemplates/ {{promptTemplateName}}/versions/{{versionId}}/substitution" \ -H "Authorization: Bearer <your_auth_token>" \ -H "Content-Type: application/json" \ -d '{ "inputParams": { "inputExample": "Sample text" } }'
```

To return a response containing the full prompt template definition, include the optional query parameter

metadata = &lt;true&gt;

8.4.6  Import a Prompt Template

Prerequisites

 · You have created and exported a prompt template. For more information, see Create a Prompt Template (Imperative) [page 143] and Export a Prompt Template [page 151].

Context

You can import a declarative prompt template as a single file export in yaml format.

Procedure

Send a POST request to the endpoint {{apiurl}}/lm/promptTemplates/import , and include the name of your file in the file parameter.

```
 Sample Code curl -X POST "{{apiurl}}/lm/promptTemplates/import" \ -Header "Authorization: Bearer <your_auth_token>" \ -Header "Content-Type: multipart/form-data" \ -File "file=@/path/to/your/file.yaml"
```

```
 Output Code "message": "Prompt <prompt-name> for version <version-number> imported
```

```
{ successfully.", "resource": { "name": "example-prompt-template", "version": "0.0.1", "scenario": "job-description", "spec": { "template": [ { "role": "system", "content": "You classify input text into the two following categories: {{?categories}}" }, { "role": "user", "content": "{{?inputExample}}" } ], "defaults": { "categories": "Finance, Tech, Sports" },
```

```
"additionalFields": { "modelParams": { "temperature": 0.7, "max_tokens": 100 }, "modelGroup": "chat" } } } }
```

8.4.7  Export a Prompt Template

Prerequisites

 · You have created a prompt template. For more information, see Create a Prompt Template (Imperative) [page 143].

Context

You can export a prompt template as a single file export in declarative compatible yaml format.

Procedure

Send a GET request to the endpoint {{apiurl}}/lm/promptTemplates/{{promptTemplateId}}/ export , and include the name of your file in the output parameter.

```
 Sample Code curl -X GET "{{apiurl}}/lm/promptTemplates/{{promptTemplateId}}/export" \ Header "Authorization: Bearer <your_auth_token>" \ --output "exported_prompt_template.yaml" -
```



```
Output Code name: example-prompt-template ersion: 0.0.1 v scenario: job-description spec: template: - role: 'system' content: 'You classify input text into the two following categories: {{?categories}}'
```

```
- role: 'user' content: '{{?inputExample}}' defaults: categories: Finance, Tech, Sports additionalFields: modelParams: temperature: 0.7 max_tokens: 100 modelGroup: chat collection: abc
```

8.4.8  Delete a Prompt Template

Prerequisites

 · You have created a prompt template. For more information, see Create a Prompt Template (Imperative) [page 143].

 · You know the template ID of the prompt template that you want to delete. For more information, see Get a Prompt Template [page 146].

Context

Delete a specific version of the prompt template, for imperatively managed prompt templates only.

Procedure

Send a DELETE request to the endpoint {{apiurl}}/lm/promptTemplates/{{promptTemplateId}} and include the template ID of the prompt you want to delete.

```
 Output Code { "message": "Prompt deleted successfully." }
```

8.5 AI Data Management

8.5.1  Create a Resource Group for AI Data Management

Context

```
 Note Resource group Ids must be of length minimum: 3, maximum: 253. The first and last characters must penultimate, may include a lower case letter, an upper case letter, a number, a period (.), or a hyphen (-).
```

be either a lowercase letter, an uppercase letter, or a number. Character entries from the second to No other special characters are permitted.

Procedure

Create a resource group by sending a curl request, including the label that makes the resource group available to the grounding service.

```
curl --location --request POST "$AI_API_URL/v2/admin/resourceGroups" -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ --data-raw '{ "resourceGroupId": "{{resource_group}}",   // Resource group ID to manage document grounding "labels": [                                // Labels for specifying resource purpose { "key": "ext.ai.sap.com/document-grounding",  // Key for Document Grounding feature "value": "true"                              // Enable Document Grounding } ] ' }
```

Existing resource groups can be made available to the grounding service using a PATCH request, including the document-grounding label.

Next Steps

To edit a resource group, see Edit a Resource Group.

To delete a resource group, see Delete a Resource Group.

8.5.2  Create a Generic Secret for AI Data Management

Prerequisites

Your have prepared your SharePoint integration. For more information, see Prepare SharePoint Integration with Joule.

Procedure

Send a POST request and enter the URL {{apiurl}}/v2/admin/secrets .

 · AI-Tenant-Scope : true . The operation will be performed at the main tenant level.

 · AI-Resource-Group : &lt;resource-group-name&gt; . The operation will be performed at the resourcegroup level.

The following API request is used to create a generic secret with base64 encoded values, such as client credentials and other necessary fields, for integrating grounding in generative AI hub. It uses OAuth2 Password Authentication to generate access tokens.

```
curl --location --request POST "$AI_API_URL/v2/admin/secrets" \ -header "Authorization: Bearer $TOKEN" \ ---header 'Content-Type: application/json' \ --header 'AI-Resource-Group: default' \ --data-raw '{ "name": "<generic secret name>",                      // Name of the generic secret to be created "data": { "type": "SFRUUA==",                                 // Base64 encoded value for HTTP protocol "description": "<description of generic secret>",   // Base64 encoded description of the secret "clientId": "<client id>",                          // Base64 encoded value of client ID "authentication": "T0F1dGgyUGFzc3dvcmQ=",           // Base64 encoded value for OAuth2Password authentication "tokenServiceURL": "<token service url>",           // Base64 encoded URL for the token service "password": "<password>",                           // Password (Base64 encoded) "proxyType": "SW50ZXJuZXQ=",                        // Base64 encoded value for Internet as proxy type "url": "aHR0cHM6Ly9ncmFwaC5taWNyb3NvZnQuY29t",      // Base64 encoded value for https://graph.microsoft.com "tokenServiceURLType": "RGVkaWNhdGVk",              // Base64 encoded value for Dedicated token service URL type "user": "<user>",                                   // Base64 encoded value for user "clientSecret": "<client secret>",                  // Base64 encoded value for client secret "scope": "aHR0cHM6Ly9ncmFwaC5taWNyb3NvZnQuY29tLy5kZWZhdWx0" // Base64 encoded scope for https://graph.microsoft.com/.default }, "labels": [ {
```

```
"key": "ext.ai.sap.com/document-grounding",       // Label for Document Grounding feature "value": "true" } ] ' }
```

 Note

The secret name is written without hyphens to make it simple to consume as a Unix environment variable later. It is written in capitals as is convention.

Next Steps

To edit a generic secret, see Edit a Generic Secret.

To delete a generic secret, see Delete a Generic Secret.

8.5.3  Pipeline API

This API call creates a pipeline for indexing documents.

Prerequisites

You have created a resource group for grounding purposes. For more information, see Create a Resource Group for AI Data Management [page 153]

You have created a generic secret for grounding purposes. For more information, see Create a Generic Secret for AI Data Management [page 154]

Context

This pipeline segments data into chunks and generates embeddings, which are multidimensional representations of textual information. The embeddings are stored in a vector database.

The following document stores are compatible:

 · Microsoft SharePoint

 · Maximum of 2000 documents

 · Content updated once per day

The following document formats are compatible:

 · .pdf

 · .docx

The following document content formats are compatible:

 · Plain text

For region and pricing information, see SAP Note 350347 .

Procedure

The following example shows a request using Microsoft SharePoint:

```
 Sample Code curl --request POST \ -url $AI_API_URL/v2/lm/document-grounding/pipelines \ ---header 'AI-Resource-Group: {{resource_group}}' \ --header 'Content-Type: application/json' \ --data '{ "type": "MSSharePoint", "configuration": { "destination": "<generic secret name>", "sharePoint": { "site": { "name": "<site name>"  // The name of the SharePoint site to be indexed } } } }'
```

```
 Sample Code
```

```
curl --request POST \ --url $AI_API_URL/v2/lm/document-grounding/pipelines \ --header 'AI-Resource-Group: {{resource_group}}' \ --header 'Content-Type: application/json' \ --data '{ "type": "MSSharePoint",  // Currently, only Microsoft SharePoint is supported "configuration": { "destination": "<generic secret name>",  // Generic secret name created in the previous step "sharePoint": { "site": { "name": "<site name>",  // The name of the SharePoint site to be indexed "includePaths": ["<folder1>", "<folder2>"]  // [Optional] Array of folders within the SharePoint site for selective indexing } } } }'
```

The includePaths is optional. It takes an array of folder paths within the SharePoint site to target for indexing and limits the scope of the pipeline to the specified subfolders or files, meanind that only this content is indexed.

Next Steps

You can retrieve details of indexing pipelines in the following ways:

 · Get all pipelines

```
 Sample Code curl --request GET \  # Use GET method to retrieve all pipelines --url $AI_API_URL/v2/lm/document-grounding/pipelines \  # URL to fetch all pipelines --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

 · Get Pipeline Details by ID

```
 Sample Code url --request GET \  # Use GET method to retrieve pipeline details by c pipeline ID --url $AI_API_URL/v2/lm/document-grounding/pipelines/{{pipelineId}} \  # URL to fetch details of a specific pipeline --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

 · Get Pipeline Status by ID

```
 Sample Code url --request GET \  # Use GET method to check the status of a pipeline c
```

```
--url $AI_API_URL/v2/lm/document-grounding/pipelines/{{pipelineId}}/ status \  # URL to fetch the status of a specific pipeline --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

You can delete pipelines in the following ways:

 · by ID

```
 Sample Code curl --request DELETE \  # Use DELETE method to remove a pipeline --url $AI_API_URL/v2/lm/document-grounding/pipelines/{{pipelineId}} \  # URL to delete a specific pipeline by pipeline ID --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

8.5.4  Vector API

Vector API is a microservice provided with a Rest API and endpoints for creating and managing collection and documents.

Prerequisites

You have created a resource group for grounding purposes. For more information, see Create a Resource Group for AI Data Management [page 153].

You have created a generic secret for grounding purposes. For more information, see Create a Generic Secret for AI Data Management [page 154].

Next Steps

You can create a document collection. For more information see Create a Collection [page 158].

You can create a document collection. For more information see Create a Document [page 162].

8.5.4.1 Collections

8.5.4.1.1 Create a Collection

Prerequisites

You have created a resource group for grounding purposes. For more information, see Create a Resource Group for AI Data Management [page 153]

You have created a generic secret for grounding purposes. For more information, see Create a Generic Secret for AI Data Management [page 154]

Context

You can add a new document collection in the SAP HANA Vector Database for document grounding in generative AI hub.

Procedure

Send a POST request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/collections

```
 Sample Code url --request POST \  # Use POST method to create a collection c --url $AI_API_URL/v2/lm/document-grounding/vector/collections \  # Endpoint to create a new collection --header 'AI-Resource-Group: {{resource_group}}' \  # Resource group ID for targeting the collection's location --data '{ "title": "<title of the collection>",  // Define collection title "embeddingConfig": { "modelName": "<embedding-model-name>" }, "metadata": [ { "key": "purpose", "value": [ "<text>"  // Purpose of this collection ] }, { "key": "<a-random-key>", "value": [ "<text>"  // Example metadata value ] } ] ' }
```

8.5.4.1.2 Get Collection Creation Status

Context

This API call checks the current status of a collection creation process using the collection ID. For more information, see Get Collections [page 160].

Procedure

Send a GET request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{id}}/creationStatus

```
 Sample Code url --request GET \  # Use GET method to check collection creation status c
```

```
--url $AI_API_URL/v2/lm/document-grounding/vector/collections/{{id}}/ creationStatus \  # Endpoint to get the status of collection creation --header 'AI-Resource-Group: {{resource_group}}'  # Include the resource group ID in the header
```

8.5.4.1.3 Get Collections

Context

Retrieve details of all collections within a specified resource group.

Procedure

Send a GET request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/collections

```
 Sample Code url --request GET \  # Use GET method to retrieve the list of all collections c --url $AI_API_URL/v2/lm/document-grounding/vector/collections \  # Endpoint to get all collections --header 'AI-Resource-Group: {{resource_group}}'  # Include the resource group ID in the header
```

8.5.4.1.4 Get a Collection

Context

Retrieve details of a specific collection within a specified resource group.

Procedure

Send a GET request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/collections



```
Sample Code url --request GET \ c --url $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}} \  # URL to fetch a collection using collectionId --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

8.5.4.1.5 Delete a Collection

Procedure

Send a DELETE request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/ collections/{{collectionId}}  Sample Code url --request DELETE \  # Use DELETE method to remove the collection c --header 'AI-Resource-Group: {{resource_group}}'  # Specify the resource

```
--url $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}} \  # URL with the collection ID group ID
```

8.5.4.1.6 Get Collection Deletion Status

Context

Check the current status of a collection deletion. For more information, see Get Collections [page 160].

Procedure

Send a GET request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{id}}/deletionStatus

```
 Sample Code url --request GET \  # Use GET method to check deletion status c
```

```
--url $AI_API_URL/v2/lm/document-grounding/vector/collections/{{id}}/ deletionStatus \  # URL with collection ID for status check --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

8.5.4.2 Documents

8.5.4.2.1 Create a Document

Context

Aadd a document with associated metadata and chunks, to a collection. For more information, see Get Collections [page 160].

Procedure

Send a POST request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/ collections/{{collectionId}}/documents

 Sample Code

```
url --request POST \  # Use POST method to create a document c --url $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}}/documents \  # Endpoint to add a document to a collection --header 'AI-Resource-Group: {{resource_group}}' \  # Include the resource group ID in the header --header 'Content-Type: application/json' \ --data '{ "documents": [ { "metadata": [  // Metadata for the document { "key": "url", "value": [ "http://hello.com", "123" ] } ], "chunks": [  // Content chunks within the document { "content": "<chunk content 1>",  // Text content of chunk 1 "metadata": [  // Metadata for chunk 1 { "key": "index", "value": [ "1" ] }
```

```
] }, { "content": "<chunk content 2>",  // Text content of chunk 2 "metadata": [  // Metadata for chunk 2 { "key": "index", "value": [ "2" ] } ] } ] } ] ' }
```

8.5.4.2.2 Update a Document

Context

Procedure

Send a POST request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/ collections/{{collectionId}}/documents

```
 Sample Code url --request PATCH \  # Use PATCH method to update a document c --url $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}}/documents \  # Endpoint to update a document in a collection --header 'AI-Resource-Group: {{resource_group}}' \  # Include the resource group ID in the header --header 'Content-Type: application/json' \ --data '{ "documents": [ { "id": {{documentId}},  // Document ID of the document to be updated "metadata": [  // Metadata to update { "key": "url", "value": [ "http://hello.com", "123" ] } ], "chunks": [  // Array of document chunks {
```

```
"content": "<This is an updated document chunk content>",  // Updated content for chunk 1 "metadata": [  // Updated metadata for chunk 1 { "key": "index", "value": [ "1" ] } ] }, { "content": "<No update to chunk 2>",  // Content for chunk 2 remains unchanged "metadata": [  // Metadata for chunk 2 remains unchanged { "key": "index", "value": [ "2" ] } ] } ] } ] ' }
```

8.5.4.2.3 Get All Documents in a Collection

Context

Retrieve details of all documents within a collection. For more information, see Get Collections [page 160].

Procedure

Send a GET request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}}/documents

```
 Sample Code curl --request GET \  # Use GET method to retrieve documents --url $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}}/documents \  # URL to fetch all documents in the collection --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

8.5.4.2.4 Get A Document

Context

Retrieve details of a document within a collection. For more information, see Get Collections [page 160] and Get All Documents in a Collection [page 164].

Procedure

Send a GET request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}}/documents/{{documentId}}



```
Sample Code url --request GET \  # Use GET method to retrieve a specific document c --url $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}}/documents/{{documentId}} \  # URL to fetch a specific document --header 'AI-Resource-Group: {{resource_group}}'  # Add the resource group ID in the header
```

8.5.4.2.5 Vector Call Search

Context

Search for relevant chunks in a collection or across all collections based on a user query. The response contains chunks that match the query, filtered by collection and document metadata.

Procedure

Send a: POST request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/search

```
 Sample Code curl --request POST \  # Use POST method to perform search --url $AI_API_URL/v2/lm/document-grounding/vector/search \  # URL for vector search
```

```
--header 'AI-Resource-Group: {{resource_group}}' \  # Specify resource group ID --header 'Content-Type: application/json' \  # Define content type as JSON --data '{ "query": "is Joule an AI Copilot?", "filters": [ { "id": "String", "collectionIds": [ "*" OR collection_id # Search across all collections OR Specify specific collection ID ], "configuration": {}, "collectionMetadata": [], "documentMetadata": [ { "key": "url", "value": [ "http://hello.com", "1234" ] } ], "chunkMetadata": [] } ] ' }
```

8.5.4.2.6 Delete Documents

Procedure

Send a DELETE request to the endpoint: $AI_API_URL/v2/lm/document-grounding/vector/ collections/{{collectionId}}/documents/{{documentId}}



```
Sample Code url --request DELETE \  # Use DELETE method to remove the document c --url $AI_API_URL/v2/lm/document-grounding/vector/collections/ {{collectionId}}/documents/{{documentId}} \  # URL with collection ID and document ID --header 'AI-Resource-Group: {{resource_group}}'  # Specify the resource group ID
```

8.5.5  Retrieval API

The retrieval API searches data repositories and returns the relevant chunks for the user query.

Prerequisites

You have created a resource group for grounding purposes. For more information, see Create a Resource Group for AI Data Management [page 153]

You have created a generic secret for grounding purposes. For more information, see Create a Generic Secret for AI Data Management [page 154]

Context

The following repository types are supported:

 · vector

8.5.5.1 Get a Repository

Prerequisites

You have created a resource group for grounding purposes. For more information, see Create a Resource Group for AI Data Management [page 153]

You have created a generic secret for grounding purposes. For more information, see Create a Generic Secret for AI Data Management [page 154]

Context

Procedure

Send a GET request to the endpoint $AI_API_URL/v2/lm/document-grounding/retrieval/ dataRepositories

To search within a specific repository specify it's repositoryid in dataRepositories attribute of the request or specify * to search in all the repositories.

Example for all repositories:

```
 Sample Code curl --request GET \  # Use GET method to retrieve all data repositories --url $AI_API_URL/v2/lm/document-grounding/retrieval/dataRepositories \  # Endpoint to retrieve all data repositories --header 'AI-Resource-Group: {{resource_group}}'  # Add resource group ID in the header for context
```

Example for a repository by ID:

```
 Sample Code curl --request GET \ --url $AI_API_URL/v2/lm/document-grounding/retrieval/dataRepositories/ {{repositoryId}} \ --header 'AI-Resource-Group: {{resource_group}}'  # Replace with your actual resource group ID
```

Results

```
 Output Code { "resources": [ { "id": "<respositoryid>", "title": "<repository title>", "metadata": [ // repository metadata { "key": "pipeline", "value": [ "137f3100-9171-48fb-b8d4-f93804bf7bac" ] }, { "key": "type", "value": [ "custom" ] }, { "key": "pipelineType", "value": [ "MSSharePoint" ] } ], "type": "vector" }, { "id": "<respositoryid>", "title": "<repository title>", "metadata": [
```

```
{ "key": "purpose", "value": [ "demonstration" ] }, { "key": "a-random-key", "value": [ "hello world!" ] } ], } ], "count": 2 }
```

8.5.5.2 Retrieval Search call

The retrieval search call searches data repositories and returns the relevant chunks for the user query.

Prerequisites

You have created a resource group for grounding purposes. For more information, see Create a Resource Group for AI Data Management [page 153]

You have created a generic secret for grounding purposes. For more information, see Create a Generic Secret for AI Data Management [page 154]

Context

The following repository types are supported:

 · vector

Procedure

Send a POST request to the endpoint $AI_API_URL/v2/lm/document-grounding/retrieval/search

To search within a specific repository specify it's repositoryid in dataRepositories attribute of the request or specify * to search in all the repositories.

For example:

 Sample Code

```
url --request POST \  # Use POST method to initiate the search query in the c retrieval API --url $AI_API_URL/v2/lm/document-grounding/retrieval/search \  # Specify the endpoint URL to trigger the search in data repositories --header 'AI-Resource-Group: {{resource_group}}' \  # Add the resource group ID in the header to specify the resource group for the query --data '{ "query": "<user query to look for an answer in vector database>",  # User query that will be used to search the data repository "filters": [  # Optional filters to customize the search, such as the repository and metadata constraints { "id": "string",  # Identifier for the filter, can be a custom string to track or categorize the filter "searchConfiguration": { "maxChunkCount": 1  # Limit the number of chunks returned in the search results (e.g., 1 chunk) }, "dataRepositories": [ "<data repository id or '*' >"  # Specify a specific repository ID or use '*' to search across all repositories ], "dataRepositoryType": "vector",  # The type of repository to search in, can be either "vector" "dataRepositoryMetadata": [  # Optional metadata filters for data repositories to refine the search {  # Example metadata filter for data repository "key": "type",  # The metadata key to filter on "value": [ "custom"  # Metadata value for the key (can filter by specific repository types, e.g., "custom") ] } ], "documentMetadata": [  # Optional metadata filters for the documents within the repository {  # Example document metadata filter "key": "url",  # The metadata key for document (e.g., filtering based on document URL) "value": [ "http://hello.com",  # The value to match (e.g., URL of the document) "123"  # Another value for URL filter or other document identifiers ] } ], "chunkMetadata": [  # Optional metadata filters for chunk-level metadata {  # Example chunk metadata filter "key": "index",  # The metadata key for chunk (e.g., filtering based on chunk index) "value": [ "1"  # The value to match (e.g., retrieve chunk with index 1) ] } ] } ] }'
```

8.6 Orchestration

Orchestration combines content generation with a set of functions that are often required in business AI scenarios.

Functions include:

 · Templating, which lets you compose a prompt with placeholders that are filled during inference

 · Content filtering, which lets you restrict the type of content that is passed to and received from a generative AI model

 · Data Masking, which lets you mask data via anonymization or pseudonymization before passing it into a genereative AI model. In case of pseudonymization masked data present in the generative AI model response will be unmasked.

 · Gounding, which lets you integrate external, contextually relevant, domain-specific, or real-time data into AI processes. This data supplements the natural language processing capabilities of pre-trained models, which are trained on general material.

8.6.1  Orchestration Workflow

In a basic orchestration scenario, you can combine different modules from orchestration into a pipeline that can be executed with a single API call. Within the pipeline, the response from one module is used as the input for the next module.

The order in which the pipeline is executed is defined centrally in orchestration. However, you can configure the details for each module and omit optional modules by passing an orchestration configuration in JSON format with the request body.

The image below is interactive. Click each part of the workflow for more information.

<!-- missing-text -->

Model Configuration

 · Grounding [page 172]

 · Templating [page 172]

 · Data Masking [page 174]

 · Input Filtering [page 175]

 · Model Configuration [page 176]

 · Output Filtering [page 176]

8.6.1.1 Grounding

Grounding integrates external, contextually relevant, domain-specific, or real-time data into AI processes. This data supplements the natural language processing capabilities of pre-trained models, which are trained on general material.

Grounding is a service designed to handle data-related tasks, for example grounding and retrieval, using vector databases.

The Pipeline API is proxied through the SAP AI Core generative AI hub, and incorporates vector stores, such as the managed HANA database.

Grounding provides specialized data retrieval through vector databases, grounding the retrieval process using your own external and context relevant data. Grounding combines generative AI capabilities with the capacity to use real-time, precise data to improve decision-making and business operations, for specific business AI driven solutions.

8.6.1.2 Templating

The templating module is mandatory. It enables you to compose prompts and define placeholders. It then generates the final query that is sent to the model configuration module. Any placeholders that you define in your prompt can be filled at runtime. For example, the following template places the input text into the {{ ?input }} placeholder:

```
"templating_module_config": { "template": [ { "role": "user", "content": "{{ ?input }}" } ] }
```

The {{ ?input }} placeholder is filled using the contents of the input_params.input parameter:

```
"input": "A sample prompt to be sent to the model"
```

```
"input_params": { }
```

You can also set default values for the placeholders. For example, to request either 5 paraphrases or a userspecific number of paraphrases for a given phrase, you can use the following configuration:

```
"templating_module_config": { "template": [ { "role": "user", "content": "Create {{ ?number }} paraphrases of {{ ?phrase }}" } ], "defaults": { "number": 5 } }, //... other configuration
```

```
"input_params": { "phrase": "Please respond as soon as possible, thanks."
```

```
"number": "3", }
```

```
Templates can also be retrieved from the prompt registry. For more informaiton, see Prompt Registry [page
```

142].

Templates can be retrieved by ID (immutable) or the combination of scenario, name, and version (mutable). For examples:

```
 Sample Code "templating_module_config":{ "template_ref":{ "id": "d22342e7-1f91-4c52-a794-04833bc2574a" } }, or templating_module_config":{ " "template_ref":{ "scenario": "<scenario>", "name": "<name>", "version": "semver version e.g. 0.0.1 or 'latest'" } , }
```

Input Image Support

Images can be provided as additional input where supported.

For example:

```
 Sample Code "template": [ {"role": "system", "content": "You are a helpful assistant."}, { "role": "user", "content": [ {"type": "text", "text": "what is in this image?"}, {"type": "image_url", "image_url": {"url": "..."}}, ], }, , ]
```

The URL for the image supports Base64 or a web based URL of the image. Check with the model provider that your prefered upload method is supported.

```
"image_url": { "url":  f"data:image/jpeg;base64,{base64_image}" }
```

```
"image_url": { "url": "https://some.jpg" }
```

8.6.1.3 Data Masking

The data masking module is optional. It anonymizes or pseudonymizes personally identifiable information from the input.

Anonymized data can not be unmasked as information about the original data is not retained. Personally identifiable information is replaced with a MASKED_ENTITY placeholder.

Pseudonymized data can be unmasked in the response. personally identifiable information is replaced with a MASKED_ENTITY_ID placeholder.

The module currently supports the following anonymization services:

 · SAP Data Privacy Integration - Anonymization

SAP Data Privacy Integration - Anonymization

SAP Data Privacy Integration - Anonymization service recognizes the following entity categories:

<!-- missing-text -->

<!-- missing-text -->

 Caution

The masking service can mask personally identifiable information in the prompt. However, because it is using automated detection mechanisms, there is no guarantee that all personally identifiable information will be found and masked.

Anonymization, replaces personally identifiable information in an irreversible manner. This results in a loss of context, which may limit the capability of the LLM to process the input. For example, if asked to write a story about Ben and Anna, with anonymization of profile-person, the LLM would be asked to write a story about MASKED_PERSON and MASKED_PERSON and could no longer distinguish between the two.

8.6.1.4 Input Filtering

The content filtering module is optional. It lets you filter the input and output based on content safety criteria.

The module supports the Azure Content Safety classification service. This service recognizes four distinct content categories: Hate Violence Sexual , , , and SelfHarm . For more information, see Harm categories in Azure AI Content Safety . T ext can have more than one label (for example, a text sample can be classified as both Hate and Violence ). The returned content categories include a severity level rating of 0, 2, 4, or 6. The value increases with the severity of the content.

 Note

For all Azure OpenAI models, a global content filter is configured on the Azure AI platform. This global filter removes all input and output that is classified as medium (4) or high (6) in any of the categories.

If no content filter is configured in the orchestration configuration, the input is passed to the model configuration without filtering. Similarly, the response is returned without filtering. If no severity levels are set in the configuration (i.e. the config key is not given), the default level of 2 (low) is used for all categories.

8.6.1.5 Model Configuration

The model configuration module is mandatory. It lets you inference the large language module by making a call to the specified LLM and returning its response. You can configure the module by passing the following information:

 · Model Name: The name of the model to be used.

 · Model Parameters: The parameters to be applied to the model.

 · Model Version: The version of the model to be used, which can either be latest or a specific version number.

8.6.1.6 Output Filtering

The content filtering module is optional. It lets you filter the input and output based on content safety criteria.

The module supports the Azure Content Safety classification service. This service recognizes four distinct content categories: Hate Violence Sexual , , , and SelfHarm . For more information, see Harm categories in Azure AI Content Safety . T ext can have more than one label (for example, a text sample can be classified as both Hate and Violence ). The returned content categories include a severity level rating of 0, 2, 4, or 6. The value increases with the severity of the content.

 Note

For all Azure OpenAI models, a global content filter is configured on the Azure AI platform. This global filter removes all input and output that is classified as medium (4) or high (6) in any of the categories.

If no content filter is configured in the orchestration configuration, the input is passed to the model configuration without filtering. Similarly, the response is returned without filtering. If no severity levels are set in the configuration (i.e. the config key is not given), the default level of 2 (low) is used for all categories.

8.6.2  Harmonized API

The harmonized API lets you use different foundation models without the need to change the client code. It does so by taking the OpenAI API as the standard and mapping other model APIs to it. This includes standardizing message formats, model parameters, and response formats. The harmonized API is integrated into the templating module, the model configuration, and the orchestration response.

In the OpenAI format, a prompt and its response can contain a list of messages with a role and content. The role can be system user , , or assistant and the content is the text of the message. The response can contain a list of choices with index messages , , and a finish_reason .

Example

When you use a Gemini model with orchestration, the OpenAI message format is translated internally to the Vertex AI message format expected by the Gemini model. Consider the following message list in OpenAI format:

```
[ { "role": "system", "content": "You are a friendly assistant." }, { "role": "user", "content": "Hello" } { "role": "assistant", "content": "Hi" } ]
```

To use this message list with a Gemini model, it is translated internally to the following:

```
{ "systemInstruction": { "role": "system", "parts": [ { "text": "You are a friendly assistant." } ] }, "contents": [ { "role": "user", "parts": [ { "text": "Hello" } ] }, { "role": "assistant", "parts": [ { "text": "Hi" }
```

```
]
```

```
} ] }
```

OpenAI model parameters include max_tokens temperature frequency_penalty presence_penalty , , , , n , and top_p . Where possible, these parameters are used with any model and mapped to the corresponding parameter of the foundation model. For example, OpenAI's max_tokens will be mapped to Vertex AI's maxOutputTokens . Additionally, model-specific parameters can be sent. For instance, you can use Vertex AI's top_k parameter, which is not available in OpenAI's API.

Responses from Vertex AI are converted back into OpenAI's format, ensuring the client receives the response in a consistent format. For example, consider the following response in Vertex AI format:

```
{ "candidates": [ { "content": { "parts": [ { "text": "Hello there! How can I assist you today?" } ], "role": "model" }, "finishReason": "STOP", "safetyRatings": [ { "category": "HARM_CATEGORY_HATE_SPEECH", "probability": "NEGLIGIBLE", "probabilityScore": 0.031439852, "severity": "HARM_SEVERITY_NEGLIGIBLE", "severityScore": 0.04509957 }, { "category": "HARM_CATEGORY_DANGEROUS_CONTENT", "probability": "NEGLIGIBLE", "probabilityScore": 0.089933015, "severity": "HARM_SEVERITY_NEGLIGIBLE", "severityScore": 0.025957357 }, { "category": "HARM_CATEGORY_HARASSMENT", "probability": "NEGLIGIBLE", "probabilityScore": 0.053799648, "severity": "HARM_SEVERITY_NEGLIGIBLE", "severityScore": 0.01711089 }, { "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "probability": "NEGLIGIBLE", "probabilityScore": 0.19117664, "severity": "HARM_SEVERITY_NEGLIGIBLE", "severityScore": 0.046378203 } ] } ], "usageMetadata": { "candidatesTokenCount": 10, "promptTokenCount": 1, "totalTokenCount": 11 } }
```

This is translated internally back into the OpenAI format, returning the following:

```
{ "choices": [ { "index": 0, "message": { "role": "assistant", "content": "Hello there! How can I assist you today?" }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 10, "prompt_tokens": 1, "total_tokens": 11 } }
```

8.6.3  Streaming

Streaming is optional. Where models support streaming, output is generated in chunks, and passed through the modules in the orchestration workflow. This is useful for applications such as chatbots, where interactions with the model happen in real time.

 Caution

Streaming can increase the number of calls, increasing consumption and associated costs. For example, if a text of 500 characters generates 5 chunks of 100 characters each, the content filtering module receives 5 calls instead of one.

Streaming can decrease the accuracy of filtering and unmasking, because smaller chunks contain less context. Benchmarking and testing with actual use case data is recommended to ensure output quality in relation to chunk size.

For more information, see Metering and Pricing for the Generative AI Hub [page 48] and SAP Note 3505347 .

Activate Streaming

Streaming is set to false by default, and is part of the orchestration_config . T o activate streaming, set stream to true . For example:

```
 Sample Code { "orchestration_config": { "stream": true, "module_configurations": { ... },
```

```
"input_params": { ... } } }
```

The chunk_size parameter defines the maximum number of characters contained in a single chunk, and takes an integer value. The default value is 100.

You can configure the chunk_size using the stream_options parameter in the orchestration_config :

The following example shows a streaming configuration with chunk size of 200 characters.

```
 Sample Code { "orchestration_config": { "stream": true, "stream_options": { "chunk_size": 200 }, "module_configurations": { ... "filtering_module_config": { "output": { "filters": [ ... ], "stream_options": { "overlap": 10 } } } ... }, "input_params": { ... } } }
```

Module Specific Streaming Behavior

The following orchestration modules use content passed from streaming:

 · Data Masking (unmasking pseudonymization only)

 · Output filtering

Data Masking

For the data masking module, the full MASKED_ENTITY_DIGIT tag is needed for unmasking. When a tag is split by chunking, the entire tag is moved to the following chunk, changing the chunk size.

For example, two chunks containing:

```
 Sample Code Yesterday, I spent time with MASKED_PER
```

```
ON_1 discussing the changes pro S
```

Would become:

```
 Sample Code Yesterday, I spent time with ASKED_PERSON_1 discussing the changes pro M
```

This may affect behavior of future modules that are executed after unmasking.

Output filtering

The overlap parameter defines the number of characters at the end of a chunk, that are repeated at the beginning of the next chunk. It takes an integer value. The default value is 0.

Increasing the context window improves output filtering results, but increasing chunk size can affect streaming experience negatively. Overlap repeats a specified amount of text across two adjacent chunks, creating a larger context window, without increasing chunk size.

For example:

```
 Sample Code "This is the text from the 1st chunk." This is the text from the 2nd chunk" "
```

Would become:

```
 Sample Code "This is the text from the 1st chunk." the 1st chunk. This is the text from the 2nd chunk" "
```

You can configure the overlap parameter using the output &gt; stream_options &gt; overlap parameter in the filtering_module_config .

The following example shows a streaming configuration with an overlap of 10 characters.

```
 Sample Code { "orchestration_config": { "stream": true, "stream_options": { "chunk_size": 200 }, "module_configurations": { ... "filtering_module_config": { "output": { "filters": [ ... ], "stream_options": { "overlap": 10 }
```

```
} } ... }, "input_params": { ... } } }
```

Streaming Response Format

Streaming responses follow the conventions used by OpenAI models and are sent as Server-Sent Events (SSEs).

The first chunk always contains the module results of all modules executed before calling the LLM.

The LLM's output starts from the second chunk, and each chunk produced by orchestration sends a data event followed by a newline.

Successful responses are terminated with [DONE] . If an error occurs, orchestration sends a data event containing an error object as payload, and doesn't terminate with [DONE] .

Example for a Successful Response

The first data event contains the module result of modules that are run before the LLM access. This example shows the templating module result, and an empty orchestration result:

```
 Output Code data: { "request_id": "...", "module_results": { "templating": [ { "role": "user", "content": "Create 3 paraphrases of I love you."} ] }, "orchestration_result": { "id": "", "object": "", "created": 0, "model": "", "system_fingerprint": "", "choices": [ { "index": 0, "delta": { "role": "", "content": "" }, "finish_reason": "" } ] } }
```

The second and following data events contain module results from the LLM and following modules. This example shows results for LLM access and orchestration. The orchestration result is given in two chunks.

First chunk:

```
 Output Code data: { "request_id": "...", "module_results": { "llm": { ... } }, "orchestration_result": { "id": "...", "object": "chat.completion.chunk", "created": 1728043660, "model": "gpt-35-turbo-16k", "choices": [ { "index": 0, "delta": { "role": "assistant", "content": "1. My affection for you knows no bounds.\n2. You hold a special place in my heart.\n3. You mean the wo" }, "finish_reason": "" } ] } }
```

Second chunk:

```
 Output Code data: { "request_id": "163ea321-8f4f-4d00-9b34-7603f1499c24", "module_results": { "llm": { ... } }, "orchestration_result": { "id": "...", "object": "chat.completion.chunk", "created": 1728043660, "model": "gpt-35-turbo-16k", "choices": [ { "index": 0, "delta": { "role": "assistant", "content": "rld to me." }, "finish_reason": "stop" } ] } }
```

The final data-event contains [DONE] :

```
 Output Code ata: [DONE] d
```

Example for an Error Response

```
 Sample Code data: {"json": "payload"} ata: {"code": 500, "message": "message"} d
```

8.6.4  Model Restriction

When creating a deployment for orchestration, you can restrict the choice of models using an allow or deny list. You can use this to implement internal standards, for example where only certain LLMs are approved for use.

To restrict model access, set the parametersBindings to contain modelFilterList and modelFilterListType .

 · modelFilterList : a list of modelNames and modelVersions to be considered. If modelVersions is not defined, all versions of the given model are considered.

 · modelFilterListType : a value that controls how modelFilterList should be interpreted.

 · deny : excludes the models and defined versions from use within orchestration.

 · allow : only allows the models and defined versions in the modelFilterList to be used within orchestration.

```
 Sample Code { "name": "yourNameChoice", "executableId": "orchestration", "scenarioId": "orchestration", "versionId": "0.0.1", "parameterBindings": [ { "key": "modelFilterList", "value": "[{modelName\": gpt-35-turbo\", modelVersions\": [\"0613\", \"0301\"]}, {modelName\": gemini-1.5-pro\", modelVersions\": [\"001\"]}]" }, { "key": "modelFilterListType", "value": "false" } ] }
```

8.6.5  Using the Orchestration Service

8.6.5.1 Get an Auth Token for Orchestration

Using Postman

Prerequisites

 · You have downloaded and installed the Postman client from https:/ /www.postman.com/

 · You have familiarized yourself with the Postman documentation and interface.

Procedure

 1. Download the JSON collection from Orchestration JSON.

 2. In Postman, click Import , select the JSON file, and choose Import to start the import.

 3. After the import is complete, expand the collection and navigate to Get Auth Token .

 4. Select the LLM Orchestration environment within Postman and configure it using the following values:

 · The LLM Orch Url is the URL of your orchestration deployment. For more information, see Create a Deployment for Orchestration [page 187].

 · The Client Id is the XSUAA client id of your AI API connection.

 · The Client Secret is the XSUAA client secret of your AI API connection.

 · The Grant Type URL parameter should be set to Client Credentials .

 · Leave the Token value empty.

 5. To retrieve an authentication token, send a GET request to Auth Token .

The response includes an access token, which is saved as the Token variable in the Postman environment. Check that the access token has been added correctly to the variable and if not, copy and paste it from the response into the variable.

Next Steps

Send a POST request using the Simple LLM Call from the Postman collection to try out the templating mechanism of orchestration.

.

Using Curl

Start by setting the required environment variables, which you can get from your SAP AI Core instance.

Prerequisites

curl is likely to be installed on your operating system by default. To check, open a command prompt and enter curl -V . If curl isn't installed, download and install it from https:/ /curl.se/ .

 Note

On macOS, you may need to install jq so that you can follow the curl commands.

 1. Install brew from https:/ /brew.sh/ .

 2. In a T erminal session, run brew install jq to install jq in your shell environment.

Procedure

 1. Set up your environment as follows:

```
AI_API_URL=<YOUR AI API URL> UTH_URL=<YOUR AUTH URL> A LIENT_ID=<YOUR CLIENT ID> C LIENT_SECRET=<YOUR CLIENT SECRET> C ESOURCE_GROUP=default R
```

```
2. Obtain the auth token by sending the following request: curl --request POST \ --url "${AUTH_URL}/oauth/token" \ --header 'content-type: application/x-www-form-urlencoded' \ --data grant_type=client_credentials \ --data client_id="$CLIENT_ID" \ --data client_secret="$CLIENT_SECRET" The response includes an access token. {"access_token":"ey...", "expires_in":7199, "jti":"...", "token_type":"bearer"} 3. Set the token to use it in the following steps:
```

```
TOKEN=ey...
```

8.6.5.2 Create a Deployment for Orchestration

Prerequisites

 · You have an SAP AI Core service instance and service key. For more information, see SAP AI Core Initial Setup Documentation.

 · You're using the extended service plan. For more information, see Service Plans [page 43] and Update a Service Plan [page 51].

 · You have completed the client authorization for your preferred user interface. For more information, see Use a Service Key in SAP AI Core.

 · You have at least one orchestration-compatible deployment for a generative AI model running. For more information, see Models and Scenarios in the Generative AI Hub [page 110].

Context

You create a deployment to make orchestration capabilities available for use. After the deployment is complete, you get a deploymentUrl . Y ou can use this URL across your organization to access orchestration in the generative AI hub.

Using Postman

Procedure

 1. Check that you have access to the orchestration scenario containing generative AI by sending a GET request to {{apiurl}}/v2/lm/scenarios .

Set the Authorization header with Bearer $TOKEN and set your resource group.

 Note

You must use the same resource group for all of your generative AI activities. To use a different resource group, these steps must be repeated for each resource group.

The scenarios listed contain a scenario with the id orchestration .

 2. Create a configuration by sending a POST request to the endpoint {{apiurl}}/v2/lm/ configurations .

Include the following parameters:

 · name is your free choice of identifier.

 · executableId must be orchestration .

 · scenarioId must be orchestration .

 · versionId is your own version reference.

```
 Sample Code { "name": "yourNameChoice", "executableId": "orchestration", "scenarioId": "orchestration", "versionId": "0.0.1", }
```

You receive a unique configurationId in the response.

 3. Create a deployment by sending a POST request to the endpoint {{apiurl}}/v2/lm/deployments .

Include the configurationId from the previous step in your request.

```
 Sample Code { "configurationId": "yourConfigurationId" }
```

 4. Retrieve the details of your deployment by sending a GET request to the endpoint {{apiurl}}/v2/lm/ deployments .

Using curl

Procedure

 1. Create a configure for the orchestration deployment.

Orchestration is exposed via the global scenario orchestration and the executable orchestration .

 a. Send the following request:

```
curl --request POST "$AI_API_URL/v2/lm/configurations" \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --header "Content-Type: application/json" \ --data-raw  '{ "name": "orchestration-config", "executableId": "orchestration", "scenarioId": "orchestration" }'
```

The response appears as follows:

```
 Output Code { "id": "f7ac7f77-e70f-4e9c-86b5-1504b44fe789", "message": "Configuration created"
```

```
} b. Take the id of the configuration and set it as an environment variable. ORCH_CONFIG_ID=f7ac7f77-e70f-4e9c-86b5-1504b44fe789
```

 2. Create the deployment for orchestration using the configuration id :

 a. Send the following request:

```
curl --request POST $AI_API_URL/v2/lm/deployments \ --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw "{ configurationId\": \"$ORCH_CONFIG_ID\" }"
```

The response appears as follows:

```
 Output Code { "deploymentUrl": "", "id": "d4168482710c6cf9", "message": "Deployment scheduled.", "status": "UNKNOWN" }
```

 b. Take the id of the deployment response and set it as an environment variable.

```
ORCH_DEPLOYMENT_ID=d4168482710c6cf9
```

 3. Wait for the deployment to start.

It takes a few minutes for the orchestration deployment to reach the status RUNNING. You can check the status of the deployment using the deployment id .

```
curl --request GET "$AI_API_URL/v2/lm/deployments/$ORCH_DEPLOYMENT_ID" \
```

```
--header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP"
```

If the output looks as follows (that is, the status is RUNNING), the deployment is ready. If not, try again after a couple of minutes.

```
{ "configurationId": "f7ac7f77-e70f-4e9c-86b5-1504b44fe789", "configurationName": "orchestration-config", "createdAt": "2024-06-20T14:40:16Z", "deploymentUrl": , "details": { "resources": { "backend_details": {} }, "scaling": { "backend_details": {} } }, "id": "d4468482710c6cf9", "lastOperation": "CREATE", "latestRunningConfigurationId": "f7ac7f77-e70f-4e9c-86b5-1504b44fe789", "modifiedAt": "2024-06-20T14:48:03Z",
```

```
"scenarioId": "llm-orchestration", "startTime": "2024-06-20T14:41:22Z", "status": "RUNNING", "submissionTime": "2024-06-20T14:40:18Z", "targetStatus": "RUNNING" }
```

 4. Set the value of the returned deploymentUrl as an environment variable.

The deploymentUrl is available only when the deployment has the status RUNNING.

```
ORCH_DEPLOYMENT_URL="<deployment_url>"
```

8.6.5.3 Consume Orchestration

8.6.5.3.1 Minimal Call

A minimal call to orchestration contains only configurations of the required templating and model configuration modules. The curl command below shows how to make such a request.

```
curl --request POST $ORCH_DEPLOYMENT_URL/completion \ --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw '{ "orchestration_config": { "module_configurations": { "templating_module_config": { "template": [ { "role": "user", "content": "Reply with '{{?text}}' in {{?language}}" } ], "defaults": { "language": "English" } }, "llm_module_config": { "model_name": "gpt-35-turbo-16k", "model_params": { "max_tokens": 50, "temperature": 0.1, "frequency_penalty": 0, "presence_penalty": 0 }, "model_version": "latest" } } }, "input_params": { "text": "Orchestration is Working!", "language": "German" } ' }
```

This request configures the templating module with a single user message with two parameters: text and language . The language parameter is also configured with English as the default. The LLM module

is configured to use gpt-35-turbo-16k in the latest available version and a set of model parameters. The input_params field contains the values for the parameters text and language . These values are used during this request in the prompt sent to the model.

The response contains a request_id , the module results from each module that was executed, and the orchestration_result , which includes the response of the call to the model.

```
 Output Code { "request_id": "53fc2dcd-399d-4a2b-8bde-912b9f001fed", "module_results": { "templating": [ { "role": "user", "content": "Reply with 'Orchestration is Working!' in German" } ], "llm": { "id": "chatcmpl-9k8M3djXphXPWh2QkQm1YVtXK4Eki", "object": "chat.completion", "created": 1720782231, "model": "gpt-35-turbo-16k", "choices": [ { "index": 0, "message": { "role": "assistant", "content": "Orchestrierungsdienst funktioniert!" }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 10, "prompt_tokens": 20, "total_tokens": 30 } } }, "orchestration_result": { "id": "chatcmpl-9k8M3djXphXPWh2QkQm1YVtXK4Eki", "object": "chat.completion", "created": 1720782231, "model": "gpt-35-turbo-16k", "choices": [ { "index": 0, "message": { "role": "assistant", "content": "Orchestrierungsdienst funktioniert!" }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 10, "prompt_tokens": 20, "total_tokens": 30 } } }
```

The templating module result contains the user message with the filled in parameters. The LLM module result contains the response of the model execution. In this example, the LLM module result and the orchestration result are the same. However, they might differ, such as when the output filtering module filters the response.

8.6.5.3.2 Few-Shot Learning

The following example shows how you can configure the templating module to use a few-shot learning prompt.

```
curl --request POST $ORCH_DEPLOYMENT_URL/completion \ --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw '{ "orchestration_config": { "module_configurations": { "templating_module_config": { "template": [ { "role": "system", "content": "You classify input text into the two following categories: Business, Economics, Tech, and other" }, { "role": "user", "content": "input text: 'Comcast launches prepaid plans'" }, { "role": "assistant", "content": "Business" }, { "role": "user", "content": "input text: 'Slower Fed Pivot Weakens Rate-Cut Bets Across Emerging Asia'" }, { "role": "assistant", "content": "Economics" }, { "role": "user", "content": "input text: {{?input}}" } ] }, "llm_module_config": { "model_name": "gpt-35-turbo", "model_params": { "max_tokens": 50, "temperature": 0.1, "frequency_penalty": 0.0, "presence_penalty": 0.0 } } } }, "input_params": { "input": "Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation." } ' }
```

In this case, the template contains an array of messages, including a system message, as well as several user and assistant messages. The actual input to categories is configured with the final user message and the content input text: {{?input}} . The input parameter input is set to Scaling up neural models has yielded significant advancements in a wide array of tasks, particularly in language generation. .

8.6.5.3.3 Chat

Orchestration can also be used in chat scenarios. The following example shows how to configure the templating module to use a chat prompt.

```
curl --request POST $ORCH_DEPLOYMENT_URL/completion \ --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw '{ "orchestration_config": { "module_configurations": { "templating_module_config": { "template": [ { "role": "user", "content": "And now two more that sound like Goethe." } ] }, "llm_module_config": { "model_name": "gpt-35-turbo-16k", "model_params": { "max_tokens": 300, "temperature": 0.1, "frequency_penalty": 0, "presence_penalty": 0 } } } }, "input_params": { }, "messages_history": [ { "role":"user", "content":"Create 3 paraphrases of I love you." }, { "role":"assistant", "content":"1. I have deep affection for you.\n2. My feelings for you are strong and caring.\n3. You mean everything to me." }, { "role":"user", "content":"Can you please make them more romantic." }, { "role":"assistant", "content":"1. My heart is filled with an intense passion for you.\n2. You are the light of my life, and my love for you knows no bounds.\n3. I cherish you more than words can express, my dear." }, { "role":"user",
```

```
"content":"Try sounding like shakespeare." }, { "role":"assistant", "content":"1. Thou art the sun in my sky, and my love for thee doth burn with unyielding fervor.\n2. Within mine heart, thou hast ignited a flame of ardor that doth shine brightly for thee, my dearest.\n3. My love for thee doth surpass all measure, for thou art the true essence of my being." } ] }'
```

As shown above, messages from the chat can be passed to the current request using the messages_history parameter. In this case, the chat history contains several user messages and assistant responses. The templating module is then configured with the current user message, which is used to generate the next assistant response. The templating module appends the current user message to the message history to generate the prompt that is sent to the LLM module.

The response contains the messages from the chat history and the response to the new message. The output of module_results.templating and the orchestration_result.choices results can be used as the message history for a subsequent inference request:

```
{ "request_id": "5445f8d8-8b68-43c3-a149-26c1e6a88a22", "module_results": { "templating": [ { "role": "user", "content": "Create 3 paraphrases of I love you." }, { "role": "assistant", "content": "1. I have deep affection for you.\n2. My feelings for you are strong and caring.\n3. You mean everything to me." }, { "role": "user", "content": "Can you please make them more romantic." }, { "role": "assistant", "content": "1. My heart is filled with an intense passion for you.\n2. You are the light of my life, and my love for you knows no bounds.\n3. I cherish you more than words can express, my dear." }, { "role": "user", "content": "Try sounding like shakespeare." }, { "role": "assistant", "content": "1. Thou art the sun in my sky, and my love for thee doth burn with unyielding fervor.\n2. Within mine heart, thou hast ignited a flame of ardor that doth shine brightly for thee, my dearest.\n3. My love for thee doth surpass all measure, for thou art the true essence of my being." }, { "role": "user", "content": "And now two more that sound like Goethe." } ], "llm": { "id": "chatcmpl-9kXqisJKnuNv1B4eXTUzqZEJSmzdC", "object": "chat.completion", "created": 1720880232,
```

```
"model": "gpt-35-turbo-16k", "choices": [ { "index": 0, "message": { "role": "assistant", "content": "1. In thy presence, my soul finds solace, for thou art the embodiment of love's sweetest melody.\n2. Like a gentle breeze upon a summer's eve, thy love doth caress my heart and fill it with eternal longing." }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 51, "prompt_tokens": 212, "total_tokens": 263 } } }, "orchestration_result": { "id": "chatcmpl-9kXqisJKnuNv1B4eXTUzqZEJSmzdC", "object": "chat.completion", "created": 1720880232, "model": "gpt-35-turbo-16k", "choices": [ { "index": 0, "message": { "role": "assistant", "content": "1. In thy presence, my soul finds solace, for thou art the embodiment of love's sweetest melody.\n2. Like a gentle breeze upon a summer's eve, thy love doth caress my heart and fill it with eternal longing." }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 51, "prompt_tokens": 212, "total_tokens": 263 } } }
```

8.6.5.3.4 Grounding

Prerequisites

You have created a resource group for grounding purposes. For more information, see Create a Resource Group for AI Data Management [page 153]

You have created a generic secret for grounding purposes. For more information, see Create a Generic Secret for AI Data Management [page 154]

Procedure

The following code uses SAP HANA Vector Store:

```
 Sample Code curl --request POST $ORCH_DEPLOYMENT_URL/completion \ -header 'content-type: application/json' \ ---header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw '{ "orchestration_config": { "module_configurations": { "grounding_module_config": {  // Configuration for the document grounding module "type": "document_grounding_service",  // Specifies the module type as a grounding service "config": { "filters": [  // Define the filters to apply during the grounding process { "id": "filter",  // Filter identifier "data_repositories": [  // Specifies which repositories to use for document grounding "*"  // Use all available data repositories ], "search_config": {  // Configuration related to search functionality "max_chunk_count": 10  // Limits the maximum number of chunks to retrieve }, "data_repository_type": "vector"  // Defines the type of data repository (vector-based in this case) } ], "input_params": [ "groundingRequest"  // Input parameter, typically the query or grounding request ], "output_param": "groundingOutput"  // Output parameter where the grounding results will be stored } }, "llm_module_config": {  // Configuration for the Large Language Model (LLM) module "model_name": "gemini-1.5-pro",  // Name of the model to use for processing (Gemini 1.5 Pro) "model_params": {},  // Parameters for the LLM model, left empty for default behavior "model_version": "001"  // Version of the LLM model being used }, "templating_module_config": {  // Configuration for the templating module "template": [ { "role": "user",  // Role within the conversation template, e.g., user or assistant "content": "<prompt>"  // Content placeholder for the user prompt } ], "defaults": {}  // Default settings for the templating module, currently empty } } }, "input_params": {
```

```
"groundingRequest": "<grounding query>"  // The input parameter containing the grounding query }, "return_module_results": true  // Return results from each module used in the orchestration }
```

8.6.5.3.5 Data Masking

In the following example we use the data masking module to anonymize persons, organisations, and contact information in the input.

The entries "Harvard University" and "Boston" in the allowlist will not be masked.

In this case, the input will be masked before the call to the LLM. However data can not be unmasked in the LLM output.

```
 Sample Code url --request POST $ORCH_DEPLOYMENT_URL/completion \ c --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw '{ "orchestration_config": { "module_configurations": { "templating_module_config": { "template": [ { "role": "user", "content": "Summarize the following CV in 10 sentences: {{? orgCV}}" } ], }, "llm_module_config": { "model_name": "gpt-4", "model_params": { "max_tokens": 300, "temperature": 0.1, "frequency_penalty": 0, "presence_penalty": 0 } }, "masking_module_config": { "masking_providers": [ { "type": "sap_data_privacy_integration", "method": "anonymization", "entities": [ {"type": "profile-email"}, {"type": "profile-person"}, {"type": "profile-phone"}, {"type": "profile-org"}, {"type": "profile-location"} ], "allowlist": ["Harvard University", "Boston"] // "Harvard University" and "Boston" will be preserved and not masked } ]
```

```
} } }, "input_params": { "orgCV": "Patrick Morgan \n +49 (970) 333-3833 \n patric.morgan@hotmail.com \n\n Highlights \n - Strategic and financial planning expert \n - Accurate forecasting \n - Process implementation \n Staff leadership and development \n - Business performance improvement \n Proficient in SAP,  Excel VBA\n\n Education \n Master of Science: Finance 2014 \n Harvard University, Boston \n\n Bachelor of Science: Finance - 2011 \n Harvard University, Boston \n\n\n Certifications \n Certified Management Accountant \n\n\n Summary \n Skilled Financial Manager adept at increasing work process efficiency and profitability through functional and technical analysis. Successful at advising large corporations, small businesses, and individual clients. Areas of expertise include asset allocation, investment strategy, and risk management. \n\n\n Experience \n Finance Manager - 09/2016 to 05/2018 \n M&K Group, York \n - Manage the modelling, planning, and execution of all financial processes. \n - Carry short and long-term custom comprehensive financial strategies to reach company goals. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Research market trends and surveys and use information to stimulate business. \n\n Finance Manager - 09/2013 to 05/2016 \n Ago Group, Chicago \n - Drafted executive analysis reports highlighting business issues, potential risks, and profit opportunities. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Analysed market trends and surveys and used information to revenue growth." } ' }
```

As the response shows, the configured entities are masked by the data masking module before being sent to the LLM. The LLM operates on the masked data and is still able to provide a summary.

 Output Code

{ "request_id": "0b32ef8d-5f92-43de-ae2b-ae23cecba662", "module_results": { "templating": [ { "role": "user", "content": "Summarize the following CV in 10 sentences: Patrick Morgan \n +49 (970) 333-3833 \n patric.morgan@hotmail.com \n\n Highlights \n - Strategic and financial planning expert \n - Accurate forecasting \n - Process implementation \n - Staff leadership and development \n Business performance improvement \n - Proficient in SAP,  Excel VBA\n\n Education \n Master of Science: Finance - 2014 \n Harvard University, Boston \n\n Bachelor of Science: Finance - 2011 \n Harvard University, Boston \n\n\n Certifications \n Certified Management Accountant \n\n\n Summary \n Skilled Financial Manager adept at increasing work process efficiency and profitability through functional and technical analysis. Successful at advising large corporations, small businesses, and individual clients. Areas of expertise include asset allocation, investment strategy, and risk management. \n\n\n Experience \n Finance Manager - 09/2016 to 05/2018 \n M&amp;K Group, York \n - Manage the modelling, planning, and execution of all financial processes. \n - Carry short and long-term custom comprehensive financial strategies to reach company goals. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Research market trends and surveys and use information to stimulate business. \n\n Finance Manager - 09/2013 to 05/2016 \n Ago Group, Chicago \n - Drafted executive analysis reports highlighting business issues, potential risks, and profit opportunities. \n - Recommended innovative

alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Analysed market trends and surveys and used information to revenue growth."

```
} ], "input_masking": { "message": "Input to LLM is masked successfully.", "data": { "masked_template": [ { "role": "user",
```

"content": "Summarize the following CV in 10 sentences: MASKED_PERSON \n MASKED_PHONE_NUMBER \n MASKED_EMAIL \n\n Highlights \n - Strategic and financial planning expert \n - Accurate forecasting \n - Process implementation \n - Staff leadership and development \n Business performance improvement \n - Proficient in SAP, Excel VBA\n\n Education \n Master of Science: Finance - 2014 \n Harvard University, Boston \n\n Bachelor of Science: Finance - 2011 \n Harvard University, Boston \n\n\n Certifications \n Certified Management Accountant \n\n\n Summary \n Skilled Financial Manager adept at increasing work process efficiency and profitability through functional and technical analysis. Successful at advising large corporations, small businesses, and individual clients. Areas of expertise include asset allocation, investment strategy, and risk management. \n\n\n Experience \n Finance Manager - 09/2016 to 05/2018 \n MASKED_ORG, MASKED_LOCATION \n - Manage the modelling, planning, and execution of all financial processes. \n - Carry short and long-term custom comprehensive financial strategies to reach company goals. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Research market trends and surveys and use information to stimulate business. \n\n Finance Manager - 09/2013 to 05/2016 \n MASKED_ORG, MASKED_LOCATION \n - Drafted executive analysis reports highlighting business issues, potential risks, and profit opportunities. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including handson negotiations with potential investors. \n - Analysed market trends and surveys and used information to revenue growth."

<!-- missing-text -->

"content": "The CV belongs to a skilled Financial Manager with expertise in strategic and financial planning, accurate forecasting, process implementation, staff leadership and development, and business performance improvement. They are proficient in SAP and Excel VBA. They hold a Master of Science in Finance from an undisclosed institution and location, and a Bachelor of Science in Finance from another undisclosed institution and location. They are also a Certified Management Accountant. Their experience includes managing financial processes, implementing financial strategies, recommending cost-saving measures, and conducting deal analysis and market research. They have worked as a Finance Manager for two different organizations from 2013 to 2018."

<!-- missing-text -->

```
}, "finish_reason": "stop" } ], "usage": {
```

```
"completion_tokens": 124, "prompt_tokens": 354, "total_tokens": 478 } } }, "orchestration_result": { "id": "chatcmpl-A5vfibe3l7gpZLPKQnym4NLmknErY", "object": "chat.completion", "created": 1725976694, "model": "gpt-4", "choices": [ { "index": 0, "message": { "role": "assistant", "content": "The CV belongs to a skilled Financial Manager with expertise in strategic and financial planning, accurate forecasting, process implementation, staff leadership and development, and business performance improvement. They are proficient in SAP and Excel VBA. They hold a Master of Science in Finance from an undisclosed institution and location, and a Bachelor of Science in Finance from another undisclosed institution and location. They are also a Certified Management Accountant. Their experience includes managing financial processes, implementing financial strategies, recommending cost-saving measures, and conducting deal analysis and market research. They have worked as a Finance Manager for two different organizations from 2013 to 2018." }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 124, "prompt_tokens": 354, "total_tokens": 478 } } }
```

Using "method": "pseudonymization" instead of "method": "anonymization" in the masking module configuration will pseudonymize the data before sending it to the LLM. Additionally the LLM response is checked for any data that can be unmasked before sending out the final response.



```
Output Code { "request_id": "f5d14279-9cbe-4f35-815d-ea2df5f6d9d4", "module_results": { "templating": [ { "role": "user", "content": "Summarize the following CV in 10 sentences: Patrick Morgan \n +49 (970) 333-3833 \n patric.morgan@hotmail.com \n\n Highlights \n - Strategic and financial planning expert \n - Accurate forecasting \n - Process implementation \n - Staff leadership and development \n Business performance improvement \n - Proficient in SAP,  Excel VBA\n\n Education \n Master of Science: Finance - 2014 \n Harvard University, Boston \n\n Bachelor of Science: Finance - 2011 \n Harvard University, Boston \n\n\n Certifications \n Certified Management Accountant \n\n\n Summary \n Skilled Financial Manager adept at increasing work process efficiency and profitability through functional and technical analysis. Successful at advising large corporations, small businesses, and individual clients. Areas of expertise include asset allocation, investment strategy, and risk management. \n\n\n Experience \n Finance Manager - 09/2016 to 05/2018 \n M&K Group, York \n - Manage the modelling, planning, and execution of all
```

financial processes. \n - Carry short and long-term custom comprehensive financial strategies to reach company goals. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Research market trends and surveys and use information to stimulate business. \n\n Finance Manager - 09/2013 to 05/2016 \n Ago Group, Chicago \n - Drafted executive analysis reports highlighting business issues, potential risks, and profit opportunities. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Analysed market trends and surveys and used information to revenue growth." }

```
], "input_masking": { "message": "Input to LLM is masked successfully.", "data": { "masked_template": [ { "role": "user",
```

"content": "Summarize then following CV in 10 sentences: MASKED_PERSON_1 \n MASKED_PHONE_NUMBER_1 \n MASKED_EMAIL_1 \n\n Highlights \n - Strategic and financial planning expert \n - Accurate forecasting \n - Process implementation \n - Staff leadership and development \n Business performance improvement \n - Proficient in SAP, Excel VBA\n\n Education \n Master of Science: Finance - 2014 \n Harvard University, Boston \n\n Bachelor of Science: Finance - 2011 \n Harvard University, Boston \n\n\n Certifications \n Certified Management Accountant \n\n\n Summary \n Skilled Financial Manager adept at increasing work process efficiency and profitability through functional and technical analysis. Successful at advising large corporations, small businesses, and individual clients. Areas of expertise include asset allocation, investment strategy, and risk management. \n\n\n Experience \n Finance Manager - 09/2016 to 05/2018 \n MASKED_ORG_1, MASKED_LOCATION_1 \n - Manage the modelling, planning, and execution of all financial processes. \n - Carry short and long-term custom comprehensive financial strategies to reach company goals. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including hands-on negotiations with potential investors. \n - Research market trends and surveys and use information to stimulate business. \n\n Finance Manager - 09/2013 to 05/2016 \n MASKED_ORG_2, MASKED_LOCATION_2 \n - Drafted executive analysis reports highlighting business issues, potential risks, and profit opportunities. \n - Recommended innovative alternatives to generate revenue and reduce unnecessary costs. \n - Employed advanced deal analysis, including handson negotiations with potential investors. \n - Analysed market trends and surveys and used information to revenue growth."

```
] } }, "llm": { "id": "chatcmpl-A5veOHbkabo04FugU7ysHeWeH7ZZC", "object": "chat.completion", "created": 1725976612, "model": "gpt-4", "choices": [ { "index": 0, "message": { "role": "assistant",
```

"content": "MASKED_PERSON_1 is a skilled Financial Manager with expertise in strategic and financial planning, accurate forecasting, process implementation, staff leadership and development, and business performance improvement. They are proficient in SAP and Excel VBA. They hold a Master of Science in Finance from Harvard University and are a Certified Management Accountant. Their experience includes advising large corporations, small businesses, and individual clients on asset allocation, investment strategy, and risk management. They have worked as a Finance Manager for MASKED_ORG_1 and MASKED_ORG_2, where they managed financial processes,

```
implemented financial strategies, recommended cost-saving measures, and conducted advanced deal analysis. They also have experience in researching market trends to stimulate business growth." }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 147, "prompt_tokens": 378, "total_tokens": 525 } } }, "orchestration_result": { "id": "chatcmpl-A5veOHbkabo04FugU7ysHeWeH7ZZC", "object": "chat.completion", "created": 1725976612, "model": "gpt-4", "choices": [ { "index": 0, "message": { "role": "assistant", "content": "Patrick Morgan is a skilled Financial Manager with expertise in strategic and financial planning, accurate forecasting, process implementation, staff leadership and development, and business performance improvement. They are proficient in SAP and Excel VBA. They hold a Master of Science in Finance from Harvard University and are a Certified Management Accountant. Their experience includes advising large corporations, small businesses, and individual clients on asset allocation, investment strategy, and risk management. They have worked as a Finance Manager for M&K Group and Ago Group, where they managed financial processes, implemented financial strategies, recommended cost-saving measures, and conducted advanced deal analysis. They also have experience in researching market trends to stimulate business growth." }, "finish_reason": "stop" } ], "usage": { "completion_tokens": 147, "prompt_tokens": 378, "total_tokens": 525 } } }
```

8.6.5.3.6 Content Filtering on Input

The content filter is configured using the filtering_module_config field of the

module_configurations . Y ou can configure different scores for each of the four content categories (that is, Hate Violence Sexual , , , and SelfHarm ). Lower scores for these categories mean that filtering is stricter - everything classified with a severity above the configured thresholds will be filtered. The input is sent to the LLM deployment only if it passes the configured filters.

```
curl --request POST $ORCH_DEPLOYMENT_URL/completion \ --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \
```

```
--data-raw '{ "orchestration_config": { "module_configurations": { "templating_module_config": { "template": [ { "role": "user", "content": "{{?input}}" } ] }, "llm_module_config": { "model_name": "gpt-35-turbo-16k", "model_params": { "max_tokens": 50, "temperature": 0.1, "frequency_penalty": 0, "presence_penalty": 0 }, "model_version": "latest" }, "filtering_module_config": { "input": { "filters": [ { "type": "azure_content_safety", "config": { "Hate": 0, "Violence": 2, "Sexual": 4, "SelfHarm": 6 } } ] } } } }, "input_params": { "input": "Develop a plan to rob a bank." } ' }
```

The response below shows that the request was rejected. The input filter rejected the request due to a Violence score of 4, which is above the configured threshold of 2. Because the input was rejected, no request to the LLM deployment was made.

```
{ "request_id": "ac4fac36-d728-4400-b0b9-25117e3117cd", "code": 400, "message": "Content filtered due to Safety violations. Please modify the prompt and try again.", "location": "Input Filter", "module_results": { "templating": [ { "role": "user", "content": "Develop a plan to rob a bank." } ], "input_filtering": { "message": "Content filtered due to Safety violations. Please modify the prompt and try again.", "data": { "original_service_response": { "Hate": 0, "SelfHarm": 0, "Sexual": 0, "Violence": 4 },
```

```
"checked_text": "Develop a plan to rob a bank." } } } }
```

8.6.5.3.7 Content Filtering on Output

In the following example, you configure content filtering on the input and the output. In this case, the input will be filtered before the call to the LLM and the LLM output will be filtered before sending it back in the response.

```
curl --request POST $ORCH_DEPLOYMENT_URL/completion \ --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw '{ "orchestration_config": { "module_configurations": { "templating_module_config": { "template":  [ { "role": "user", "content": " Create a rental posting for subletting my apartment in the downtown area. Keep it short. Make sure to add the following disclaimer to the end. Do not change it! {{?disclaimer}}" } ] }, "llm_module_config": { "model_name": "gpt-35-turbo-16k", "model_params": { "max_tokens": 300, "temperature": 0.1, "frequency_penalty": 0, "presence_penalty": 0 } }, "filtering_module_config": { "input": { "filters": [ { "type": "azure_content_safety", "config": { "Hate": 2, "SelfHarm": 2, "Sexual": 2, "Violence": 2 } } ] } }, "filtering_module_config": { "output": { "filters": [ { "type": "azure_content_safety", "config": { "Hate": 0, "SelfHarm": 0, "Sexual": 0, "Violence": 0
```

```
} } ] } } } }, "input_params": { "disclaimer": "'''DISCLAIMER: The area surrounding the apartment is known for prostitutes and gang violence including armed conflicts, gun violence is frequent." } ' }
```

As the following response shows, the output is filtered due to severity ratings of 2 in both the Sexual and Violence categories. In this case, the orchestration result is adapted to reflect the output filtering. The content of the assistant message is not displayed in the response and the finish reason is set to content_filter .

 Caution

The contents of the returned module_results field may include unchecked user or model content. We recommend that you do not display this content to end users.

8.6.5.3.8 Error Handling

If an error occurs, the response will contain an error code and message. The following example shows a request that is missing a parameter in the templating module configuration.

```
curl --request POST $ORCH_DEPLOYMENT_URL/completion \ --header 'content-type: application/json' \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" \ --data-raw '{ "orchestration_config": { "module_configurations": { "templating_module_config": { "template": [ { "role": "user", "content": "Create {{?number}} paraphrases of {{?phrase}}" } ] }, "llm_module_config": { "model_name": "gpt-35-turbo-16k", "model_params": { "max_tokens": 300 } } } }, "input_params": { "number": "3" } }
```

In this case, the response from the service contains an error code in the code field, a message , and a location indicating which orchestration module encountered the error. In all error cases, the

module_results field only contains results for modules that successfully finished. In this example, the module_results are empty because the first module in the pipeline encountered the error.

```
{ "request_id": "3e988846-1360-4a4a-a7ad-e77b85057321", "code": 400, "message": "Missing required parameters: ['phrase']", "location": "Module: Templating", "module_results": {} }
```

9 Predictive AI

SAP AI Core allows you to make data driven decisions confidently and efficiently and is tailored to business problems. It handles large volumes of data and provides scalable machine learning capabilities to automate tasks such as triage services for customer feedback or tickets and classification tasks. SAP AI Core comes with preconfigured SAP solutions, can be configured for open-source machine learning frameworks, can be used with Argo Workflow and KServe, and can be embedded into other applications.

ML Operations [page 207]

This section guides you through the end-to-end AI lifecycle of SAP AI Core.

Metrics [page 307]

The AI API provides the ability to track metrics, and to customize or filter which metrics are reported.

Advanced Features [page 323]

Explore advanced features, within SAP AI Core.

9.1 ML Operations

This section guides you through the end-to-end AI lifecycle of SAP AI Core.

Connect Your Data [page 208]

Use cloud storage with SAP AI Core to store AI assets such as datasets and model files. You use Artifacts in SAP AI Core to reference to your AI Assets.

Train Your Model [page 217]

You execute a training workflow to train your AI learning model.

Use Your Model [page 265]

You deploy your AI learning model to run inferences against it.

Parent topic: Predictive AI [page 207]

Related Information

Metrics [page 307]

Advanced Features [page 323]

9.1.1  Connect Your Data

Use cloud storage with SAP AI Core to store AI assets such as datasets and model files. You use Artifacts in SAP AI Core to reference to your AI Assets.

Manage Files [page 208]

An artifact refers to data or a file that is produced or consumed by executions or deployments. They are managed through SAP AI Core and your connected object store.

Manage Files Using the Dataset API [page 213]

Where direct access to files in the object store is not possible or desirable (for example, in Content as a Service Scenarios, where the Service Consumers might not be the owners of the object store) you can upload, download, and delete files from the pre-registered object store using the SAP AI Core Dataset API.

Parent topic: ML Operations [page 207]

Related Information

Train Your Model [page 217] Use Your Model [page 265]

9.1.1.1 Manage Files

An artifact refers to data or a file that is produced or consumed by executions or deployments. They are managed through SAP AI Core and your connected object store.

The object store secret allows SAP AI Core to access your cloud storage and data without exposing your compromising your credentials.

Create Files [page 209]

List Files [page 211]

Parent topic: Connect Your Data [page 208]

Related Information

Manage Files Using the Dataset API [page 213]

208

PUBLIC

9.1.1.1.1 Create Files

The object store secret allows SAP AI Core to access your cloud storage and data without exposing your compromising your credentials.

<!-- missing-text -->

Restriction

The objectStore name data path , and scenarioId refer to pre-existing values. For the objectStore name and data path values, you must use the values that you used when registering the object storage, following the naming convention outlined in the diagram below. In example output code blocks, these values are represented by ai://default/data .

<!-- missing-text -->

Related Information

List Files [page 211]

Using Postman

Procedure

 1. Create a new POST request using URL {{apiurl}}/v2/lm/artifacts

 2. Toggle the body tab, and enter the following JSON:

```
{ "name": "name of artifact", "kind": "dataset", "url": "ai://<objectStore name>/<data path>", "description": "<description of artifact>", "scenarioId": "<scenarioID>" }
```

Results

The response body contains the ID of your new artifact.

```
{ "id": "3x4mpl3-651c-4f3e-8e1d-81a408041bc1",
```

```
"message": "Artifact acknowledged", "url": "ai://default/data" }
```

Using curl

Procedure

Run the following code:

```
curl --location --request POST "$API_URL/v2/lm/artifacts" \ -header "Authorization: Bearer $TOKEN" \ ---header "Content-Type: application/json" \ --header "AI-Resource-Group: <Resource group>" \ --data-raw '{ "name": "name of artifact", "kind": "dataset", "url": "ai://<objectStore name>/<data path>", "description": "<description of artifact>", "scenarioId": "<scenarioID>" }
```

Results

The response body contains the ID of your new artifact.

```
{ "id": "3x4mpl3-651c-4f3e-8e1d-81a408041bc1", "message": "Artifact acknowledged", "url": "ai://default/data" }
```

9.1.1.1.2 List Files

Using Postman

 1. Send a GET request to the endpoint {{apiurl}}/v2/lm/artifacts

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

<!-- missing-text -->

 4. On the Header tab, add the following entry:

<!-- missing-text -->

 5. Send the request.

Using curl

curl --request GET "$AI_API_URL/v2/lm/artifacts" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"

```
 Output Code { "count":3, "resources":[ { "createdAt":"2021-02-09T08:08:12Z", "description":"", "executionId":"d44edae36c187cf6", "id":"3088b75f-5448-4c19-8055-392668a043ec", "kind":"model", "modifiedAt":"2021-02-09T08:08:12Z", "name":"pytf-model", "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "url":"ai://default/d44edae36c187cf6/pytf-model" }, {k "createdAt":"2021-02-09T07:56:37Z", "description":"", "executionId":"d44edae36c187cf6", "id":"38f7a46b-454d-4543-9457-b1eede5036f8", "kind":"model", "modifiedAt":"2021-02-09T07:56:37Z", "name":"churn-pickle", "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "url":"ai://default/d44edae36c187cf6/churn-pickle" }, { "createdAt":"2021-02-07T16:07:16Z", "description":"Churn and Text Classifier Dataset", "id":"b45265f2-9bc3-441a-a0e1-fac1438acb79", "kind":"dataset", "modifiedAt":"2021-02-07T16:07:16Z", "name":"pytf", "scenarioId":"84fe6957-1145-4183-b682-8f11ca56d060", "url":"ai://default/pytf/" } ] }
```

Parent topic: Manage Files [page 208]

Related Information

Create Files [page 209]

9.1.1.2 Manage Files Using the Dataset API

Where direct access to files in the object store is not possible or desirable (for example, in Content as a Service Scenarios, where the Service Consumers might not be the owners of the object store) you can upload, download, and delete files from the pre-registered object store using the SAP AI Core Dataset API.

For full details on the Dataset API specification, see SAP AI Core API documentation .

Prerequisites

 · You must have an object store secret defined in the target resource group.

 Restriction

Only S3 object stores can be used with the Dataset API.

<!-- missing-text -->

 Restriction

Only csv type files can be uploaded with the Dataset API.

Create Files [page 214]

Where direct access to files in the object store is not possible or desirable (for example, in Content as a Service Scenarios, where the Service Consumers might not be the owners of the object store) you can upload, download, and delete files from the pre-registered object store using the SAP AI Core Dataset API.

Download Files [page 216]

Delete Files [page 216]

Parent topic: Connect Your Data [page 208]

Related Information

Manage Files [page 208]

9.1.1.2.1 Create Files

Where direct access to files in the object store is not possible or desirable (for example, in Content as a Service Scenarios, where the Service Consumers might not be the owners of the object store) you can upload, download, and delete files from the pre-registered object store using the SAP AI Core Dataset API.

Using Curl

Prerequisites

 · You must have an object store secret defined in the target resource group.

 Restriction Only S3 object stores can be used with the Dataset API.  Restriction Only csv type files can be uploaded with the Dataset API.

Context

For full details on the Dataset API specification, see SAP AI Core API documentation .

Procedure

Run the following code:

```
curl --location --request PUT '$AI_API_URL/v2/lm/dataset/files/$SECRET_NAME/ $FILE_PATH' \\ --header 'Authorization: Bearer $TOKEN' \ --header 'Content-Type: text/csv' \ --header 'ai-resource-group: $RESOURCE_GROUP' \ --data @$FILE_LOCATION
```

 · If you use a service provider token, specify the resource group. If you use a service consumer token, the resource group information is contained in the token, and need not be specified.

 · In the request body, submit the file as binary data.

Using Postman

Prerequisites

 · You must have an object store secret defined in the target resource group.

 Restriction Only S3 object stores can be used with the Dataset API.  Restriction Only csv type files can be uploaded with the Dataset API.

Context

For full details on the Dataset API specification, see SAP AI Core API documentation

.

Procedure

 1. Send a PUT request to the endpoint {{apiurl}}/v2/lm/dataset/files/{secret name}/{full file path}

 2. In the header set the Content-Type as &lt;text/csv&gt; .

 3. If you use a service provider token, specify the resource group. If you use a service consumer token, the resource group information is contained in the token, and need not be specified.

 4. In the request body, submit the file as binary data.

Results

Your file will be uploaded to the S3 storage bucket with the prefix specified in the object store secret, and the full file path specified in the upload request.

```
 Output Code { message": "File default/test.csv created successfully.", " "url": "ai://default/test.csv" }
```

9.1.1.2.2 Download Files

Using Curl

Procedure

Run the following code:

```
curl --location --request GET '$AI_API_URL/v2/lm/dataset/files/$SECRET_NAME/
```

```
$FILE_PATH' \\ --header 'Authorization: Bearer $TOKEN' \ --header 'ai-resource-group: $RESOURCE_GROUP' \ --data @$FILE_LOCATION
```

Using Postman

Procedure

Send a GET request to the endpoint {{apiurl}}/v2/lm/dataset/files/{secret name}/{full file path}

9.1.1.2.3 Delete Files

Using Curl

Procedure

Run the following code:

```
curl --location --request DELETE '$AI_API_URL/v2/lm/dataset/files/$SECRET_NAME/ $FILE_PATH' \\ --header 'Authorization: Bearer $TOKEN' \ --header 'ai-resource-group: $RESOURCE_GROUP
```

Using Postman

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/lm/dataset/files/{secret name}/{full file path}

Ensure that the following headers are selected:

<!-- missing-text -->

9.1.2  Train Your Model

You execute a training workflow to train your AI learning model.

SAP AI Core uses data as specified by your resource group. Models are specified in your workflow and are trained on a remote server. The resulting artifacts are stored to your object store.

The execution service includes:

 · Batch jobs/pipelines via a state-of-the-art workflow engine.

 · Parameterized templates for pipelines stored and managed at tenant level.

 · Pipeline instances with specific parameters and resource isolation.

 · Support for bringing own Docker registries and Docker images for running pipeline steps.

 · Data extraction and validation modeled as pipeline steps.

 · Dedicated object store buckets used as data storage. A default bucket should be provided by users per resource group.

 · Bring your own hyperscaler-backed object store buckets.

The execution engine in SAP AI Core leverages the Argo Workflows open source project. It supports container-native workflows and pipelines modeled as direct acyclic graphs or steps. The Argo Workflows are used to ingest data, perform preprocessing and postprocessing, and train models and execute batch inference pipelines. SAP AI Core also leverages the parallel processing of steps in the form of a DAG ( Directed Acyclic Graph) structure in workflow templates. For information about how using parallel nodes may affect your costs, see Metering and Pricing for SAP AI Core [page 47].

 Note

Argo Workflow isn't optimized for time critical tasks. Each step must be scheduled onto a node in the cluster, and the cluster initialized. The time this takes depends on the load of the workflow controller and the node availability in the cluster. Therefore, it isn't recommended to use multistep Argo Workflows for time-critical tasks.

It's possible to train the same model multiple times, with varying parameters (for parameters compatible with the workflow, only) and evaluate them in SAP AI Launchpad.

Choose a Resource Plan [page 219]

You can configure SAP AI Core to use different infrastructure resources for different tasks, based on demand. SAP AI Core provides several preconfigured infrastructure bundles called 'resource plans' for this purpose.

Workflow Templates [page 221]

Here, you'll find a basic workflow example template. Feel free to adjust it to suit your workflow needs.

List Scenarios [page 225]

A scenario is an implementation of a specific AI use case within a user's tenant. It consists of a pre-defined set of AI capabilities in the form of executables and templates.

List Executables [page 228]

An executable is a reusable template that defines a workflow or pipeline for tasks such as training a machine learning model or creating a deployment. It contains placeholders for input artifacts (datasets or models) and parameters (custom key-pair values) that enable the template to be reused in different scenarios.

Create Configurations [page 234]

A configuration is a collection of parameters, artifact references (such as datasets or models), and environment settings that are used to instantiate and run an execution or deployment of an executable or template.

List Configurations [page 236]

Using Artifact Signatures [page 238]

Artifact signatures in the form of a hash can be added to output artifacts from executions.

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Discover features of the SAP AI Core runtime that improve efficiency and help manage resource consumption.

Retrieve Execution Logs [page 255]

accessed in the deployment and execution logs.

Training Schedules [page 258]

Parent topic: ML Operations [page 207]

Related Information

Connect Your Data [page 208]

Use Your Model [page 265]

9.1.2.1 Choose a Resource Plan

You can configure SAP AI Core to use different infrastructure resources for different tasks, based on demand. SAP AI Core provides several preconfigured infrastructure bundles called 'resource plans' for this purpose.

Context

Resource plans are used to select resources in workflow and serving templates. Different steps of a workflow can have different resource plans.

In general, if your workload needs GPU acceleration, you should use one of the GPU-enabled resource plans. Otherwise, choose a resource plan based on the anticipated CPU and memory need of your workloads.

Within SAP AI Core, the resource plan is selected via the ai.sap.com/resourcePlan label at pod level. It maps the selected resource plan and takes a string value, which can be any of the following resource plan IDs:

Resource Plan Specifications for AWS

<!-- missing-text -->

<!-- missing-text -->

Restriction

For the Free Tier service plan, only the Starter resource plan is available. Specifying other plans will result in error. For the Standard service plan, all resource plans are available. For more information, see Free Tier [page 45] and Service Plans [page 43].

 Note

There are limits to the default disk storage size for all of these nodes. Datasets that are loaded to the nodes will consume disk space. If you have large data sets (larger than 30 GB), or have large models, you may have to increase the disk size. To do so, use the persistent volume claim in Argo Workflows to specify the required disk size (see Volumes ).

Task overview: Train Your Model [page 217]

Related Information

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

Service Usage Reporting

Usage consumption of services is reported in the SAP BTP cockpit on the Overview page for your global account and on the Overview and Usage Analytics pages of your subaccount. The usage report lists usage in billable measures and non-billable measures. Your final monthly bill is based on the billable measure only. Non-billable measures are displayed for reporting purposes only.

9.1.2.2 Workflow Templates

Here, you'll find a basic workflow example template. Feel free to adjust it to suit your workflow needs.

Workflow templates help manage your training pipelines at the main tenant level. You can store these templates in your git repository and version them as needed. SAP AI Core executes workflows using the Argo Workflows open source project. Argo Workflows is an open-source, container-native workflow engine that orchestrates parallel jobs on Kubernetes. It's implemented as a Kubernetes CRD (Custom Resource Definition).

Workflow templates act as executables . T o map them, you need certain attributes in your template's metadata section. The AI API uses the annotations and labels in the template to find scenarios and executables.

Workflow templates are built on the Argo Workflows engine and are defined as WorkflowTemplates . These are your cluster's workflow definitions. For more information about WorkflowTemplates and sample workflows, see the Argo documentation.

In SAP AI Core, Argo Workflows are used to:

 · Train models

 · Ingest data

 · Preprocess and postprocess data

 · Execute batch inference pipelines

Workflows are executed in batch mode. A workflow can produce multiple output artifacts, but only an output artifact with a globalName is considered to be the final output artifact of the workflow.

For the model training code, SAP AI Core is language-agnostic. However, you need to specify the relevant programming language in the workflow parameters. If you're importing any packages, list them in a separate file named requirements.txt and store it in the same directory.

<!-- missing-text -->

Restriction

The maximum number of workflow templates is limited at tenant level to 50. If you reach this limit, you will receive an error message. To free up space, delete some workflow templates. Alternatively, raise a ticket to increase your quota.

To get started, copy the generic workflow template below and add your own values as required. You can use any text editor with a YAML plugin to create your templates. Workflows support the following parameters:

<!-- missing-text -->

<!-- missing-text -->

```
 Note In the artifact-related parameters above, <argo_artifact_name> refers to the globalName of an output artifact.
```

Generic Workflow Template

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: text-clf-train-tutorial annotations: scenarios.ai.sap.com/description: "SAP developers tutorial scenario" scenarios.ai.sap.com/name: "text-clf-tutorial-scenario" executables.ai.sap.com/description: "Text classification Scikit training executable" executables.ai.sap.com/name: "text-clf-train-tutorial-exec" artifacts.ai.sap.com/text-data.kind: "dataset" artifacts.ai.sap.com/text-model-tutorial.kind: "model" artifacts.ai.sap.com/text-model-tutorial.description: "artifact description" artifacts.ai.sap.com/text-model-tutorial.labels: | {"ext.ai.sap.com/customkey1":"customvalue1", "ext.ai.sap.com/ customkey2":"customvalue2"} labels: scenarios.ai.sap.com/id: "text-clf-tutorial" executables.ai.sap.com/id: "text-clf-train-tutorial" ai.sap.com/version: "1.0.0" spec: imagePullSecrets: - name: <name of your Docker registry secret> entrypoint: text-clf-sk-training arguments: parameters: # placeholder for string like inputs - name: DEPTH # identifier local to this workflow description: description of the parameter default: test templates: - name: text-clf-sk-training metadata: labels: ai.sap.com/resourcePlan: starter inputs: artifacts: - name: text-data path: /app/data/ outputs: artifacts: - name: text-model-tutorial path: /app/model globalName: text-model-tutorial archive: none: {} container: image: "<DOCKER IMAGE URL GOES HERE>" imagePullPolicy: Always command: ["/bin/sh", "-c"] args: - > set -e && echo "---Start Training---" && python /app/src/ train_scikit.py && ls -lR /app/model && echo "---End Training---"
```

 Note

For every container in the template, the command: ["/bin/sh", "-c"] field is mandatory. The contents of the argument can be amended, but must not be empty. The CMD and ENDPOINT specified in the Dockerfile of a container are ignored.

User ID and group ID 65534 is required to run the Docker image. This user has permission to access the files while the application is running. You can check and change the permissions by using the chown and chmod commands.

 Tip

To make sure that the container is working as expected before submitting it to SAP AI Core, run docker run -it --user 65534:65543 &lt;docker-image&gt; locally.

 Note

The archive: none: {} option in the outputs artifacts sections disables automatic archiving for the artifact. If archiving is enabled, the output artifacts are archived to a tar-gzip file before they are uploaded to the object store. If archiving is disabled through archive: none: {} , the artifact will be uploaded to the object store in its current format. If the output artifact points to a directory, the directory will be uploaded 'as is' to the object store. However, object stores in SAP HANA Cloud, data lake do not support this. In this case, remove archive: none: {} and archive the directory to a single tar-gzip file before it is uploaded to the object store.

 Note

When multiple containers are defined in a single WorkflowTemplate, for example when using Sidecar or Container Set, one of the containers must be named 'main'. To fetch logs of another container in the same template using the a GET request to the endpoint /logs , the name of the container must have the 'readable-' prefix. The execution can still run without the 'readable-' prefix, but logs will be inaccessible through the endpoint.. For more information, see Argo Sidecar and Argo Container Set .

Sync an Application Manually

Applications sync with your GitHub repository automatically at intervals of ~3 minutes. Use the endpoint below to manually request a sync: {{apiurl}}/admin/applications/{{appName}}/refresh

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219] List Scenarios [page 225] List Executables [page 228]

Create Configurations [page 234]

```
List Configurations [page 236] Using Artifact Signatures [page 238] Start Training [page 243] Stop Training Instances [page 245] Delete Training Instances [page 249] Efficiency Features [page 253] Retrieve Execution Logs [page 255] Training Schedules [page 258]
```

Stop or Delete Multiple Training Instances

The feature is set to false by default. To enable bulk PATCH operations, your template must contain the following snippet, with the relevant values set to true .

```
eta: M "bulkUpdates": { "executions": false, "deployments": false }
```

Related Information

Argo Workflows

```
Register Your Docker Registry Secret [page 96] Stop Multiple Training Instances [page 247] Delete Multiple Training Instances [page 251]
```

9.1.2.3 List Scenarios

A scenario is an implementation of a specific AI use case within a user's tenant. It consists of a pre-defined set of AI capabilities in the form of executables and templates.

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

```
List Executables [page 228] Create Configurations [page 234] List Configurations [page 236] Using Artifact Signatures [page 238] Start Training [page 243] Stop Training Instances [page 245] Delete Training Instances [page 249] Efficiency Features [page 253] Retrieve Execution Logs [page 255] Training Schedules [page 258]
```

List Scenarios Using Curl

Procedure

```
Run the following code: curl --request GET "$AI_API_URL/v2/lm/scenarios" --header "Authorization: Bearer
```

```
$TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"
```

Results

```
 Output Code { "count":2, "resources":[ { "createdAt":"2021-02-03T18:38:32+00:00", "description":"churn and text class scenario desc", "id":"84fe6957-1145-4183-b682-8f11ca56d060", "labels":[ ], "modifiedAt":"2021-02-04T11:14:02+00:00", "name":"churntextclassscenname" }, { "createdAt":"2021-02-04T14:11:02+00:00", "description":"churn and text class scenario desc", "id":"ae0bd260-41ef-4162-81b0-861bd78a8516", "labels":[ ], "modifiedAt":"2021-02-09T07:35:03+00:00", "name":"churntextclassscenname" } ] }
```

 Note

Only scenarios that have a defined training executable can be created. When a new scenario ID is specified in the workflow template for a training executable, a new scenario will be created along with the training executable.

For now, a new scenario with only a deployment executable cannot be created. A possible work-around is to create a scenario with a dummy training executable and then use the same scenario ID in the serving template.

Get Scenario Versions Using Curl

Procedure

```
Run the following code:
```

```
curl --location --request GET '$API_URL/v2/lm/scenarios/$SCENARIO_ID/versions' \
```

List Scenarios Using Postman

Procedure

 1. Send GET request to the endpoint {{apiurl}}/v2/lm/scenarios

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

 4. On the Header tab, add the following entry:

Key

Value

<!-- missing-text -->

 5. Send the request.

 Note

Only scenarios that have a defined training executable can be created. When a new scenario ID is specified in the workflow template for a training executable, a new scenario will be created along with the training executable.

For now, a new scenario with only a deployment executable cannot be created. A possible work-around is to create a scenario with a dummy training executable and then use the same scenario ID in the serving template.

Get Scenario Versions Using Postman

Procedure

Create a new GET request and enter the URL {{apiurl}}/v2/lm/scenarios/{{scenarioid}}/ versions

9.1.2.4 List Executables

An executable is a reusable template that defines a workflow or pipeline for tasks such as training a machine learning model or creating a deployment. It contains placeholders for input artifacts (datasets or models) and parameters (custom key-pair values) that enable the template to be reused in different scenarios.

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

List Executables Using Curl

Procedure

Run the following code:

```
curl --request GET "$AI_API_URL/v2/lm/scenarios" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"
```

Results

```
 Output Code { "count":4, "resources":[ { "createdAt":"2021-02-04T13:11:01+00:00", "deployable":true, "description":"churn n text class serving executable desc", "id":"pytf-serving", "inputArtifacts":[ { "name":"model_uri" } ], "labels":[ ], "modifiedAt":"2021-02-04T13:11:01+00:00", "name":"churntextclassexecname", "parameters":[ { "name":"modelName", "type":"string", "default": "value", "description": "description of the parameter" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }, { "createdAt":"2021-02-09T07:35:02+00:00", "deployable":true, "description":"churn n text class serving executable desc", "id":"pytf-serving-tracking", "inputArtifacts":[ { "name":"textmodel", "kind": "model", "description": "artifact description", "labels": [ { "key": "ext.ai.sap.com/customkey1", "value": "customvalue1" }, { "key": "ext.ai.sap.com/customkey2", "value": "customvalue2" } ] } ], "labels":[ ], "modifiedAt":"2021-02-09T07:35:02+00:00", "name":"churntextclassexecname", "parameters":[ { "name":"modelName", "type":"string" } ],
```

```
"scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }, { "createdAt":"2021-02-09T07:35:03+00:00", "deployable":false, "description":"churn and text class executable desc", "id":"pytf-training-tracking", "inputArtifacts":[ { "name":"churn-data" }, { "name":"textclass-data" } ], "labels":[ ], "modifiedAt":"2021-02-09T07:35:03+00:00", "name":"churnntextclassexecutablename", "outputArtifacts":[ { "name":"churn-pickle", "kind": "model", "description": "artifact description", "labels": [ { "key": "ext.ai.sap.com/customkey1", "value": "customvalue1" }, { "key": "ext.ai.sap.com/customkey2", "value": "customvalue2" } ] }, { "name":"pytf-model" } ], "parameters":[ { "name":"train-epoch", "type":"string" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }, { "createdAt":"2021-02-04T14:11:02+00:00", "deployable":false, "description":"churn and text class executable desc", "id":"test-training", "inputArtifacts":[ { "name":"churn-data" }, { "name":"textclass-data" } ], "labels":[ ], "modifiedAt":"2021-02-04T14:11:02+00:00", "name":"churnntextclassexecutablename",
```

```
"outputArtifacts":[ { "name":"churn-pickle" }, { "name":"pytf-model" } ], "parameters":[ { "name":"train-epoch", "type":"string" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" } ] }
```

```
 Note The <modifiedAt> field denotes the timestamp of the latest successful sync. The output 1970-01-01T00:00:00+00:00 indicates an error.
```

Get Executable Details Using Curl

Procedure

Run the following code:

```
curl --request GET "{{apiurl}}/v2/lm/scenarios/{{scenarioid}}/executables" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"
```

Results

```
 Output Code { "createdAt":"2021-02-04T14:11:02+00:00", "deployable":false, "description":"churn and text class executable desc", "id":"test-training", "inputArtifacts":[ { "name":"churn-data" }, { "name":"textclass-data" } ],
```

```
"labels":[ ], "modifiedAt":"2021-02-04T14:11:02+00:00", "name":"churnntextclassexecutablename", "outputArtifacts":[ { "name":"churn-pickle" }, { "name":"pytf-model" } ], "parameters":[ { "name":"train-epoch", "type":"string" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }
```

 Note

```
The <modifiedAt> field denotes the timestamp of the latest successful sync. The output 1970-01-01T00:00:00+00:00 indicates an error.
```

List Executables Using Postman

Procedure

 1. Add your scenario ID as the value for the scenarioid environment variable.

 2. Send a GET request to the endpoint {{apiurl}}/v2/lm/scenarios/{{scenarioid}}/executables

 3. On the Authorization tab, set the type to Bearer Token .

 4. Set the token value to {{token}} .

 5. On the Header tab, add the following entry:

Key

Value

```
ai-resource-group <Name of your resourceGroup>default is used)
```

 6. Send the request.

```
 Output Code { count": 3, " "resources": [ { "createdAt": "2021-10-07T20:07:18+00:00", deployable": " true ,
```

```
"description": "Inference executable for text classification with Scikitlearn", "id": "text-clf-infer-tutorial", "input artifacts": [ ...
```

 Note

The &lt;modifiedAt&gt; field denotes the timestamp of the latest successful sync. The output 1970-01-01T00:00:00+00:00 indicates an error.

Get Executable Details Using Postman

Procedure

 1. Add the environment variable executableid and as its value, enter the ID of the executable.

<!-- missing-text -->

 2. Send a GET request to the endpoint {{apiurl}}/v2/lm/scenarios/{{scenarioid}}/ executables/{{executableid}}

 3. On the Authorization tab, set the type to Bearer Token .

 4. Set the token value to {{token}} .

<!-- missing-text -->

 5. On the Header tab, add the following entry:

<!-- missing-text -->

 6. Send the request.

9.1.2.5 Create Configurations

A configuration is a collection of parameters, artifact references (such as datasets or models), and environment settings that are used to instantiate and run an execution or deployment of an executable or template.

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

Using Curl

Procedure

Run the following code:

```
curl --request POST "$AI_API_URL/v2/lm/configurations" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP" --header "ContentType: application/json" \ d '{ -"name": "dummy-configuration",
```

```
"executableId": "'"$EXECUTABLE"'", "scenarioId": "'"$SCENARIO"'", "parameterBindings": [ { "key": "parameter_name_in_template", "value": "some_value" } ], "inputArtifactBindings": [ { "key": "input_artifact_name_in_template", "artifactId": "a4d62a76-52aa-44cf-a789-743246d6d55b" } ] }'
```

```
 Output Code { "id":"f5bf305f-7c3f-4882-9f6b-8b95e3687b9b", "message":"Configuration created" }
```

Using Postman

Procedure

 1. Send a POST request to the endpoint {{apiurl}}/v2/lm/configurations

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

 4. On the Header tab, add the following entries:

KEY

VALUE

```
ai-resource-group <Name of your resourceGroup> Content-Type application/json
```

 5. On the Body tab, select the raw radio button and add the request body as given below:

```
{ "name": "configuration-name", "executableId": "<executable ID>", "scenarioId": "<scenario ID>", "parameterBindings": [ { "key": "<parameter name>", "value": "<value>" } ], "inputArtifactBindings": [ { "key": "<artifact name>", "artifactId": "<artifact ID>"
```

}

<!-- missing-text -->

 6. Send the request.

9.1.2.6 List Configurations

Using Postman

 1. Send a GET request to the endpoint {{apiurl}}/v2/lm/configurations

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

<!-- missing-text -->

 4. On the Header tab, add the following entry:

<!-- missing-text -->

<!-- missing-text -->

 5. Send the request.

<!-- missing-text -->

Using curl

curl --request GET "$AI_API_URL/v2/lm/configurations" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"

<!-- missing-text -->

```
{ "artifactId":"521f7f17-876e-4369-9162-09748b56d27a", "key":"textclass-data" } ], "name":"pytf-demo-config2", "parameterBindings":[ { "key":"train-epoch", "value":"1" } ], "scenarioId":"84fe6957-1145-4183-b682-8f11ca56d060" } ] }
```

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

9.1.2.7 Using Artifact Signatures

Artifact signatures in the form of a hash can be added to output artifacts from executions.

Providing Signatures for Output Artifacts

A hash is an identifier that is generated using the file contents. Hashes can therefore be used to compare two files and verify the integrity of a file. SAP AI Core doesn't create or verify the signatures, but stores them and makes them available. Artifact signatures are created and stored with the artifact.

To provide a signature for an output artifact, specify the AICORE_ARTIFACT_SIGNATURES output parameter in the workflow template. For more information, see Argo Workflows Output Parameters . This parameter specifies a path to a file that will contain the signatures as key-value-pairs in JSON format.

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: signature-example annotations: scenarios.ai.sap.com/description: "Example for signature feature" scenarios.ai.sap.com/name: "signature-example-scenario" executables.ai.sap.com/description: "Output artifact example for signature feature" executables.ai.sap.com/name: "signature-example-output-artifacts" labels: scenarios.ai.sap.com/id: "signature-example-scenario" executables.ai.sap.com/id: "signature-example-out-art" ai.sap.com/version: "1.0.0" spec: entrypoint: output-artifact-signatures-example templates: - name: output-artifact-signatures-example outputs: # specify an output artifact named "dummy-model" artifacts: - name: dummy-model path: /tmp/model globalName: dummy-model # specify an output parameter with name 'AICORE_ARTIFACT_SIGNATURES' # this parameter should point to a file in valid JSON format, containing an object with key-value pairs, where each key is # the globalName of an output artifact and the value is its signature parameters: - name: AICORE_ARTIFACT_SIGNATURES  # the parameter name has to be exactly 'AICORE_ARTIFACT_SIGNATURES' valueFrom: path: /tmp/signatures.json  # the value of the path can be set arbitrary container: image: python:3.11-alpine imagePullPolicy: Always command: ["python", "-c"] args: - | import hashlib, json model = "a dummy string serving as output artifact" model_signature = hashlib.sha256(model.encode("utf-8")).hexdigest()  # create a hash of the dummy model signatures = { # the key is the globalName of the output artifact, the value is its signature "dummy-model": model_signature } # write the dummy model to disk with open("/tmp/model", "w") as f: f.write(model) # write the model signatures dictionary to the path specified in the AICORE_ARTIFACT_SIGNATURES output parameter with open("/tmp/signatures.json", "w") as f: json.dump(signatures, f) print("success")
```

The example template can be used with multiple output artifacts. For each artifact, you add another entry in the signatures object.

The template produces the output artifact dummy-model , with its name-signature key-value pair provided in the AICORE_ARTIFACT_SIGNATURES output parameter. The key is the global name of the output artifact, and the value is the signature. For example:

```
{ "name-of-model1": "signature-of-model1" }
```

These signatures can then be consumed in another execution.

 Note

Signatures must be generated as part of the training workflow, and can't be added retrospectively.

 Note

When you query the execution via the GET {{apiurl}}/v2/lm/executions/{{executionid}} endpoint you won't see the signatures of the output artifacts, as they're kept internally. However, you'll still be able to use them in other executions or deployments.

Multi-Step Workflows

For multistep workflow templates, the AICORE_ARTIFACT_SIGNATURES output parameter needs to be supplied in each step that requires an output artifact and signature to be created. See the following example:

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: signature-example-2 annotations: scenarios.ai.sap.com/description: "Example for signature feature" scenarios.ai.sap.com/name: "signature-example-scenario" executables.ai.sap.com/description: "Output artifact example for multi signature feature" executables.ai.sap.com/name: "signature-example-multi-output-artifacts" labels: scenarios.ai.sap.com/id: "signature-example-scenario" executables.ai.sap.com/id: "signature-example-multi-out-art" ai.sap.com/version: "1.0.0" spec: entrypoint: steps-example templates: # a template with two steps, each having an output artifact - name: steps-example steps: - - output-artifact-signatures-step1 - - output-artifact-signatures-step2 - name: output-artifact-signatures-step1 outputs: # specify an output artifact named "dummy-model1" artifacts: - name: dummy-model1 path: /tmp/model globalName: dummy-model1 # the AICORE_ARTIFACT_SIGNATURES parameter can only hold artifact signatures of the current step, # e.g. it could not hold the signature of "dummy-model2" below, because that is created in a different workflow step
```

```
parameters: - name: AICORE_ARTIFACT_SIGNATURES valueFrom: path: /tmp/signatures.json container: image: python:3.11-alpine imagePullPolicy: Always command: ["python", "-c"] args: - > # ... skipped, as it will be the same as in the previous example - name: output-artifact-signatures-step2 outputs: # specify an output artifact named "dummy-model2" artifacts: - name: dummy-model2 path: /tmp/model globalName: dummy-model2 # we need to define the AICORE_ARTIFACT_SIGNATURES parameter in each workflow step, where we want to provide # signatures for an output artifact parameters: - name: AICORE_ARTIFACT_SIGNATURES valueFrom: path: /tmp/signatures.json container: image: python:3.11-alpine imagePullPolicy: Always command: ["python", "-c"] args: - > # ... skipped, as it will be the same as in the previous example
```

This workflow template creates two output artifacts: dummy-model1 and dummy-model2 , and their namesignature key-value-pairs in JSON format.

Verifying Signatures from Input Artifacts

If you use input artifacts with signatures in another execution, SAP AI Core provides the signatures of the artifacts as an environment variable named AICORE_ARTIFACT_SIGNATURES . The variable is a JSON keyvalue pair, where each key is the input artifact name and the value is its signature. For example:

```
{ "name-of-model1": "signature-of-model1", "name-of-model2": "signature-of-model2" }
```

To extract the signature, include the variable AICORE_ARTIFACT_SIGNATURES in your workflow template, that returns the value from the key of the pair, for example:

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: signature-example-3 annotations: scenarios.ai.sap.com/description: "Example for signature feature" scenarios.ai.sap.com/name: "signature-example-scenario" executables.ai.sap.com/description: "Input artifact example for signature feature" executables.ai.sap.com/name: "signature-example-input-artifacts" labels:
```

```
scenarios.ai.sap.com/id: "signature-example-scenario" executables.ai.sap.com/id: "signature-example-in-art" ai.sap.com/version: "1.0.0" spec: entrypoint: input-artifact-signatures-example templates: - name: input-artifact-signatures-example inputs: artifacts: - name: dummy-model path: /tmp/model container: image: python:3.11-alpine imagePullPolicy: Always command: ["python", "-c"] args: - | import hashlib, json, os # create a hash of the input artifact to compare against the provided signature with open("/tmp/model", "rb") as f: model_hash_actual = hashlib.file_digest(f, "sha256").hexdigest() # SAP AI Core will provide the artifact signature via the AICORE_ARTIFACT_SIGNATURES environment variable # it will be a string in JSON format signatures = json.loads(os.environ["AICORE_ARTIFACT_SIGNATURES"]) model_hash_expected = signatures["dummy-model"]  # the name of the input artifact is the key assert model_hash_expected == model_hash_actual, "signatures did not match" print("success")
```

This template takes an input artifact with the name &lt;dummy-model&gt; , calculates its signature, and compares it to the signature provided by SAP AI Core in the AICORE_ARTIFACT_SIGNATURES environment variable. The AICORE_ARTIFACT_SIGNATURES environment variable is only set when there is at least one input artifact with a signature, otherwise, it won't be set at all. In a multi-step workflow template, the AICORE_ARTIFACT_SIGNATURES environment variable will be supplied to each step of the workflow template, irrespective of where the actual input artifact is defined. In each step, the AICORE_ARTIFACT_SIGNATURES variable will hold the signatures of the input artifacts from all steps.

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255] Training Schedules [page 258]

9.1.2.8 Start Training

Execute Training with Postman

 1. Send a POST request to the endpoint {{apiurl}}/v2/lm/executions . Pass the configurationId in the request body.

<!-- missing-text -->

 2. Check the status of the execution by submitting a GET request to {{apiurl}}/v2/lm/executions/ {{executionid}} .

<!-- missing-text -->

 Note

If the status is dead or pending, there might be errors in the execution. You can check the execution logs for more details, see Retrieve Execution Logs [page 255].

Execute Training with curl

 1. Trigger the training workflow via a configuration.

```
curl --location --request POST '$AI_API_URL/v2/lm/executions' \
```

```
--header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP"
```



```
Output Code { "id": "dea6263e6283321b", "message": "Execution scheduled", "status": "UNKNOWN", "targetStatus": "COMPLETED" }
```

 2. Check the status of the execution.

```
curl --request GET $AI_API_URL/v2/lm/executions/$EXECUTION \ \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP"
```

 Note

If the status is dead or pending, there might be errors in the execution. You can check the execution logs for more details, see Retrieve Execution Logs [page 255].

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

9.1.2.9 Stop Training Instances

You can stop a running execution by submitting a PATCH request to $AI_API_URL/v2/lm/executions/ $EXECUTION \ .

 Note

Stop is only enabled if the status is running or pending.

Stop a Single Training Instance [page 246]

Stop Multiple Training Instances [page 247]

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

9.1.2.9.1 Stop a Single Training Instance

Using Postman

<!-- missing-text -->

Using curl

```
curl --request PATCH $AI_API_URL/v2/lm/executions/$EXECUTION \ --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP" --header "Content-Type: application/json" d '{ -"targetStatus": "STOPPED" }'
```

<!-- missing-text -->

Parent topic: Stop Training Instances [page 245]

Related Information

Stop Multiple Training Instances [page 247]

9.1.2.9.2 Stop Multiple Training Instances

bulkUpdates is a meta capability endpoint of the AI API. It enables or disables bulk PATCH operations. For more information, see AI API Overview [page 36].

The feature is set to false by default. To enable bulk PATCH operations, your template must contain the following snippet, with the relevant values set to true .

```
eta: M "bulkUpdates": { "executions": false, "deployments": false }
```

About bulkUpdates :

 · The maximum number of updates per request is 100.

 · Your bulk update can contain a mixture of STOP and DELETE requests.

 · Only running or pending executions or deployments can be stopped.

 · Only stopped dead , or unknown executions or deployments can be deleted.

 · An ID can only appear once per bulk request. For multiple modifications of the same ID, multiple requests are needed.

Using Postman

Send a bulk PATCH request to the endpoint: - /executions

Update the request body to:

```
{ "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED" } ]
```

```
}  Output Code { "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Execution modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Execution modification scheduled" } ] }
```

Using curl

Update the request body to:

```
curl --request PATCH  - /executions \ --header {"executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED" } ] }
```

```
 Output Code { "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Execution modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Execution modification scheduled" } ] }
```

Parent topic: Stop Training Instances [page 245]

Related Information

Stop a Single Training Instance [page 246] AI API Overview [page 36]

9.1.2.10  Delete Training Instances

Deleting a training instance releases the SAP AI Core resources that it used.

<!-- missing-text -->

 Restriction

If your execution is running, you must stop it first. Y ou can stop a running execution by submitting a PATCH request to $AI_API_URL/v2/lm/executions/$EXECUTION \ . For more information, see Stop a Single Training Instance [page 246] and Stop Multiple Training Instances [page 247].

Delete a Single Training Instance [page 249]

Delete Multiple Training Instances [page 251]

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

9.1.2.10.1  Delete a Single Training Instance

Deleting a training instance releases the SAP AI Core resources that it used.

<!-- missing-text -->

Restriction

If your execution is running, you must stop it first. You can stop a running execution by submitting a PATCH request to $AI_API_URL/v2/lm/executions/$EXECUTION \ . For more information, see Stop a Single Training Instance [page 246] and Stop Multiple Training Instances [page 247].

Parent topic: Delete Training Instances [page 249]

Related Information

Delete Multiple Training Instances [page 251]

Using Curl

Procedure

Run the following code:

```
curl --request DELETE $AI_API_URL/v2/lm/executions/$EXECUTION \ \ --header "Authorization: Bearer $TOKEN" \
```

```
--header "ai-resource-group: $RESOURCE_GROUP"
```

```
 Output Code { "id": "ee6769e4dc19c0fd", "message": "Deletion scheduled", "targetStatus": "DELETED" }
```

Using Postman

Procedure

Send a DELETE request to {{apiurl}}/v2/lm/executions/{{executionid}} . The header for this request is: AI-Resource-Group: {YOUR-Resource-Group} .

```
{ "targetStatus": "DELETED" }
```

Results

```
 Output Code { "id": "e8c53facc2bfb87a", "message": "Deletion scheduled", "targetStatus": "DELETED" }
```

9.1.2.10.2  Delete Multiple Training Instances

Deleting a training instance releases the SAP AI Core resources that it used.

<!-- missing-text -->

 Restriction

If your execution is running, you must stop it first. Y ou can stop a running execution by submitting a PATCH request to $AI_API_URL/v2/lm/executions/$EXECUTION \ . For more information, see Stop a Single Training Instance [page 246] and Stop Multiple Training Instances [page 247].

bulkUpdates is a meta capability endpoint of the AI API. It enables or disables bulk PATCH operations. For more information, see AI API Overview [page 36].

The feature is set to false by default. To enable bulk PATCH operations, your template must contain the following snippet, with the relevant values set to true .

```
eta: M "bulkUpdates": { "executions": false, "deployments": false }
```

About bulkUpdates :

 · The maximum number of updates per request is 100.

 · Your bulk update can contain a mixture of STOP and DELETE requests.

 · Only running or pending executions or deployments can be stopped.

 · Only stopped dead , or unknown executions or deployments can be deleted.

 · An ID can only appear once per bulk request. For multiple modifications of the same ID, multiple requests are needed.

Parent topic: Delete Training Instances [page 249]

Related Information

Delete a Single Training Instance [page 249]

Using Curl

Procedure

Update the request body to:

```
curl --request PATCH  - /executions \ --header {"executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED" } ] }
```

```
 Output Code { "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Execution modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Execution modification scheduled" } ] }
```

Using Postman

Procedure

Send a bulk PATCH request to the endpoint: - /executions

Update the request body to:

```
{ "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED" } ] }
```

```
 Output Code { "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Execution modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Execution modification scheduled" } ] }
```

9.1.2.11  Efficiency Features

Discover features of the SAP AI Core runtime that improve efficiency and help manage resource consumption.

Tenant Warm Node Pool

Tenant warm nodes allow tenants to reserve nodes from specific resource plan types, ensuring that a predefined number of nodes exist in the cluster. The reserved nodes can then be used during model training and serving. Reserving nodes reduces waiting time at workload startup, but incurs costs in line with node use, whether consumed or not.

Mechanism of Node Reservation

 1. The tenant specifies number of nodes to reserve.

 2. An execution or deployment utilizes a reserved node, a replacement reserve node of the same resource plan type is requested from the hyperscaler.

The number of reserved nodes specified by the tenant are consistently available for use.

The minimum number of reserved nodes is 0.

About Node Reservation

 · The number of reserved nodes specified by the tenant are consistently available for use.

 · The minimum number of reserved nodes is 0.

 · The maximum number of reserved nodes is 10.

 · The default number of reserved nodes is 0.

Reserve Nodes Using Postman

 1. Send a PATCH request to the endpoint {{apiurl}}/v2/admin/resources/nodes

 2. Provide the resource plan type and quantity of nodes to reserve in the request body in JSON format:

```
{ "resourcePlans": [
```

```
{ "name": "infer.l", "request": 1 }, { "name": "infer.m", "request": 1 }, { "name": "train.l", "request": 1 } ... ] }
```

Reserve Nodes Using curl

Submit a PATCH request to the endpoint {{apiurl}}/v2/admin/resources/nodes

```
curl --request PATCH $AI_API_URL/v2/admin/resources/nodes \ -data-raw '{ -"resourcePlans": [ { "name": "infer.l", "request": 1 }, { "name": "infer.m", "request": 1 }, { "name": "train.l", "request": 1 } ] }'
```

 Remember

All reserved nodes are charged at the same rate as nodes used during model training and serving.

Check Reserve Node Status Using Postman

Send a GET request to the endpoint {{apiurl}}/v2/admin/resources/nodes

Check Reserve Node Status Using curl

```
curl --request GET $AI_API_URL/v2/resources/nodes
```

```
 Output Code { "resourcePlans": { "infer.l": { "provisioned": 1, "requested": 1 }, "infer.m": { "provisioned": 1, "requested": 1 }, "train.l": {
```

```
"provisioned": 1, "requested": 1 } } }
```

 · requested : the number of reserve nodes requested by the tenant.

 · provisioned : the number of reserve nodes that are currently present in the cluster.

Update Quantities of Reserved Nodes

To update the number of nodes reserved, repeat the reservation procedure, with updated quantities in the request field.

Delete Reserved Nodes

To delete reserved nodes, repeat the reservation procedure, with quantities in the request field set to 0 .

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Retrieve Execution Logs [page 255]

Training Schedules [page 258]

Service Plans [page 43]

9.1.2.12  Retrieve Execution Logs

accessed in the deployment and execution logs.

You can retrieve the logs for a specific deployment or execution by submitting a GET request. Use the following endpoints to retrieve the logs:

 · GET /v2/lm/deployments/{deploymentId}/logs

 · GET /v2/lm/executions/{executionId}/logs

Query parameters include:

 · start : the start time for a query as a string, in RFC 3339-compliant datetime format. Defaults to 1 hour before the current. Example: 2021-05-19T00:00:14.347Z

 · end : the end time for a query as a string, in RFC 3339-compliant datetime format. Defaults to the current time. Example: 2021-05-19T00:00:14.347Z

 · $top : the maximum number of entries returned. The default value is 1000; the upper limit is 5000.

 · $order : the sort order of logs, either asc (for ascending, earliest in the order will appear at the top of the list) or desc (for descending, most recent in the order will appear at the top of the list). Note the default value is asc .

For example:

 · /v2/lm/deployments/{deploymentId}/logs? start=2021-05-19T00:00:14.347Z&amp;end=2021-05-19T01:00:14.347Z&amp;$top=100&amp;$order=asc - returns the first

 100 lines of a deployment log between 2021-05-19T00:00:14.347Z and 2021-05-19T01:00:14.347Z

 · /v2/lm/deployments/{deploymentId}/logs - returns deployment logs from the preceding hour

 · /v2/lm/executions/{executionId}/logs - returns execution logs from the preceding hour

Using Postman

 1. Send a GET request to the endpoint {{apiurl}}/v2/lm/executions/{{executionId}}/logs .

<!-- missing-text -->

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

 4. On the Header tab, add the following entry:

<!-- missing-text -->

 5. Send the request.

Using curl

```
curl --request GET "$AI_API_URL/v2/lm/executions/$EXCUTION_ID/logs? start=2021-05-19T00:00:14.347Z" --header "Authorization: Bearer $TOKEN" --header
```

```
"ai-resource-group: $RESOURCE_GROUP"
```

Sample Output

For example, see the following JSON output from the API.

```
 Output Code { "data": { "result": [ { "container": "storage-initializer", "msg": "[I 210531 08:20:51 initializer-entrypoint:13] Initializing, args: src_uri [gs://kserve-samples/models/tensorflow/flowers] dest_path[ [/mnt/models]\n", "pod": "tfs-dep-i543026-predictor-default-v6nf5deployment-8b58c8ddcfdx", "stream": "stderr", "timestamp": "2021-05-31T08:20:51.334+00:00" }, { "container": "storage-initializer", "msg": "[I 210531 08:20:51 storage:45] Copying contents of gs://kserve-samples/models/tensorflow/flowers to local\n", "pod": "tfs-dep-i543026-predictor-default-v6nf5deployment-8b58c8ddcfdx", "stream": "stderr", "timestamp": "2021-05-31T08:20:51.335+00:00" }, { "container": "storage-initializer", "msg": "[W 210531 08:20:51 _metadata:104] Compute Engine Metadata server unavailable onattempt 1 of 3. Reason: [Errno 111] Connection refused\n", "pod": "tfs-dep-i543026-predictor-default-v6nf5deployment-8b58c8ddcfdx", "stream": "stderr", "timestamp": "2021-05-31T08:20:51.338+00:00" }, ... ]
```

```
} }
```

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Training Schedules [page 258]

Retrieve Deployment Logs [page 304]

9.1.2.13  Training Schedules

In order to run executions periodically, you can define a schedule to start your executions automatically. To do this, you must have prepared your configuration for the execution.

Training schedules have a start and end timestamp. The start timestamp defines when the schedule will first be run to generate executions. The end timestamp defines when the schedule will expire.

A schedule that has not yet expired has ACTIVE status. Upon expiry, the status will changed to INACTIVE, and no further executions will start.

The format for these timestamps is defined by RFC3339 section 5.6, without fractions of seconds, for example: 2023-02-09T12:53:47Z . All timestamps are interpreted in UTC time. For more information, see RFC3339 section 5.6 and AI API Overview [page 36].

Create a Training Schedule [page 259]

List Executions Created by a Training Schedule [page 261]

Change an Existing Training Schedule [page 263]

Delete a Training Schedule [page 264]

Parent topic: Train Your Model [page 217]

Related Information

Choose a Resource Plan [page 219]

Workflow Templates [page 221]

List Scenarios [page 225]

List Executables [page 228]

Create Configurations [page 234]

List Configurations [page 236]

Using Artifact Signatures [page 238]

Start Training [page 243]

Stop Training Instances [page 245]

Delete Training Instances [page 249]

Efficiency Features [page 253]

Retrieve Execution Logs [page 255]

9.1.2.13.1  Create a Training Schedule

Parent topic: Training Schedules [page 258]

Related Information

List Executions Created by a Training Schedule [page 261] Change an Existing Training Schedule [page 263] Delete a Training Schedule [page 264]

Using Curl

Procedure

 1. Create a training schedule by submitting a POST request to $AI_API_URL/v2/lm/ executionSchedules .

In the request body you need to define:

 · The schedule in cron format. For more information, see Cron Formatting

 · A name

 · The configurationId that you want to use for the training instance

 · A start timestamp when the schedule should become active

 · An end timestamp when the schedule should become inactive

```
curl --location --request POST '$AI_API_URL/v2/lm/executionSchedules/ $EXECUTION_SCHEDULE' \\ -header 'Authorization: Bearer $TOKEN' \ ---header 'ai-resource-group: $RESOURCE_GROUP' \ --data-raw '{ \ "cron": "0 * * * *",\ "name": "Hourly training run",\ "configurationId": "35b60591-1e48-473b-9b44-d5f8e9e4de32",\ "start": "2023-02-10T10:50:31Z",\ "end": "2023-02-10T12:55:31Z"\ }'
```

 2. Check the status of the training schedule by submitting a GET request:

```
curl --location -- request GET '$AI_API_URL/v2/lm/executionSchedules/ $EXECUTION_SCHEDULE' \\ -header 'Authorization: Bearer $TOKEN' \ --header 'ai-resource-group: $RESOURCE_GROUP' -
```

Using Postman

Procedure

Create a training schedule by submitting a POST request to {{apiurl}}/v2/lm/executionSchedules .

In the request body you need to define:

 · The schedule in cron format. For more information, see Cron Formatting

 · A name

 · The configurationId that you want to use for the training instance

 · A start timestamp when the schedule should become active

 · An end timestamp when the schedule should become inactive

```
"configurationId": "36b60691-1e48-473b-9b44-d6f8e9r4de32",
```

```
{ "cron": "0 * * * * *", "name": "Hourly training run", "start": "2023-02-08T10:50:31Z", "end": "2023-02-08T12:56:31Z" }
```

```
 Output Code { "id": "799b4e67-a213-40b9-9550-637fde75dbda", "message": "Execution Schedule created" }
```

9.1.2.13.2  List Executions Created by a Training Schedule

Using Curl

Get a list of executions created by a training schedule by submitting a GET request:

```
curl --location -- request GET '$AI_API_URL/v2/lm/executionSchedules/
```

```
$EXECUTION_SCHEDULE' \\ -header 'Authorization: Bearer $TOKEN' \ --header 'ai-resource-group: $RESOURCE_GROUP' -
```

Using Postman

Get a list of executions created by a training schedule by submitting a GET request to {{apiurl}}/v2/lm/ executions?executionScheduleId={{executionScheduleId}}

<!-- missing-text -->

Related Information

Create a Training Schedule [page 259] Change an Existing Training Schedule [page 263] Delete a Training Schedule [page 264]

9.1.2.13.3  Change an Existing Training Schedule

You can change the cron definition, start, end, and configuration of an existing training schedule.

You cannot change the name field.

Parent topic: Training Schedules [page 258]

Related Information

```
Create a Training Schedule [page 259] List Executions Created by a Training Schedule [page 261] Delete a Training Schedule [page 264]
```

Using Curl

Procedure

Submit a PATCH request:

```
curl --location --request PATCH $AI_API_URL/v2/lm/executions?
```

```
executionScheduleId=$EXECUTION_SCHEDULE -header 'Authorization: Bearer $TOKEN' \ ---header 'ai-resource-group: $RESOURCE_GROUP' \ --data-raw '{ "cron": "0 0 * * *" ' }
```

Using Postman

Procedure

```
Send your changes in a PATCH request to {{apiurl}}/v2/lm/executionSchedules/ {{executionScheduleId}} . { "cron": "0 * * * * *", }  Output Code { "id": "799b4e67-a213-40b9-9550-637fde75dbda",
```

```
"message": "Execution Schedule modified" }
```

Pause or Resume a Training Schedule

The status parameter pauses or resumes a schedule:

 · &lt;INACTIVE&gt; : pauses a schedule.

 · &lt;ACTIVE&gt; : resumes a schedule.

9.1.2.13.4  Delete a Training Schedule

You can delete training schedules that are in states ACTIVE and INACTIVE.

Parent topic: Training Schedules [page 258]

Related Information

Create a Training Schedule [page 259] List Executions Created by a Training Schedule [page 261] Change an Existing Training Schedule [page 263]

Using Curl

Context

Procedure

Submit a DELETE request:

```
curl --location -- request DELETE '$AI_API_URL/v2/lm/executionSchedules/
```

```
$EXECUTION_SCHEDULE' \\ -header 'Authorization: Bearer $TOKEN' \ --header 'ai-resource-group: $RESOURCE_GROUP' -
```

Using Postman

Context

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/lm/executionSchedules/ {{executionScheduleId}}

```
 Output Code { "id": "799b4e67-a213-40b9-9550-637fde75dbda", "message": "Execution Schedule deleted" }
```

9.1.3  Use Your Model

You deploy your AI learning model to run inferences against it.

SAP AI Core provides the means to run AI training and offline batch inference jobs, using the required computational resources (such as GPUs) efficiently.

You can use a model trained in SAP AI Core, or your own pretrained model for deployment and inferencing.

The model serving service includes:

 · Serving of AI models through fast, secure, and resource-efficient inference endpoints that can be integrated into online inference scenarios.

 · Flexibility to deploy user-supplied Docker images.

 · Cost-efficient serving (for example, by autoscaling or serverless support or multimodel serving).

 · Parameterized serving templates for main tenants to manage their serving instances.

 · Serving instances in resource group namespaces with specific parameters and resource isolation.

The serving templates are used to create model servers. When a model server is up and running, it processes incoming inference requests and returns the results from the AI learning model. Serving templates define how a model is to be deployed. For the model to work, you must provide a spec in line with the Serving Templates [page 268], together with input parameters and artifacts.

Restrict the number of nodes used for processing by specifying parameters minReplicas and maxReplicas.

Choose a Resource Plan [page 266]

You can configure SAP AI Core to use different infrastructure resources for different tasks, based on demand. SAP AI Core provides several preconfigured infrastructure bundles called 'resource plans' for this purpose.

PUBLIC

265

Serving Templates [page 268]

You use serving templates to manage your serving instances at the level of the main tenant. Serving templates define how a model is to be deployed.

List Executables [page 280]

An executable is a reusable template that defines a workflow or pipeline for tasks such as training a machine learning model or creating a deployment. It contains placeholders for input artifacts (datasets or models) and parameters (custom key-pair values) that enable the template to be reused in different scenarios.. You can list all of the executables in a resource group and get details of specific executables from a resource group. Serving templates are mapped to deployment executables.

Deploy Models [page 285]

Inferencing [page 288]

Update a Deployment [page 290]

Stop Deployments [page 292]

Delete Deployments [page 296]

Efficiency Features [page 301]

Discover features of the SAP AI Core runtime that improve efficiency and help manage resource consumption.

Retrieve Deployment Logs [page 304]

accessed in the deployment and execution logs.

Parent topic: ML Operations [page 207]

Related Information

Connect Your Data [page 208]

Train Your Model [page 217]

9.1.3.1 Choose a Resource Plan

You can configure SAP AI Core to use different infrastructure resources for different tasks, based on demand. SAP AI Core provides several preconfigured infrastructure bundles called 'resource plans' for this purpose.

Context

Resource plans are used to select resources in workflow and serving templates. Different steps of a workflow can have different resource plans.

In general, if your workload needs GPU acceleration, you should use one of the GPU-enabled resource plans. Otherwise, choose a resource plan based on the anticipated CPU and memory need of your workloads.

Within SAP AI Core, the resource plan is selected via the ai.sap.com/resourcePlan label at pod level. It maps the selected resource plan and takes a string value, which can be any of the following resource plan IDs:

Resource Plan Specifications for AWS

Code to Allocate Resources

<!-- missing-text -->

<!-- missing-text -->

Restriction

For the Free Tier service plan, only the Starter resource plan is available. Specifying other plans will result in error. For the Standard service plan, all resource plans are available. For more information, see Free Tier [page 45] and Service Plans [page 43].

 Note

There are limits to the default disk storage size for all of these nodes. Datasets that are loaded to the nodes will consume disk space. If you have large data sets (larger than 30 GB), or have large models, you may have to increase the disk size. To do so, use the persistent volume claim in Argo Workflows to specify the required disk size (see Volumes ).

Task overview: Use Your Model [page 265]

Related Information

Serving Templates [page 268] List Executables [page 280] Deploy Models [page 285] Inferencing [page 288] Update a Deployment [page 290] Stop Deployments [page 292] Delete Deployments [page 296] Efficiency Features [page 301] Retrieve Deployment Logs [page 304]

Service Usage Reporting

Usage consumption of services is reported in the SAP BTP cockpit on the Overview page for your global account and on the Overview and Usage Analytics pages of your subaccount. The usage report lists usage in billable measures and non-billable measures. Your final monthly bill is based on the billable measure only. Non-billable measures are displayed for reporting purposes only.

9.1.3.2 Serving Templates

You use serving templates to manage your serving instances at the level of the main tenant. Serving templates define how a model is to be deployed.

Serving templates are used to deploy one or more trained models on a model server, which process incoming inference requests and returns the results from the machine learning model. Serving templates are stored in your git repository, where you can version them as required.

In SAP AI Core, serving templates are mapped as executables . Mapping requires certain attributes in the metadata section of your template.

Models are deployed using a simple Kubernetes Custom Resource Definition (CRD), which is provided by KServe. To deploy a model, you must provide a YAML specification that follows the KServe specification and that defines the required input parameters and artifacts.

<!-- missing-text -->

Restriction

The maximum number of serving templates is limited at tenant level to 50. If you reach this limit, you will receive an error message. To free up space, delete some serving templates. Alternatively, raise a ticket to increase your quota.

To get started, copy the generic serving template below and add your own values as required. You can use any text editor with a YAML plugin to create the templates. Your YAML specification must follow the KServe specs and define the required input parameters and artifacts.

<!-- missing-text -->

<!-- missing-text -->

Generic Serving Template

```
apiVersion: ai.sap.com/v1alpha1 ind: ServingTemplate k metadata: name: text-clf-infer-tutorial annotations: scenarios.ai.sap.com/description: "SAP developers tutorial scenario" scenarios.ai.sap.com/name: "text-clf-tutorial-scenario" executables.ai.sap.com/description: "Inference executable for text classification with Scikit-learn" executables.ai.sap.com/name: "text-clf-infer-tutorial-exec" artifacts.ai.sap.com/textmodel.kind: "model" artifacts.ai.sap.com/textmodel.description: "artifact description" artifacts.ai.sap.com/textmodel.labels: | {"ext.ai.sap.com/customkey1":"customvalue1", "ext.ai.sap.com/ customkey2":"customvalue2"} labels: scenarios.ai.sap.com/id: "text-clf-tutorial" ai.sap.com/version: "1.0.0" spec: inputs: parameters: - name: modelName default: value type: string description: description of the parameter artifacts:
```

```
- name: textmodel template: apiVersion: "serving.kserve.io/v1beta1" metadata: annotations: | autoscaling.knative.dev/metric: concurrency autoscaling.knative.dev/target: 1 autoscaling.knative.dev/targetBurstCapacity: 0 labels: | ai.sap.com/resourcePlan: starter spec: | predictor: imagePullSecrets: - name: <Name of your Docker registry secret> minReplicas: 0 maxReplicas: 5 containers: - name: kserve-container image: "<DOCKER IMAGE URL GOES HERE>" ports: - containerPort: 9001 protocol: TCP env: - name: STORAGE_URI value: "{{inputs.artifacts.textmodel}}"
```

 · STORAGE_URI environment variable name is currently hard-coded in KServe to indicate that the model should be downloaded when a custom predictor is configured with the env var STORAGE_URI.

 · /mnt/models is currently hard-coded in KServe. When you try this example with your own docker container, read the models from that path.

Sync an Application Manually

Applications sync with your GitHub repository automatically at intervals of ~3 minutes. Use the endpoint below to manually request a sync: {{apiurl}}/admin/applications/{{appName}}/refresh

Parent topic: Use Your Model [page 265]

Related Information

```
Choose a Resource Plan [page 266] List Executables [page 280] Deploy Models [page 285] Inferencing [page 288] Update a Deployment [page 290] Stop Deployments [page 292] Delete Deployments [page 296] Efficiency Features [page 301] Retrieve Deployment Logs [page 304]
```

Stop or Delete Multiple Deployments

The feature is set to false by default. To enable bulk PATCH operations, your template must contain the following snippet, with the relevant values set to true .

```
eta: "executions": false, "deployments": false
```

```
M "bulkUpdates": { }
```

Related Information

```
Stop Multiple Deployments [page 294] Delete Multiple Deployments [page 298]
```

9.1.3.2.1 Serving Template API Reference

API Schema Spec ai.sap.com/v1alpha1 [page 272]

Package valpha1 contains API Schema definitions for the serving v1alpha1 API group ( ai.sap.com/v1alpha1 ). KServe Spec serving.kserve.io/v1beta1 [page 275] Package v1beta1 contains API Schema definitions for the serving v1beta1 API group

( serving.kserve.io/v1beta1 ).

9.1.3.2.1.1   API Schema Spec ai.sap.com/v1alpha1

Package valpha1 contains API Schema definitions for the serving v1alpha1 API group ( ai.sap.com/ v1alpha1 ).

ServingTemplate

ServingTemplate is a type of executable that specifies how a model is to be served.

<!-- missing-text -->

ServingTemplate Metadata Annotations

(Appears on ServingTemplate [page 272])

A subset of supported Kubernetes Metadata Annotations

<!-- missing-text -->

ServingTemplate Metadata Labels

(Appears on ServingTemplate [page 272])

A subset of supported Kubernetes Metadata Labels

Field

Description

<!-- missing-text -->

ServingTemplate Spec

(Appears on ServingTemplate [page 272])

ServingTemplate spec is the top level type for this resource.

<!-- missing-text -->

ServingTemplate Input Parameters

(Appears on ServingTemplate Spec [page 274])

Input parameters required for KServe InferenceService Template [page 275] spec

Field

Description

<!-- missing-text -->

ServingTemplate Input Artifacts

(Appears on ServingTemplate Spec [page 274])

Input artifacts required for KServe InferenceService Template [page 275] spec

<!-- missing-text -->

KServe InferenceService Template

(Appears on:ServingTemplate Spec [page 274])

KServe InferenceService template

<!-- missing-text -->

Parent topic: Serving Template API Reference [page 272]

Related Information

KServe Spec serving.kserve.io/v1beta1 [page 275]

9.1.3.2.1.2  KServe Spec serving.kserve.io/v1beta1

Package v1beta1 contains API Schema definitions for the serving v1beta1 API group ( serving.kserve.io/v1beta1 ).

InferenceService

InferenceService is the schema for the InferenceServices API.

<!-- missing-text -->

KServe Metadata Annotations

(Appears on InferenceService [page 275])

A multiline string containing a subset of supported KServe Metadata Labels. You can add a placeholder using {{variable_name}} to define a variable and replace the value dynamically. For example:

autoscaling.knative.dev/target: "{{inputs.parameters.MyAutoScalingTarget}}"

<!-- missing-text -->

 Example

metadata:

annotations: |

autoscaling.knative.dev/metric: rps

autoscaling.knative.dev/target:

{{inputs.parameters.MyAutoScalingTarget}}

autoscaling.knative.dev/targetBurstCapacity: 70

autoscaling.knative.dev/window: 10s

KServe Metadata Labels

(Appears on InferenceService [page 275])

A multiline string containing a subset of supported KServe Metadata Labels. You can add a placeholder using {{variable_name}} to define a variable and replace the resource plan dynamically. For example:

```
ai.sap.com/resourcePlan: "{{inputs.parameters.MyResourcePlan}}"
```

<!-- missing-text -->

```
 Example metadata: labels: | ai.sap.com/resourcePlan: basic
```

KServe InferenceServiceSpec

(Appears on InferenceService [page 275])

InferenceServiceSpec is the top level type for this resource.

<!-- missing-text -->

KServe PredictorSpec

(Appears on KServe InferenceServiceSpec [page 277])

PredictorSpec defines the configuration for a predictor. The following fields follow a '1-of' semantic. Users must specify exactly one spec.

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

```
 Example spec: | predictor: imagePullSecrets: - name: [DOCKER REGISTRY SECRET GOES HERE] minReplicas: 1 maxReplicas: 5 containers: - name: kserve-container image: "[DOCKER IMAGE URL GOES HERE]" ports: - containerPort: 9001 protocol: TCP env: - name: STORAGE_URI value: "{{inputs.artifacts.textmodel}}"
```

Parent topic: Serving Template API Reference [page 272]

Related Information

API Schema Spec ai.sap.com/v1alpha1 [page 272]

9.1.3.2.2 Change Serving Template and Update Deployments

If you change a serving template, you can automatically update the deployments that are associated with that template.

Prerequisites

The executables.ai.sap.com/cascade-update-deployments parameter is present and set to true in the serving template. This allows any change to the serving template to trigger an automatic update of the associated deployments.

Procedure

 1. Make any required changes to the fields for the serving template. Changes to the following are supported:

 · spec.template.spec.default.predictor.minReplicas

 · spec.template.spec.default.predictor.custom.container.image

 · spec.template.spec.default.predictor.custom.container.ports

 · spec.template.spec.default.predictor.custom.container.env

 2. Make any required changes to the fields for the beta serving template. Changes to the following are supported:

 · spec.template.spec.predictor.containers.image

 · spec.template.spec.predictor.containers.ports

 · spec.template.spec.predictor.containers.env

 · Modify a value

 · Add a name-value pair

 · Parameterized values can be updated to hardcoded and vice versa

9.1.3.3 List Executables

An executable is a reusable template that defines a workflow or pipeline for tasks such as training a machine learning model or creating a deployment. It contains placeholders for input artifacts (datasets or models) and parameters (custom key-pair values) that enable the template to be reused in different scenarios.. You can list all of the executables in a resource group and get details of specific executables from a resource group. Serving templates are mapped to deployment executables.

Parent topic: Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

Serving Templates [page 268]

Deploy Models [page 285]

Inferencing [page 288]

Update a Deployment [page 290]

Stop Deployments [page 292]

Delete Deployments [page 296]

Efficiency Features [page 301]

Retrieve Deployment Logs [page 304]

Using Curl

Procedure

Run the following code:

```
curl --request GET "$AI_API_URL/v2/lm/scenarios" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"
```

Results

```
 Output Code { "count":4, "resources":[ { "createdAt":"2021-02-04T13:11:01+00:00", "deployable":true, "description":"churn n text class serving executable desc", "id":"pytf-serving", "inputArtifacts":[ { "name":"model_uri" } ], "labels":[ ], "modifiedAt":"2021-02-04T13:11:01+00:00", "name":"churntextclassexecname", "parameters":[ { "name":"modelName", "type":"string", "default": "value", "description": "description of the parameter" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }, { "createdAt":"2021-02-09T07:35:02+00:00", "deployable":true, "description":"churn n text class serving executable desc", "id":"pytf-serving-tracking", "inputArtifacts":[ { "name":"textmodel", "kind": "model", "description": "artifact description", "labels": [ { "key": "ext.ai.sap.com/customkey1", "value": "customvalue1" }, {
```

```
"key": "ext.ai.sap.com/customkey2", "value": "customvalue2" } ] } ], "labels":[ ], "modifiedAt":"2021-02-09T07:35:02+00:00", "name":"churntextclassexecname", "parameters":[ { "name":"modelName", "type":"string" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }, { "createdAt":"2021-02-09T07:35:03+00:00", "deployable":false, "description":"churn and text class executable desc", "id":"pytf-training-tracking", "inputArtifacts":[ { "name":"churn-data" }, { "name":"textclass-data" } ], "labels":[ ], "modifiedAt":"2021-02-09T07:35:03+00:00", "name":"churnntextclassexecutablename", "outputArtifacts":[ { "name":"churn-pickle", "kind": "model", "description": "artifact description", "labels": [ { "key": "ext.ai.sap.com/customkey1", "value": "customvalue1" }, { "key": "ext.ai.sap.com/customkey2", "value": "customvalue2" } ] }, { "name":"pytf-model" } ], "parameters":[ { "name":"train-epoch", "type":"string" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }, {
```

```
"createdAt":"2021-02-04T14:11:02+00:00", "deployable":false, "description":"churn and text class executable desc", "id":"test-training", "inputArtifacts":[ { "name":"churn-data" }, { "name":"textclass-data" } ], "labels":[ ], "modifiedAt":"2021-02-04T14:11:02+00:00", "name":"churnntextclassexecutablename", "outputArtifacts":[ { "name":"churn-pickle" }, { "name":"pytf-model" } ], "parameters":[ { "name":"train-epoch", "type":"string" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" } ] }
```

 Note

The &lt;modifiedAt&gt; field denotes the timestamp of the latest successful sync. The output 1970-01-01T00:00:00+00:00 indicates an error.

Using Postman

Procedure

 1. Add your scenario ID as the value for the scenarioid environment variable.

 2. Send a GET request to the endpoint {{apiurl}}/v2/lm/scenarios/{{scenarioid}}/executables

 3. On the Authorization tab, set the type to Bearer Token .

 4. Set the token value to {{token}} .

 5. On the Header tab, add the following entry:

Key

Value

```
ai-resource-group <Name of your resourceGroup>default is used)
```

 6. Send the request.

```
 Output Code { count": 3, " "resources": [ { "createdAt": "2021-10-07T20:07:18+00:00", deployable": " true , description": "Inference executable for text classification with Scikit-" learn", "id": "text-clf-infer-tutorial", "input artifacts": [ ...
```

 Note

The &lt;modifiedAt&gt; field denotes the timestamp of the latest successful sync. The output 1970-01-01T00:00:00+00:00 indicates an error.

Get Executable Details with curl

Procedure

Run the following code:

```
curl --request GET "$AI_API_URL/v2/lm/scenarios" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"
```

```
 Output Code { "createdAt":"2021-02-04T14:11:02+00:00", "deployable":false, "description":"churn and text class executable desc", "id":"test-training", "inputArtifacts":[ { "name":"churn-data" }, { "name":"textclass-data" } ], "labels":[ ], "modifiedAt":"2021-02-04T14:11:02+00:00", "name":"churnntextclassexecutablename",
```

```
"outputArtifacts":[ { "name":"churn-pickle" }, { "name":"pytf-model" } ], "parameters":[ { "name":"train-epoch", "type":"string" } ], "scenarioId":"ae0bd260-41ef-4162-81b0-861bd78a8516", "versionId":"0.0.1" }
```

 Note

The &lt;modifiedAt&gt; field denotes the timestamp of the latest successful sync. The output 1970-01-01T00:00:00+00:00 indicates an error.

Get Executable Details with Postman

Procedure

 1. Add the environment variable executableid and as its value, enter the ID of the executable.

 2. Send a GET request to the endpoint {{apiurl}}/v2/lm/scenarios/{{scenarioid}}/ executables/{{executableid}}

 3. On the Authorization tab, set the type to Bearer Token .

 4. Set the token value to {{token}} .

 5. On the Header tab, add the following entry:

<!-- missing-text -->

9.1.3.4 Deploy Models

Parent topic: Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

Serving Templates [page 268]

List Executables [page 280]

Inferencing [page 288]

Update a Deployment [page 290]

Stop Deployments [page 292]

Delete Deployments [page 296]

Efficiency Features [page 301]

Retrieve Deployment Logs [page 304]

Using Curl

Procedure

 1. Trigger the deployment.

```
curl --request POST $AI_API_URL/v2/lm/deployments \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP" --data-raw '{ "configurationId": " 2b72d740-5a89-4cf7-b37c-85973eeed6ae " }'
```



```
Output Code { "deploymentUrl": "", "id": "dda5d19065d5b1f4", "message": "Deployment created.", "status": "UNKNOWN" }
```

 2. Note the deploy environment variable for later use.

 3. Check the status of the deployment.

```
curl --request GET $AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP"
```

 Note

Configurations are checked when POST requests are submitted. If a configuration is incorrect configuration, an error message outlining the details will be returned..

If the status of the deployment is dead or pending after successful submission, there might be errors in the deployment. You can check the deployment logs for more details, see Retrieve Deployment Logs [page 304].

```
 Output Code { "configurationExecutableId": "hello-tf-1-15", "configurationId": "2b72d740-5a89-4cf7-b37c-85973eeed6ae", "configurationName": "hello-tf-1-15-config", "createdAt": "2020-11-11T06:34:24Z", "createdBy": "user", "deploymentUrl": "https://my-deployment-url.com", "id": "d291766fd1072b3f", "modifiedAt": "2020-11-11T06:37:29Z", "modifiedBy": "user", "scenarioId": "dba85cf3-2d69-498d-8ce5-4a415c9116dc", "status": "RUNNING", "targetStatus": "RUNNING", "versionId": "0.1.0" }
```

Using Postman

Procedure

 1. Send a POST request to the endpoint {{apiurl}}/v2/lm/deployments .

 2. Pass the configurationId in the request body.

```
{ deploymentUrl": "", " "id": "d94771b082c5cbcc", "status": "UNKNOWN"
```

```
"message": "Deployment scheduled.", }
```

 3. Check the status of the deployment by sending a GET request to {{apiurl}}/v2/lm/deployments/

 {{deploymentid}} .

```
"scenarioId": "b5379278-887c-4156-9a65-a6c11d6f1a71",
```

```
{ deploymentUrl": "your deployment url", " "modifiedAt": "2021-09-27T10:48:51Z", "startTime": "2021-09-27T10:33:12Z", "status": "RUNNING" }
```

 Note

Configurations are checked when POST requests are submitted. If a configuration is incorrect configuration, an error message outlining the details will be returned..

If the status of the deployment is dead or pending after successful submission, there might be errors in the deployment. You can check the deployment logs for more details, see Retrieve Deployment Logs [page 304].

Parameters and Quotas

Optional Parameters

The duration of a deployment can be limited using the ttl parameter. It takes an integer for quantity, and a single letter to specify units of time. Only minutes ( m ), hours ( h ) and days ( d ), are supported, and values must be natural numbers. For example, "ttl": "5h" gives the deployment a duration of 5 hours. 4.5h and 4h30m are not valid inputs. If no value is passed, the duration of the deployment if indefinite. Once the duration expires, the deployment is stopped and deleted.

Deployment Quotas

Each tenant is assigned a default quota that limits the number of deployments and replicas per deployment. If you reach this quota, your deployment will not be created, and you will be notified. You can free up your quota by deleting existing deployments.

Alternatively, you can request a quota increase by creating a ticket. The component name is CA-ML-AIC and ticket title is Request to Increase Quota .

9.1.3.5 Inferencing

Use the URL from your model deployment to access the results of your model.

To inference your model, send a POST request to "$DEPLOYMENT_URL/&lt;path that you have defined&gt;" For example, TensorFlow models follow the path v1/models/$MODEL_NAME:predict .

Inference with Postman

As the deployment URL, pass the URL that was returned in the response body in the {{apiurl}}/v2/lm/ deployments call (see Deploy Models [page 285]).

As the request body, enter a sample instance in JSON format.

<!-- missing-text -->

Inference with curl

In this example, the name of the model is 'churn'.

curl --location --request POST ' $deploymentUrl/v1/models/churn:predict' \

Parent topic: Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

Serving Templates [page 268]

List Executables [page 280]

Deploy Models [page 285]

Update a Deployment [page 290]

Stop Deployments [page 292]

Delete Deployments [page 296]

Efficiency Features [page 301]

Retrieve Deployment Logs [page 304]

9.1.3.6 Update a Deployment

You can update a deployment with a new configuration while retaining the inference URL.

During the transition to the new deployment configuration, the inference requests will continue to work.

When the new configuration is deployed, the deployment may end in a 'dead' status, or get stuck in the 'pending' status, and never get to a 'running' state, due to incorrect configuration. In this case, inference requests will no longer work. The last known running configuration ID is documented in field latestRunningConfigurationId and can be used in another PATCH request to return to the last running configuration.

If the updated deployment reaches the 'running' state, latestRunningConfigurationId will be updated to the new configuration.

Prerequisites

 · The deployment must have the state 'pending', 'running' or 'dead'.

 · The new configuration contains the same scenarioId and executableId as the currently active configuration.

 Note

Dead deployments can only be patched within 7 days. After 7 days from the time that the deployment reached DEAD status, it will be deleted and will no longer be available.

Using Postman

 1. Update the deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} .

 2. Pass the new configurationId in the request body.

<!-- missing-text -->

 3. Check the status of the deployment by submitting a GET request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} .

<!-- missing-text -->

Using curl

 1. Update the deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} .

 2. Pass the new configurationId in the request body.

```
curl --request PATCH $AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID \ -header 'Authorization: Bearer $TOKEN' \ ---header 'ai-resource-group: $RESOURCE_GROUP' \ --header 'Content-Type: application/json' \ --data-raw '{ "configurationId": "490a02b0-4b97-48e8-b905-1c6ae5ea5b1c" }'
```

```
 Output Code { "id": "d748fdae9f88a9b0", "message": "Deployment modification scheduled" }
```

 3. Check the status of the deployment by submitting a GET request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} .

```
curl --request GET $AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP"
```

```
 Output Code { "configurationId": "490a02b0-4b97-48e8-b905-1c6ae5ea5b1c", "configurationName": "mnist-serving", "createdAt": "2021-12-01T10:20:28Z", "deploymentUrl": " "https://my-deployment-url.com"", "id": "d748fdae9f88a9b0", "lastOperation": "UPDATE", "latestRunningConfigurationId": "490a02b0-4b97-48e8-b905-1c6ae5ea5b1c", "modifiedAt": "2021-12-01T11:19:21Z", "scenarioId": "platform-test-mnist-tensorflow", "startTime": "2021-12-01T10:47:23Z", "status": "PENDING", "statusDetails": {}, "submissionTime": "2021-12-01T11:03:55Z", "targetStatus": "RUNNING" }
```

Parent topic: Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

```
Serving Templates [page 268] List Executables [page 280] Deploy Models [page 285] Inferencing [page 288] Stop Deployments [page 292] Delete Deployments [page 296] Efficiency Features [page 301] Retrieve Deployment Logs [page 304]
```

9.1.3.7 Stop Deployments

Stopping a deployment releases the SAP AI Core runtime computing resources that it used. A stopped deployment does not incur costs.

 Note

Stop is only enabled if the status is running or pending.

Stop a Single Deployment [page 293]

Stop Multiple Deployments [page 294]

Parent topic:

Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

Serving Templates [page 268]

List Executables [page 280]

Deploy Models [page 285]

Inferencing [page 288]

Update a Deployment [page 290]

Delete Deployments [page 296]

Efficiency Features [page 301]

Retrieve Deployment Logs [page 304]

9.1.3.7.1 Stop a Single Deployment

Using Postman

Stop the deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} . The header for this request is: AI-Resource-Group: {YOUR-Resource-Group} . The Body for this request is

<!-- missing-text -->

Using curl

 1. Update the deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} . 2. Update the request body to: curl --request PATCH $AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID \ -header "authorization: Bearer $TOKEN" \ ---header "ai-resource-group: $RESOURCE_GROUP" \ --header 'content-Type: application/json' \ --data-raw '{ "targetStatus": "STOPPED" }'  Output Code { "id": "d748fdae9f88a9b0", "message": "Deployment modification scheduled" } 3. Check the status of the deployment by submitting a GET request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} . curl --request GET $AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP"

Parent topic: Stop Deployments [page 292]

Related Information

Stop Multiple Deployments [page 294]

9.1.3.7.2 Stop Multiple Deployments

bulkUpdates is a meta capability endpoint of the AI API. It enables or disables bulk PATCH operations. For more information, see AI API Overview [page 36].

The feature is set to false by default. To enable bulk PATCH operations, your template must contain the following snippet, with the relevant values set to true .

```
eta: M "bulkUpdates": { "executions": false, "deployments": false }
```

About bulkUpdates :

 · The maximum number of updates per request is 100.

 · Your bulk update can contain a mixture of STOP and DELETE requests.

 · Only running or pending executions or deployments can be stopped.

 · Only stopped dead , or unknown executions or deployments can be deleted.

 · An ID can only appear once per bulk request. For multiple modifications of the same ID, multiple requests are needed.

Using curl

Update the request body to:

```
curl --request PATCH  - /deployments \ --header {"deployments": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED" } ] }
```

```
 Output Code { "deployments": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Deployment modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Deployment modification scheduled" } ] }
```

Using Postman

Send a bulk PATCH request to: - /deployments

```
{ deployments": [ " { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED"
```

```
} ] } }'  Output Code { "deployments": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Deployment modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Deployment modification scheduled" } ] }
```

Parent topic: Stop Deployments [page 292]

Related Information

Stop a Single Deployment [page 293] AI API Overview [page 36]

9.1.3.8 Delete Deployments

Deleting a deployment releases the SAP AI Core resources that it used.

<!-- missing-text -->

 Restriction

If your deployment is running, you must stop it first. You can stop a deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/{{deploymentid}} . For more information, see Stop Deployments [page 292]

Delete a Single Deployment [page 297]

Delete Multiple Deployments [page 298]

Parent topic: Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

Serving Templates [page 268]

```
List Executables [page 280] Deploy Models [page 285] Inferencing [page 288] Update a Deployment [page 290] Stop Deployments [page 292] Efficiency Features [page 301] Retrieve Deployment Logs [page 304]
```

9.1.3.8.1 Delete a Single Deployment

Deleting a deployment releases the SAP AI Core resources that it used.

<!-- missing-text -->

Restriction

If your deployment is running, you must stop it first. You can stop a deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/{{deploymentid}} . For more information, see Stop a Single Deployment [page 293]

Parent topic: Delete Deployments [page 296]

Related Information

Delete Multiple Deployments [page 298]

Using Curl

Procedure

 1. Update the deployment by submitting a DELETE request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} .

 2. Update the request body to:

```
curl --request DELETE $AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID \ -header "authorization: Bearer $TOKEN" \ ---header "ai-resource-group: $RESOURCE_GROUP" \ --header 'content-Type: application/json' \ --data-raw '{ "targetStatus": "DELETED" }'
```

```
 Output Code { "id": "d748fdae9f88a9b0", "message": "Deployment modification scheduled" }
```

 3. Check the status of the deployment by submitting a GET request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} .

```
curl --request GET $AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID \ --header "Authorization: Bearer $TOKEN" \ --header "ai-resource-group: $RESOURCE_GROUP"
```

Using Postman

Procedure

Delete the deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} . The header for this request is: AI-Resource-Group: {YOUR-Resource-Group} . The Body for this request is:

```
{ targetStatus": "DELETED" " }
```

Next Steps

Check the status of the deployment by submitting a GET request to {{apiurl}}/v2/lm/deployments/ {{deploymentid}} .

9.1.3.8.2 Delete Multiple Deployments

Deleting a deployment releases the SAP AI Core resources that it used.

<!-- missing-text -->

 Restriction

If your deployment is running, you must stop it first. You can stop a deployment by submitting a PATCH request to {{apiurl}}/v2/lm/deployments/{{deploymentid}} . For more information, see Stop Multiple Deployments [page 294].

bulkUpdates is a meta capability endpoint of the AI API. It enables or disables bulk PATCH operations. For more information, see AI API Overview [page 36].

The feature is set to false by default. To enable bulk PATCH operations, your template must contain the following snippet, with the relevant values set to true .

```
eta: M "bulkUpdates": { "executions": false, "deployments": false }
```

About bulkUpdates :

 · The maximum number of updates per request is 100.

 · Your bulk update can contain a mixture of STOP and DELETE requests.

 · Only running or pending executions or deployments can be stopped.

 · Only stopped dead , or unknown executions or deployments can be deleted.

 · An ID can only appear once per bulk request. For multiple modifications of the same ID, multiple requests are needed.

Parent topic: Delete Deployments [page 296]

Related Information

```
Delete a Single Deployment [page 297] AI API Overview [page 36]
```

Using Curl

Procedure

Send a bulk PATCH request to the endpoint: - /deployments

Update the request body to:

```
{ "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED" } ] }  Output Code {
```

```
"executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Execution modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Execution modification scheduled" } ] }
```

Using Postman

Procedure

Send a bulk PATCH request to the endpoint: - /deployments

Update the request body to:

```
{ "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "targetStatus": "STOPPED" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "targetStatus": "DELETED" } ] }
```

```
 Output Code { "executions": [ { "id": "aa97b177-9383-4934-8543-0f91a7a0283a", "message": "Execution modification scheduled" }, { "id": "qweq32131-qwee-1231-8543-0f91a7a2e2e", "message": "Execution modification scheduled" } ] }
```

9.1.3.9 Efficiency Features

Discover features of the SAP AI Core runtime that improve efficiency and help manage resource consumption.

Autoscaling

SAP AI Core includes parameters to reduce the number of nodes used based on current consumption, or impose usage limits during periods of high consumption. These parameters allow your workload the flexibility to scale based on demand, and for consumption to be capped, limiting your consumption and therefore costs. For more information, see Serving Templates.

Scaling to 0

Where non-uniform loads are expected, scaling to 0 allows nodes to enter a sleeping state when demand allows, limiting your consumption, and therefore costs. Nodes wake when demand increases, which has an increased response time. The Global Node pool reduces, this cold start time. For more information, see Serving Templates.

Global Node Pool

When the inference server scales up from a sleeping state, there is some additional response time. To reduce this, SAP AI Core has a Global Node Pool, which keeps commonly used nodes reserved to allow for shorter response times. You do not need to do anything to make use of the Global Node Pool, it is already in place. To reduce response times further, avoid a cold start altogether by setting your autoscaling parameter to 1 . For more information, see Serving Templates.

Scaling to 1

Cold starts can be avoided completely by scaling to 1 . This keeps a single node warm, even when it is not needed, reducing response time. However, it does not offer the consumption and cost savings associated with scaling to 0. For more information, see Serving Templates.

Duration

The default duration is indefinite, however the ttl parameter limits the duration of a deployment to minutes, hours, or days. This parameter allows you to plan the deletion if your model servers and model deployment

URL, allowing for an expected period of use and avoiding unnecessary consumption and costs afterwards. For more information, see Deploy Models and About the AI API.

Tenant Warm Node Pool

Tenant warm nodes allow tenants to reserve nodes from specific resource plan types, ensuring that a predefined number of nodes exist in the cluster. The reserved nodes can then be used during model training and serving. Reserving nodes reduces waiting time at workload startup, but incurs costs in line with node use, whether consumed or not.

Mechanism of Node Reservation

 1. The tenant specifies number of nodes to reserve.

 2. An execution or deployment utilizes a reserved node, a replacement reserve node of the same resource plan type is requested from the hyperscaler.

The number of reserved nodes specified by the tenant are consistently available for use.

The minimum number of reserved nodes is 0.

About Node Reservation

 · The number of reserved nodes specified by the tenant are consistently available for use.

 · The minimum number of reserved nodes is 0.

 · The maximum number of reserved nodes is 10.

 · The default number of reserved nodes is 0.

Reserve Nodes Using Postman

 1. Send a PATCH request to the endpoint {{apiurl}}/v2/admin/resources/nodes

 2. Provide the resource plan type and quantity of nodes to reserve in the request body in JSON format:

```
{ "resourcePlans": [ { "name": "infer.l", "request": 1 }, { "name": "infer.m", "request": 1 }, { "name": "train.l", "request": 1 } ... ] }
```

Reserve Nodes Using curl

Submit a PATCH request to the endpoint {{apiurl}}/v2/admin/resources/nodes

```
curl --request PATCH $AI_API_URL/v2/admin/resources/nodes \
```

```
--data-raw '{ "resourcePlans": [ { "name": "infer.l", "request": 1 }, { "name": "infer.m", "request": 1 }, { "name": "train.l", "request": 1 } ] }'
```

 Remember

All reserved nodes are charged at the same rate as nodes used during model training and serving.

Check Reserve Node Status Using Postman

Send a GET request to the endpoint {{apiurl}}/v2/admin/resources/nodes

Check Reserve Node Status Using curl

```
curl --request GET $AI_API_URL/v2/resources/nodes
```

```
 Output Code { "resourcePlans": { "infer.l": { "provisioned": 1, "requested": 1 }, "infer.m": { "provisioned": 1, "requested": 1 }, "train.l": { "provisioned": 1, "requested": 1 } } }
```

 · requested : the number of reserve nodes requested by the tenant.

 · provisioned : the number of reserve nodes that are currently present in the cluster.

Update Quantities of Reserved Nodes

To update the number of nodes reserved, repeat the reservation procedure, with updated quantities in the request field.

Delete Reserved Nodes

To delete reserved nodes, repeat the reservation procedure, with quantities in the request field set to 0 .

Parent topic: Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

Serving Templates [page 268]

List Executables [page 280]

Deploy Models [page 285]

Inferencing [page 288]

Update a Deployment [page 290]

Stop Deployments [page 292]

Delete Deployments [page 296]

Retrieve Deployment Logs [page 304]

Service Plans [page 43]

9.1.3.10  Retrieve Deployment Logs

accessed in the deployment and execution logs.

You can retrieve the logs for a specific deployment or execution by submitting a GET request. Use the following endpoints to retrieve the logs:

 · GET /v2/lm/deployments/{deploymentId}/logs

 · GET /v2/lm/executions/{executionId}/logs

Query parameters include:

 · start : the start time for a query as a string, in RFC 3339-compliant datetime format. Defaults to 1 hour before the current. Example: 2021-05-19T00:00:14.347Z

 · end : the end time for a query as a string, in RFC 3339-compliant datetime format. Defaults to the current time. Example: 2021-05-19T00:00:14.347Z

 · $top : the maximum number of entries returned. The default value is 1000; the upper limit is 5000.

 · $order : the sort order of logs, either asc (for ascending, earliest in the order will appear at the top of the list) or desc (for descending, most recent in the order will appear at the top of the list). Note the default value is asc .

For example:

 · /v2/lm/deployments/{deploymentId}/logs? start=2021-05-19T00:00:14.347Z&amp;end=2021-05-19T01:00:14.347Z&amp;$top=100&amp;$order=asc - returns the first

 100 lines of a deployment log between 2021-05-19T00:00:14.347Z and 2021-05-19T01:00:14.347Z

 · /v2/lm/deployments/{deploymentId}/logs - returns deployment logs from the preceding hour

 · /v2/lm/executions/{executionId}/logs - returns execution logs from the preceding hour

Using Postman

 1. Create a new GET request and enter the URL {{apiurl}}/v2/lm/deployments/{{deploymentid}}/ logs .

<!-- missing-text -->

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

 4. On the Header tab, add the following entry:

<!-- missing-text -->

 5. Send the request.

Using curl

```
curl --request GET "$AI_API_URL/v2/lm/deployments/$DEPLOYMENT_ID/logs? start=2021-05-19T00:00:14.347Z" --header "Authorization: Bearer $TOKEN" --header "ai-resource-group: $RESOURCE_GROUP"
```

Sample Output

For example, see the following JSON output from the API.



```
Output Code { "data": { "result": [ { "container": "storage-initializer", "msg": "[I 210531 08:20:51 initializer-entrypoint:13] Initializing, args: src_uri [gs://kserve-samples/models/tensorflow/flowers] dest_path[ [/mnt/models]\n", "pod": "tfs-dep-i543026-predictor-default-v6nf5deployment-8b58c8ddcfdx", "stream": "stderr", "timestamp": "2021-05-31T08:20:51.334+00:00" }, { "container": "storage-initializer", "msg": "[I 210531 08:20:51 storage:45] Copying contents of gs://kserve-samples/models/tensorflow/flowers to local\n", "pod": "tfs-dep-i543026-predictor-default-v6nf5deployment-8b58c8ddcfdx", "stream": "stderr", "timestamp": "2021-05-31T08:20:51.335+00:00" }, { "container": "storage-initializer", "msg": "[W 210531 08:20:51 _metadata:104] Compute Engine Metadata server unavailable onattempt 1 of 3. Reason: [Errno 111] Connection refused\n", "pod": "tfs-dep-i543026-predictor-default-v6nf5deployment-8b58c8ddcfdx", "stream": "stderr", "timestamp": "2021-05-31T08:20:51.338+00:00" }, ... ] } }
```

Parent topic: Use Your Model [page 265]

Related Information

Choose a Resource Plan [page 266]

Serving Templates [page 268]

List Executables [page 280]

Deploy Models [page 285]

Inferencing [page 288]

Update a Deployment [page 290]

Stop Deployments [page 292]

Delete Deployments [page 296]

Efficiency Features [page 301]

Retrieve Execution Logs [page 255]

9.2 Metrics

The AI API provides the ability to track metrics, and to customize or filter which metrics are reported.

Parent topic: Predictive AI [page 207]

Related Information

ML Operations [page 207]

Advanced Features [page 323]

9.2.1  Metrics Tracking with AI API

You can use AI API to track and fetch metrics for executions and models. In addition, metrics can be compared using the SAP AI Launchpad interface.

Usage Authorization

The following role collections are required to use the AI API to track metrics.

<!-- missing-text -->

9.2.1.1 Get Metrics

Getting Metrics with Postman

Procedure

 1. Prepare a GET request to the endpoint {{apiurl}}/v2/lm/metrics with the following query parameters and headers:

Query Parameters

<!-- missing-text -->

<!-- missing-text -->

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

 4. Set the query parameters in Params .

<!-- missing-text -->

 5. Set the resource group Header .

<!-- missing-text -->

 6. Send the request.

Results

You will receive the following response:

<!-- missing-text -->



```
Output Code { "count": 2, "resources": [ { "customInfo": [-], "executionID": "e3de6dbbca8621b5", "metrics": [-] }, { "customInfo": [-], "executionID": "edbbb3c7a4ba64b9", "metrics": [-] } ] }
```

 Note

It is possible to use more than one metric in the $select parameter by separating valid entries with a comma.

Key

Value

<!-- missing-text -->

Getting Metrics with curl

The following demonstrates how you can manually track metrics information, and patch and delete metrics.

Procedure

url --location -g --request GET '$AI_API_URL/v2/lm/metrics? c $select=metrics,tags,customInfo&amp;executionIds=e3de6dbbca8621c5,ed4bb3c7a4ba64b9' -header 'AI-Resource-Group: default' \ --header 'Authorization: Bearer $TOKEN' - Note It is possible to use any one of the allowed values or combination of the allowed values (metrics, tags, customInfo) or * in the $select parameter.

Results

```
 Output Code { "count": 2, "resources": [ { "customInfo": [ { "name": "confusion matrix", "value": "[{'Predicted': 'False', 'Actual': 'False','value': 34},{'Predicted': 'False','Actual': 'True', 'value': 124}, {'Predicted': 'True','Actual': 'False','value': 165},{ 'Predicted': 'True','Actual': 'True','value': 36}]" } ], "executionId": "ec83b8e837fe4a56", "metrics": [ { "labels": [ { "name": "m11", "value": "alpha" }, { "name": "metrics.ai.sap.com/Artifact.name", "value": "text-model-tutorial" } ], "name": "Error Rate", "step": 2, "timestamp": "2020-09-29T11:40:10.330000Z", "value": 0.98 } ], "tags": [
```

```
{ "name": "text-model-tutorial", "value": "beta-3" } ] }, { "customInfo": [ { "name": "confusion matrix", "value": " precision recall f1-score support\n\n 0 0.85 0.97 0.91 273\n 1 0.96 0.79 0.87 227\n\n accuracy 0.89 500\n macro avg 0.90 0.88 0.89 500\nweighted avg 0.90 0.89 0.89 500\n" } ], "executionId": "ea4af58c56c26384", "metrics": [ { "labels": [ { "name": "train", "value": "range 0-400" } ], "name": "Accuracy", "step": 1, "timestamp": "2021-12-03T06:23:57.945336Z", "value": 0.9675 }, { "labels": [ { "name": "train", "value": "range 400-800" } ], "name": "Accuracy", "step": 2, "timestamp": "2021-12-03T06:23:58.089152Z", "value": 0.94 }, { "labels": [ { "name": "train", "value": "range 800-1200" } ], "name": "Accuracy", "step": 3, "timestamp": "2021-12-03T06:23:58.195216Z", "value": 0.9625 }, { "labels": [ { "name": "train", "value": "range 1200-1600" } ], "name": "Accuracy", "step": 4, "timestamp": "2021-12-03T06:23:58.306945Z", "value": 0.9425 }, { "labels": [ {
```

```
"name": "train", "value": "range 1600-2000" } ], "name": "Accuracy", "step": 5, "timestamp": "2021-12-03T06:23:58.421745Z", "value": 0.945 }, { "labels": [ { "name": "metrics.ai.sap.com/Artifact.name", "value": "text-model-tutorial" } ], "name": "Error Rate", "step": 0, "timestamp": "2021-12-03T06:23:58.569476Z", "value": 0.10999999999999999 }, { "labels": [], "name": "n_compliments", "step": 0, "timestamp": "2021-12-03T06:23:57.576040Z", "value": 1173.0 }, { "labels": [], "name": "n_complaints", "step": 0, "timestamp": "2021-12-03T06:23:57.575618Z", "value": 1327.0 }, { "labels": [], "name": "n_samples", "step": 0, "timestamp": "2021-12-03T06:23:57.574852Z", "value": 2500.0 }, { "labels": [ { "name": "train", "value": "80%" }, { "name": "step", "value": "preprocessing" } ], "name": "split_samples", "step": 0, "timestamp": "2021-12-03T06:23:57.757594Z", "value": 2000.0 }, { "labels": [ { "name": "validation", "value": "20%" }, { "name": "step", "value": "preprocessing" }
```

```
], "name": "split_samples", "step": 0, "timestamp": "2021-12-03T06:23:57.757608Z", "value": 500.0 } ], "tags": [ { "name": "Reading Model", "value": "OK" }, { "name": "Test Inference", "value": "checked" } ] } ] }
```

9.2.1.2 Patch Metrics

Patching Metrics Using Postman

Procedure

 1. Prepare a PATCH request to the endpoint {{apiurl}}/v2/lm/metrics with the following headers:

Header

<!-- missing-text -->

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

 4. Set the Header .

<!-- missing-text -->

 5. Add the Body .

```
 Sample Code { "executionId": "{{executionid}}", "metrics": [ { "name": "Error Rate", "value": 0.98, "timestamp": "2021-06-28T07:50:24.589Z", "step": 2, "labels": [ { "name": "group", "value": "tree-82" } ] } ], "tags": [ { "name": "Artifact Group", "value": "RFC-1" } ], "customInfo": [ { "name": "Confusion Matrix", "value": "[{'Predicted': 'False', 'Actual': 'False','value': 34}, {'Predicted': 'False','Actual': 'True', 'value': 124}, {'Predicted': 'True','Actual': 'False','value': 165},{ 'Predicted': 'True','Actual': 'True','value': 36}]" } ] }
```

6. Send the request.

Results

As the response, you will receive a 204 (no content) response.

Patching Metrics Using curl

Procedure

Run the following code:

```
url --location --request GET '$AI_API_URL/v2/lm/metrics? c executionId=e1c49497ccf6dde8' \ -header 'AI-Resource-Group: default' \ ---header 'Authorization: Bearer $TOKEN' \ --data-raw '{ "executionId": "e1c49497ccf6dde8", "metrics": [
```

```
{ "name": "Error Rate", "value": 0.98, "timestamp": "2021-06-28T07:50:24.589Z", "step": 2, "labels": [ { "name": "group", "value": "tree-82" } ] } ], "tags": [ { "name": "Artifact Group", "value": "RFC-1" } ], "customInfo": [ { "name": "Confusion Matrix", "value": "[{'\''Predicted'\'': '\''False'\'', '\''Actual'\'': '\''False'\'','\''value'\'': 34},{'\''Predicted'\'': '\''False'\'','\''Actual'\'': '\''True'\'', '\''value'\'': 124}, {'\''Predicted'\'': '\''True'\'','\''Actual'\'': '\''False'\'','\''value'\'': 165},{ '\''Predicted'\'': '\''True'\'','\''Actual'\'': '\''True'\'','\''value'\'': 36}]" } ] ' }
```

9.2.1.3 Query Metric Data

You can query metrics by submitting a GET request to the endpoint /v2/lm/metrics . T o fetch the tracking data, you can use the following parameters:

 · AI-Resource-Group (header) - string - UUID

 · executionIds (query) - string - Retrieve metrics based on up to 10 execution IDs (comma-separated list)

 · $select (query) - Selectively retrieve metric resource data, such as metrics and custom info. For example, if the parameter value is a wildcard ( * ), then all metric resource data is returned. If the parameter value is custom info , then only custom info data is returned. Values should be entered as an array. Only the values metrics tags customInfo , , and * are supported.

Examples

Responses of GET API

In this example, the $select parameter value is a wildcard (*), so all metric resource data is returned.

200

Description

List of tracking metadata, where each item includes metrics, labels, parameters, and tags.

```
 Sample Code { "count": 1, "resources": [ { "executionId": "aa97b177-9383-4934-8543-0f91a7a0283a", "metrics": [ { "name": "Error Rate", "value": 0.98, "timestamp": "2020-11-12T12:36:13.730Z", "step": 2, "labels": [ { "name": "group", "value": "tree-82" } ] } ], "tags": [ { "name": "Artifact Group", "value": "RFC-1" } ], "customInfo": [ { "name": "Confusion Matrix", "value": "[{'Predicted': 'False',  'Actual': 'False','value': 34},{'Predicted': 'False','Actual': 'True',  'value': 124}, {'Predicted': 'True','Actual': 'False','value': 165},{  'Predicted': 'True','Actual': 'True','value': 36}]" } ] } ] }
```

Response Code

Description

400

The specification of the resource was incorrect.

```
 Sample Code { "error": { "code": "<error code>", "message": "<error message>.", "requestId": "9832bf934f3743v3948v3", "target": "/metrics", "details": [ { "code": "<error code>", "message": "<error message>." }
```

```
] } }
```

Response Code

Description

501

The operation is not supported.

```
 Sample Code { "error": { "code": "02010055", "message": "Metrics was not found.", "requestId": "9832bf934f3743v3948v3", "target": "/metrics", "details": [ { "code": "9827389374", "message": "Empty result set." } ] } }
```

9.2.1.4 Store Metric Data

You can store metrics by submitting a PATCH request to the endpoint /v2/lm/metrics . T o store tracking data, you must provide the parameter AI-Resource-Group header string.

The executionId metrics labels tags , , , and customInfo together form a MetricResource that is used as the request body for the PATCH API to persist tracking data.

The following limits apply to each of the attributes in the MetricResource:

 · metrics - maximum 1000

 · labels - maximum 20, with a maximum character limit of 256 for each label

 · tags - maximum 100, with a maximum character limit of 256 for each tag

 · customInfo - maximum 100, with a maximum size of 5 MB for the entire customInfo object

 Recommendation

Do not use the tracking functions provided by SAP AI Core to track sensitive information. For more information, see Security [page 335].

executionId

A unique identifier for an execution.

metrics

A metric is a key/value pair where the value is numeric. Metrics are used to measure and monitor the performance, progress, and quality of a model during the training process. Common examples of metrics include accuracy, precision, recall, F1 score, and mean squared error. Metrics can have optional step, timestamp, and label fields, which provide additional information about the metric's context. Every metric (and associated labels), tag, and custom info must be associated with a specific training execution. This association allows for proper organization, tracking, and comparison of different training runs. Once the metric, tag, and custom info are saved, they can be queried using an execution ID.

<!-- missing-text -->

labels

A label is a key-value pair attached to a metric to provide additional context and metadata. Labels are used to classify and categorize metrics, enabling users to filter, group, and aggregate data for better insights and analysis. A set of labels can be applied to each instance of a metric record, allowing for more granular and targeted monitoring.

Label Attributes

<!-- missing-text -->

<!-- missing-text -->

For a metric to be associated with a model, you must explicitly associate it with a model artifact:

<!-- missing-text -->

When an execution produces only one output model artifact, all metrics captured in the execution are associated with that output artifact. If an execution produces more than one output model artifact, each metric captured must be explicitly associated with a model artifact (as shown in the example above).

tags

A tag is a name/value pair that is used to categorize and organize test executions. Tags allow for the segregation and grouping of test executions based on specific criteria, making it easier to manage, analyze, and report on the results. For example, you can assign tags to a group of selected test executions to indicate their purpose, priority, or other relevant characteristics. A set of tags can be associated with a MetricResource , which is an entity that represents a specific metric or measurement. The MetricResource , in turn, is linked to an execution, establishing a connection between the tags and the corresponding test run. This relationship enables the effective tracking, monitoring, and evaluation of test executions based on their assigned tags.

<!-- missing-text -->

customInfo

Custom info key/value pairs that enable the capture of large amounts of metadata, typically associated with an execution. Custom info provides rendering or semantic information regarding a metric for a consuming application or complex metrics in JSON format.

A set of such custom info objects can be associated with a MetricResource .

CustomInfo Attributes

<!-- missing-text -->

Examples

Example Request Body for PATCH API

```
{ "executionId": "aa97b177-9383-4934-8543-0f91a7a0283a", "metrics": [ { "name": "Error Rate", "value": 0.98, "timestamp": "2020-11-12T12:18:01.539Z", "step": 2, "labels": [ { "name": "group", "value": "tree-82" }, { 'name': 'metrics.ai.sap.com/Artifact.name', 'value': 'my_model_name' } ] } ], "tags": [ { "name": "Artifact Group", "value": "RFC-1" } ], "customInfo": [ { "name": "Confusion Matrix", "value": "[{'Predicted': 'False', 'Actual': 'False', 'value': 34}, {'Predicted': 'False', 'Actual': 'True', 'value': 124}, {'Predicted': 'True', 'Actual': 'False', 'value': 165}, {'Predicted': 'True', 'Actual': 'True', 'value': 36}]" } ] }
```

Responses of PATCH API

<!-- missing-text -->

```
 Sample Code { "error": { "code": "<error code>", "message": "<error message>.", "requestId": "9832bf934f3743v3948v3", "target": "/metrics", "details": [ { "code": "<error code>", "message": "<error message>" } ] } }
```

Response Code

Description

413

Request entity is larger than limits defined by server.

```
 Sample Code { "error": { "code": "02000005", "message": "PayloadLimitException", "requestId": "9832bf934f3743v3948v3", "target": "/metrics", "details": [ { "code": "02000005", "message": "PayloadLimitException" } ] } }
```

9.2.1.5 Delete Metrics

You can delete metrics by submitting a DELETE request to the endpoint /v2/lm/metrics . T o delete tracking data, you must provide the following parameters:

 · AI-Resource-Group (header) - string

 · executionId (query)

Query Parameters

<!-- missing-text -->

Responses of DELETE API

<!-- missing-text -->

Deleting Metrics with Postman

Procedure

 1. Send a DELETE request to the endpoint {{apiurl}}/v2/lm/metrics

 2. On the Authorization tab, set the type to Bearer Token .

 3. Set the token value to {{token}} .

 4. Set the query parameters in Params .

<!-- missing-text -->

 6. Send the request.

Results

You should receive the message Metric Resource was successfully deleted .

Deleting Metrics with curl

Procedure

Send a DELETE request to the endpoint {{apiurl}}/v2/lm/metrics with the required query parameters and headers.

```
url --location --request DELETE '$AI_API_URL/v2/lm/metrics?
```

```
c executionId=e1c49497ccf6dde8' \ -header 'AI-Resource-Group: default' \ --header 'Authorization: Bearer $TOKEN' -
```

9.3 Advanced Features

Explore advanced features, within SAP AI Core.

AI Content as a Service [page 324]

SAP AI Core helps users provide their AI content available as a service on the Service Marketplace , using GitOps.

Parent topic: Predictive AI [page 207]

Related Information

ML Operations [page 207]

Metrics [page 307]

9.3.1  AI Content as a Service

SAP AI Core helps users provide their AI content available as a service on the Service Marketplace , using GitOps.

Consumers can create service instances using SAP AI Core to use available content, and create their own executions or deployments. If the person acting as the main tenant chooses to make this available externally through the Service Marketplace , they become a service provider. The onboarding process for service providers and their content is as follows:

 1. Create AI content such as a workflow, serving template, or Docker registry that is consumer ready.

 2. Create a generic secret for broker registration. For more information, see Create a Generic Secret [page 101].

 3. Provide a service custom resource YAML in a registered git repository.

 4. SAP AI Core creates a service broker for the content to be made available to the Service Marketplace . The service broker handles onboarding and offboarding for end consumers.

 5. Fetch the service broker information by calling the endpoint: {{apiurl}}/v2/admin/services .

 6. Register the service broker in the Service Marketplace and SAP Cloud Management service.

 7. The consumer creates service instance from the Service Marketplace .

 8. SAP AI Core creates a resource group for the consumer with &lt;resourceGroupId&gt; == serviceInstanceId .

 9. The consumer creates a service key and starts using the service.

 Note

The service provider main tenant is allowed to provision only 1 Service.

SAP AI Core deploys a generic service broker instance for the main tenant as follows:

<!-- missing-text -->

9.3.1.1 Service Custom Resource

The service provider main tenant needs to prepare the service custom resource. The custom resource contains service details, reference to broker credentials or secrets, and capabilities configured for service consumers.

An example service custom resource is provided in the following code block:

```
piVersion: ai.sap.com/v1alpha1 a kind: Service metadata: name: sample-service spec: brokerSecret: name: broker-credentials usernameKeyRef: username passwordKeyRef: password description: Service used for demos capabilities: basic: staticDeployments: true userDeployments: true createExecutions: true logs: executions: true deployments: true serviceCatalog: - extendCredentials: shared: serviceUrls: AI_API_URL: https://api.ai.internalprod.eucentral-1.aws.ml.hana.ondemand.com extendCatalog: name: sample-service id: sample-service-broker-id description: sample service bindable: true plans: - id: sample-service-standard description: Standard plan for sample service name: standard free: false metadata: supportedPlatforms: - cloudfoundry - kubernetes - sapbtp
```

This can be used as a guide, with values amended as required. To create YAML descriptors, use any text editor with a YAML plugin.

For details about parameters, refer to the following table:

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

Restriction

Updating of Service Custom Resource is not Supported

Once a service custom resource has synced successfully, updates of any parameters in templates will not take any effect.

9.3.1.2 Onboarding

To onboard a service, complete the following:

 1. Create a brokerSecret to be used as credentials when registering the service broker.

 1. Use a new POST request to URL {{apiurl}}/v2/admin/secrets

 2. Provide credentials (base64 encoded) in the request body in JSON format:

```
{ name": "broker-credentials", " "data": { "username": "bXktc2VjcmV0LWNyZWRlbnRpYWw=", "password": "bXktc2VjcmV0LW90aGVyLWNyZWRlbnRpYWw=" } }
```

 3. Specify the scope of the request in the header: AI-Tenant-Scope: true

 4. Send the request.

 2. Modify the brokerSecret specification section using these details:

```
piVersion: ai.sap.com/v1alpha1 a kind: Service metadata: name: sample-service spec: brokerSecret: name: broker-credentials usernameKeyRef: username passwordKeyRef: password description: Service used for demos capabilities: basic: staticDeployments: true userDeployments: true createExecutions: true logs: executions: true deployments: true serviceCatalog: - extendCredentials: shared: serviceUrls: AI_SVC_URL: https://api.ai.internalprod.eucentral-1.aws.ml.hana.ondemand.com extendCatalog: name: sample-service id: sample-service-broker-id description: sample service bindable: true plans: - id: sample-service-standard description: Standard plan for sample Service name: standard free: false metadata: supportedPlatforms: - cloudfoundry - kubernetes - sapbtp
```

 Note

The username and password are key names from the broker credentials in the previous step.

 3. Update the spec.serviceCatalog[].extendCredentials with the service URL you want to provide to the consumer, which will be part of the service key. Provide catalog details under spec.serviceCatalog[].extendCatalog.

 4. Push your service custom resource to your registered GitHub repository, and wait for the sync to be successful.

 5. Once it has synced, Check the service details by sending a GET request to URL {{apiurl}}/v2/admin/ services .

```
{ "count": 1 "resources": [ { "name": "sample-service", "description": "Service used for demos", "status": "PROVISIONED", "url": "https://aif-xyzabc.servicebroker.internalprod.eucentral.aws.ml.hana.ondemand.com" } ] }
```

Take note of the service broker URL.

 6. Register the service broker using smctl as subaccount-scoped.

 1. Test the registration of the service broker first as subaccount-scoped, before you register it globally. Subaccount-scoped means that your service is automatically visible in the catalog of environments where it's registered. You can follow steps described in Service Manager Guide to set up for smctl . Once smctl is installed, login as shown:

```
env variables SERVICE_BROKER_URL=https://aif-xyzabc.servicebroker.internalprod.eu-
```

```
# SERVICE_MANAGER_URL=<sm url e.g. https://servicemanager.cfapps.sap.hana.ondemand.com/> SVC_SUBACCOUNT_USER=<user-with-servicemanagerrole>SVC_SUBACCOUNT_PWD=<password + 2FA> central.aws.ml.hana.ondemand.com SVC_SUBACCOUNT_SUBDOMAIN=<subaccount e.g. subaccountxyz> SERVICE_BROKER_USER=<broker username provided in secret> SERVICE_BROKER_PWD=<broker password provided in secret> # smctl login smctl login -a $SERVICE_MANAGER_URL \ --param subdomain=$SVC_SUBACCOUNT_SUBDOMAIN \ -u=$SVC_SUBACCOUNT_USER \ -p=$SVC_SUBACCOUNT_PWD
```

 2. Register your service-broker by providing the broker-name, URL, and credentials.

```
register service-broker # smctl register-broker sample-service $SERVICE_BROKER_URL -b
```

```
$SERVICE_BROKER_USER:$SERVICE_BROKER_PWD # service-broker registration should complete successfully
```

With the service-broker registered successfully, the service is available in the Service Marketplace .

```
assuming you are logged in to Cloud Foundry, provided correct subaccount, get
```

```
# service plan information
```

```
cf marketplace -s sample-service
```

Consumers can now create a service instance and service-key. On creation of service instance, SAP AI Core will create a corresponding resource-group with id = instance id, and the service is now ready for use.

9.3.1.3 Offboarding

To prevent accidental deletion of the service, service providers must provide a deletion strategy as follows:

```
etadata: m name: sample-service annotations: ai.sap.com/serviceDeletionStrategy: "delete"
```

With the serviceDeletionStrategy annotation, service providers can delete service custom resources from the git repository and proceed for offboarding. For successful service offboarding, all the consumer service instances should be deleted.

10 APIs and API Extensions

Explore APIs and API extensions that you can use with SAP AI Core.

APIs

<!-- missing-text -->

11 Libraries and SDKs

Explore additional SDKs and Libraries, for use with SAP AI Core.

SDKs Available with SAP AI Core

<!-- missing-text -->

<!-- missing-text -->

<!-- missing-text -->

Related Information

Metrics Tracking with AI API [page 307]

12 Content Packages

Explore additional Content Packages for use with SAP AI Core.

Content Packages Available with SAP AI Core

<!-- missing-text -->

<!-- missing-text -->

13 Tutorials

All available tutorials for SAP AI Core.

Starter Tutorials

Quick Start for SAP AI Core

Quick Start for SAP AI Core with VS Code

Metrics

Provision your account, register your keys and set up tools to connect with and operate the SAP AI Core SDK. Learn the basics by using SAP AI Core from beginning to end in a simple use case. The necessary codes are all provided directly in the steps of the tutorials, with YAML files made available in GitHub additionally.

Provision your account, register your keys and set up tools to connect with and operate the SAP AI Core SDK. Learn the basics by using SAP AI Core from beginning to end in a simple use case. The necessary codes are all provided directly in the steps of the tutorials, with YAML files made available in GitHub additionally. This tutorial is the starter tutorial using VS Code.

Explore different ways of logging metrics and comparing models with SAP AI Core.

Advanced Tutorials

In addition to the basic tutorials, the following tutorials are available for you in SAP Developer Center:

Computer Vision Package for SAP AI Core

Pretrained Tensorflow Models

Generative AI Hub Tutorials

Generative AI Hub Starter Tutorial

Activate the generative AI hub and learn the basics of composing prompts.

Using foundation models with SAP AI Core

Learn how to conusme foundation models using SAP AI Core.

Discover the Computer Vision Package with SAP AI Core.

Use Pre-Trained Tensorflow Models with a GPU in SAP AI Core.

14 Security

Here, we'll explain some of the security aspects of SAP AI Core.

14.1 Security Features of Data, Data Flow, and Processes

The table below shows an overview of the data flow for SAP AI Core.

<!-- missing-text -->

14.2 Encryption in Transit

Communication with the service, including data upload and download, is encrypted using the transport layer security (TLS) protocol. SAP services support only the latest protocol versions (that is, TLS v1.2 and later) and strong cipher suites. Your systems must use the supported protocol versions and cipher suites to set up secure communication with the services. They must also validate the certificates against the services' domain names to avoid man-in-the-middle attacks.

14.3 Authentication and Administration

SAP AI Core uses the authentication mechanisms provided by SAP Authorization and Trust Management service (XSUAA). The credentials to authenticate against the XSUAA to access SAP AI Core are provided as part of the service key for the SAP AI Core service.

For information about SAP Authorization and Trust Management service (XSUAA) in SAP BTP, see What Is the SAP Authorization and Trust Management Service?

14.4 Docker Images

SAP AI Core supports tenant-specific Docker registries (registered via the administration APIs). Additional tenant workloads, such as for execution and deployments, can be created by referencing the Docker images from this Docker registry.

Docker images are cached on virtual machines. These cached Docker images cannot be accessed by other tenants and will not be accessed by SAP.

Cached Docker images are not deleted immediately upon tenant offboarding but are cleaned up as part of operational events such as cluster scaling-down behavior, maintenance, and upgrade of virtual machines.

With every service that you consume, there is a shared security responsibility between you and SAP. Because the creation of a Docker image is the responsibility of the tenant, we strongly recommend that you do not embed or hard-code personal data, sensitive data, or machine learning models inside your Docker images.

For security reasons, Docker containers in SAP AI Core are run as non-root only. For more information, see Workflow Templates [page 221].

14.5 AI Content Security

AI content covers workflow templates and serving templates, as well as the Docker images used in them. Docker images contain the machine-learning algorithms or code, along with the machine-learning libraries, frameworks, and other dependent packages. Ensure that you follow standard practices for developing secure software when working with AI content.

<!-- missing-text -->

<!-- missing-text -->

14.6 Kubernetes Security

We recommend that you enable the relevant and applicable Kubernetes security features on your workflow templates and serving templates. Ensure that you enable the appropriate Kubernetes features for your workloads.

Related Information

Security Best Practices for Kubernetes Deployment

14.7 Configuration Data and Secrets

Workloads can access network resources other than object stores by using credentials at runtime. The ways of including this information at runtime have different standards when it comes to confidentiality.

 · For sensitive information: SAP AI Core allows you to include secrets in the form of generic secrets. Generic secrets are created and

managed by the REST APIs in SAP AI Core and consumed securely in a workload.

 · For non-sensitive information:

You can include non-sensitive parameters using configurations or labels.

 Note

These parameters may be returned in clear text (for example, in GET requests).

14.8 Output Encoding

To avoid breaking the business functionality, SAP AI Core does not sanitize any user input. Consumers or applications that consume the AI API are expected to perform necessary output encoding based on the usage context to prevent XSS attacks.

Related Information

Cross Site Scripting (XSS) - OWASP Site

Cross Site Scripting Prevention Cheat Sheet - OWASP Site

14.9 Multitenancy

SAP AI Core is a tenant-aware BTP reuse service, supporting main tenants and resource groups. Resources are defined for tenants or resource groups as outlined below:

<!-- missing-text -->

Each main tenant and resource group is mapped to a namespace. The main tenant namespace only contains templates for workflows and model serving. The instances of these objects are created in the respective resource group namespaces and reference the corresponding templates in the main tenant namespace. Each main tenant has a default resource group, which can be used for workloads from the main tenant.

Tenant-Level Resources

Tenant-level resources include executables such as:

 · Workflow templates

 · Serving templates

 · Docker registries that contain Docker images

 · User authentication and authorization (UAA)

User authentication and authorization are based on the SAP AI Core tenant (access token obtained using the service key for SAP AI Core). At runtime or when managing the lifecycle via AI API, the SAP AI Core tenant must set the appropriate resource group in the request header.

Resource-Group-Level Resources

Executables at tenant level are shared across all resource groups. In contrast, runtime entities such as executions, deployments, configurations, and artifacts belong to a specific resource group and cannot be shared across resource groups. Similarly, generic secrets created within a resource group and be used only for workloads within that group.

You can register an object store at resource-group level by setting the resource group header. We recommend that you do not use the same object store bucket with the same IAM user for multiple resource groups.

Tenant Isolation of Workloads

Workloads run in a sandbox environment and cannot access workflows of other tenants or resource groups. Only TCP is supported for inbound or outbound traffic from a workload. Opening workloads to open Sockets on UDP ports is strongly discouraged. They are not usable, but may pose a theoretical security problem for the workload.

14.10  Auditing and Logging Information

Here you can find a list of the security events that are logged by SAP AI Core.

Security Events Written in Audit Logs

<!-- missing-text -->

What Events Are Logged

How to Identify Related Log Events

<!-- missing-text -->

<!-- missing-text -->

The following information is described in the table columns:

 · Event grouping - Events that are logged with a similar format or are related to the same entities.

 · What events are logged - Description of the security or data protection and privacy related event that is logged.

 · How to identify related log events - Search criteria or key words, that are specific for a log event that is created along with the logged event.

 · Additional information - Any related information that can be helpful.

Related Information

Audit Logging in the Cloud Foundry Environment Audit Logging in the Neo Environment

14.11  Data Protection and Privacy

For general information about data protection and privacy on SAP Business Technology Platform, see Data Protection and Privacy.

Data protection is associated with numerous legal requirements and privacy concerns. In addition to compliance with general data protection and privacy acts, it is necessary to consider compliance with industryspecific legislation in different countries/regions. This section describes the specific features and functions that SAP AI Core provides to support compliance with the relevant legal requirements and data privacy.

This guide does not give advice on whether these features and functions are the best method to support company, industry, regional, or country/region-specific requirements. Furthermore, this guide does not give advice or recommendations about additional features that would be required in a particular environment. Decisions related to data protection must be made on a case-by-case basis and under consideration of the given system landscape and the applicable legal requirements.

 Note

SAP does not provide legal advice in any form. SAP software supports data protection compliance by providing security features and specific data protection-relevant functions, such as simplified blocking and deletion of personal data. In many cases, compliance with applicable data protection and privacy laws will not be covered by a product feature. Definitions and other terms used in this document are not taken from a particular legal source.

The extent to which data protection is ensured depends on secure system operation. Network security, security note implementation, adequate logging of system changes, and appropriate usage of the system are the basic technical requirements for compliance with data privacy legislation and other legislation.

For a glossary of Data Protection and Privacy terms in SAP BTP, see the SAP BTP Glossary for Data Protection and Privacy.

14.11.1  Data Storage and Processing

SAP AI Core provides functionality that allows you to process data, such as configuration files or Machine Learning (ML) Training or ML Serving.

SAP AI Core acts as the data processor and is not aware of the type of data or category of data. SAP AI Core customers, as Data Controllers, are responsible for fulfilling data protection and privacy (DPP) responsibilities for data storage and processing requirements.

14.11.2  Change Logging and Read-Access Logging

SAP AI Core acts as the data processor and is not aware of the type of data or category of data. SAP AI Core customers, as Data Controllers, are responsible for fulfilling data protection and privacy (DPP) responsibilities for data storage and processing requirements.

For any applications or services you develop using SAP AI Core, you must ensure that they include relevant logging functions, and ensure compliance with the data privacy laws by making sure that the data is properly logged.

14.11.3  Consent

SAP AI Core acts as the data processor and is not aware of the type of data or category of data. SAP AI Core customers, as Data Controllers, are responsible for fulfilling asking for consent from data subjects before collecting any personal data.

14.11.4  Deletion

SAP AI Core supports Bring your object store , whereby customers register the object store secret where artifact relevant files (such as training data or machine learning models or other types) are stored. Such data is used by the ML workloads during processing, such as ML Training or ML Serving.

The notion of artifacts is limited to capture the metadata of the data. Data is physically stored in the object store and SAP AI Core is not responsible for deletion of files. When artifacts are deleted using AI API, corresponding metadata are deleted from SAP AI Core service and the actual files are not deleted from the object store. For more information about the AI API, see AI API Overview [page 36].

Upon offboarding, SAP AI Core will clean up the cached data within AI Core used for processing purposes.

SAP AI Core customers, as Data Controllers, are responsible for deletion of data from the registered object store.

14.11.5  Security and Customer Data Protection

SAP product standard security and the data protection and privacy (DPP) requirements set high standards and obligations when it comes to securing and protecting customer data that is entrusted to SAP.

Customer data protection is handled in three ways:

 · Customer data is imported, output, and processed by the services for no purpose beyond that to which the customer has subscribed.

 · Customer data is protected from malicious access by security technologies that include authentication and authorization.

 · Customer data is protected from accidental exposure to SAP administrators or support persons by security policies, access controls, and monitoring.

15 Accessibility Features in SAP AI Core

To optimize your experience of SAP AI Core, SAP AI Core provides features and settings that help you use the software efficiently.

 Note

SAP AI Core uses SAP AI Launchpad for its interface, which is based on SAPUI5. For this reason, accessibility features for SAPUI5 also apply. See the accessibility documentation for SAPUI5 on SAP Help Portal at Accessibility for End Users.

For more information on-screen reader support and keyboard shortcuts, see Screen-Reader Support for SAPUI5 Controls and Keyboard Handling for SAPUI5 Elements.

16 Monitoring and Troubleshooting

Explore solutions to potential issues, and find out how to get support.

Getting Support

If you encounter an issue with this service, we recommend that you follow the procedure below.

Check Platform Status

<!-- missing-text -->

Check the availability of the platform at SAP Trust Center

.

For more information about platform availability, updates and notifications, see Platform Updates and Notifications in the Cloud Foundry Environment.

Check Guided Answers

In the SAP Support Portal, check the Guided Answers section for SAP Cloud Platform. You can find solutions for general SAP Cloud Platform issues as well as for specific services there.

Contact SAP Support

You can report an incident or error through the SAP Support Portal .

Please use the following component(s) for your incident:

<!-- missing-text -->

We recommend that you include the following information when you submit the incident:

 · Region information (Canary, EU10, US10)

 · Subaccount technical name

 · URL of the page where the incident or error occurs

 · Steps or clicks used to replicate the error

 · Screenshots, videos, or the code entered

16.1 Troubleshooting

For troubleshooting information, see the following sections:

Repository [page 346]

Configuration [page 347]

```
Artifacts [page 349] Application [page 351] Execution [page 356] Docker [page 358] Deployment [page 359] Miscellaneous [page 360]
```

16.1.1  Repository

Repository ra-aicore-test not found for tenant

You get the result:

```
{ "error":{ "code": "500", "details": [ { "code": null, "message": " "Repository ra-aicore-test not found for tenant b82a8318" } ], "message": "Repository ra-aicore-test not found for tenant b82a8318" "request_id": null, "target": "/api/v4alpha/repositories" } }
```

and:

```
AIAPIServerException: Failed to post /admin/repositories: Repository ra-aicoretest not found for tenant 68
```

Follow the solution:

Use a different name for the value of the name parameter. The exception is raised by reuse of the name aicore-test .

```
response = ai_api_client.rest_clinet.post( path="/admin/repositories", body={ "name": "aicore-test-1", "url": "https://github.com/John/aicore-test", } ) print(response) 'message': 'Repository has been on-boarded.' {
```

Parent topic: Troubleshooting [page 345]

Related Information

Configuration [page 347]

```
Artifacts [page 349] Application [page 351] Execution [page 356] Docker [page 358] Deployment [page 359] Miscellaneous [page 360]
```

16.1.2  Configuration

Could not create configuration, because executable &lt;x&gt; for scenario &lt;y&gt; is not found

When you try to create a configuration, you are told that an executable cannot be found for your scenario.

Check the following:

 1. Check that you are using the name value from your workflow for the executable ID in the configuration.

```
scenarios.ai.sap.com/description: "SAP developers tutorial
```

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: text-clf-train-tutorial annotations: scenario" .. .
```

 Note

Do not use the value from executables.ai.sap.com/id as an executable ID.

 2. Check that you are using the value of executables.ai.sap.com/id from your workflows as your scenario ID.

```
... artifacts.ai.sap.com/text-data.kind: "dataset" artifacts.ai.sap.com/text-model-tutorial.kind: "model" labels: scenarios.ai.sap.com/id: "text-clf-tutorial" ai.sap.com/version: "2.1.0" spec:
```

```
.. .
```

Log message: using minio client

Check the following:

 1. Check that you are using the name value from your workflow for the executable ID in the configuration.

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: text-clf-train-tutorial annotations: scenarios.ai.sap.com/description: "SAP developers tutorial scenario" .. .
```

 Note

Do not use the value from executables.ai.sap.com/id as an executable ID.

 2. Check that you are using the value of executables.ai.sap.com/id from your workflows as your scenario ID.

```
... artifacts.ai.sap.com/text-data.kind: "dataset" artifacts.ai.sap.com/text-model-tutorial.kind: "model" labels: scenarios.ai.sap.com/id: "text-clf-tutorial" ai.sap.com/version: "2.1.0" spec: .. .
```

Parent topic: Troubleshooting [page 345]

Related Information

```
Repository [page 346] Artifacts [page 349] Application [page 351] Execution [page 356] Docker [page 358] Deployment [page 359] Miscellaneous [page 360]
```

16.1.3  Artifacts

No output artifact has been generated

Complete the following:

Define the globalName parameter for the output artifact in your workflow:

```
... executables.ai.sap.com/description: "Text classification Scikit training executable" executables.ai.sap.com/name: "text-clf-train-tutorial-exec" artifacts.ai.sap.com/text-data.kind: "dataset" artifacts.ai.sap.com/text-model-tutorial.kind: "mind" abels: l scenarios.ai.sap.com/id: "text-clf-tutorial" ai.sap.com/version: "2.1.0" ... ... outputs: artifacts: -name: text-model-tutorial path: /app/model global name: text-model-tutorial archive: none: {} container: ...
```

Failed to load artifacts: The specified key does not exist

Complete the following:

 1. Ensure you have created an object store secret using the naming convention &lt;name&gt; and the pathPrefix from your AWS S3 path. Refer to the following diagram:

<!-- missing-text -->

 2. When creating the artifact, don't add the trailing forward slash (/) in URL parameter:

 · Incorrect usage: "url": "ai://yourObjectStoreSecretName/folder/subfolder/"

 · Correct usage: "url": "ai://yourObjectStoreSecretName/folder/subfolder"

Parent topic: Troubleshooting [page 345]

Related Information

Repository [page 346]

Configuration [page 347]

Application [page 351]

Execution [page 356]

Docker [page 358]

Deployment [page 359]

Miscellaneous [page 360]

16.1.4  Application

Templates are not synced via applications

After you have deleted or created an application, templates are not synced.

Follow the solution:

 1. Delete the synced application using the endpoint:

 DELETE {{apiurl}}/v2/admin/applications/{{appName}}

 2. Offboard the associated GitHub repository using the endpoint:

 DELETE {{apiurl}}/v2/admin/repositories/{{repositoryName}}

 3. Onboard the associated GitHub repository using a personal access token instead of your GitHub password. For more information, see Creating a personal access token .

```
POST {{apiurl}}/v2/admin/repositories
```

```
 Sample Code Body: { name": "aicore-test", " "url": "https://github.com/john/aicore-test", "username": "john", "password": "yourGitHubPersonalAccessToken" }
```

 4. Create the application using the endpoint:

```
POST {{apiurl}}/v2/admin/applications
```

 5. Check the ArgoCD applications to determine if the repository has been synchronized correctly for the tenant. For example, check that there are no duplicated workflow names. The value of the name parameter is considered as an executable ID.

```
name: text-clf-train-tutorial annotations: ...apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata:
```

 6. Check that you're calling SAP AI Core using the expected tenant.

 7. Check if the workflow templates contain the correct scenario label.

 8. Get your application sync status using the endpoint:

 GET {{apiurl}}/v2/admin/applications/{{appName}}/status

The status will any return errors in your templates. When your templates are updated, the application will resync automatically after approximately three minutes.

Executables do not appear when you retrieve them from a scenario

Follow the solution:

 1. Delete the synced application using the endpoint: DELETE {{apiurl}}/v2/admin/repositories/{{appName}} 2. Offboard the associated GitHub repository using the endpoint: DELETE {{apiurl}}/v2/admin/repositories/{{repositoryName}} 3. Onboard the associated GitHub repository using a personal access token instead of your GitHub password. For more information, see Creating a personal access token . POST {{apiurl}}/v2/admin/repositories  Sample Code Body: apiVersion: argoproj.io/v1alpha1{ name": "aicore-test", " "url": "https://github.com/john/aicore-test", "username": "john", "password": "yourGitHubPersonalAccessToken" } 4. Create the application using the endpoint: POST {{apiurl}}/v2/admin/applications 5. Check the ArgoCD applications to determine if the repository has been synchronized correctly for the tenant. For example, check that there are no duplicated workflow names. The value of the name parameter is considered as an executable ID.

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: text-clf-train-tutorial annotations: ...
```

 6. Check if the user is calling SAP AI Core using the expected tenant.

 7. Check that the scenario label is correct in the workflow templates.

 8. Get your application sync status using the endpoint:

 GET {{apiurl}}/v2/admin/applications/{{appName}}/status

 The status will any return errors in your templates. When your templates are updated, the application will resync automatically after approximately three minutes.

Application status returns healthy , but most other properties are unknown

Follow the solution:

```
1. Delete the synced application using the endpoint: DELETE {{apiurl}}/v2/admin/applications/{{appName}} 2. Offboard the associated GitHub repository using the endpoint: DELETE {{apiurl}}/v2/admin/repositories/{{repositoryName}}
```

 3. Onboard the associated GitHub repository using a personal access token instead of your GitHub password. For more information, see Creating a personal access token .

```
POST {{apiurl}}/v2/admin/repositories
```

```
 Sample Code Body: { name": "aicore-test", " "url": "https://github.com/john/aicore-test", "username": "john", "password": "yourGitHubPersonalAccessTokenHere" }
```

 4. Create the application using the endpoint:

POST

{{apiurl}}/v2/admin/applications

 5. Check the ArgoCD applications to determine if the repository has been synchronized correctly for the tenant. For example, check that there are no duplicated workflow names. The value of the name parameter is considered as an executable ID.

```
apiVersion: argoproj.io/v1alpha1 ind: WorkflowTemplate k metadata: name: text-clf-train-tutorial annotations: ...
```

 6. Check if the user is calling SAP AI Core using the expected tenant.

 7. Check if the workflow templates contain the correct scenario label.

 8. Get your application sync status using the endpoint:

GET {{apiurl}}/v2/admin/applications/{{appName}}/status The status will any return errors in your templates. When your templates are updated, the application will

resync automatically after approximately three minutes.

Application status message: rpc error: code = Unknown desc = my-path: app path

```
does not exist
```

The specified path in your application doesn't exist in your repository.

Follow the solution:

Delete your application and create a new one using the correct path.

Application status message: application repo &lt;your git repository&gt; is not

permitted in project 'xyz'

The repository URL cannot be found in your onboarded repositories.

Check the following:

Make sure that the repository specified in your application has been successfully onboarded by using GET {{apiurl}}/v2/admin/applications and checking that the repository url has "status": "COMPLETED"

Application status message: rpc error: code = Unknown desc = Unable to resolve

'not-existing-branch' to a commit SHA

The revision you specified in your application doesn't exist in your repository.

Follow the solution:

Delete your application and create a new one using the correct revision. The revision number is found in GitHub

<!-- missing-text -->

Alternatively, enter HEAD to refer to the latest commit.

Application status message: rpc error: code = FailedPrecondition desc = Failed

to unmarshal worklflow.yaml\": failed to unmarshal manifest: error converting YAML to JSON: yaml: line 7: mapping values are not allowed in this context

You have a syntax error in your workflow template.

Follow the solution:

Use Argo Lint to identify syntax errors in your workflow template. To set up the Argo Lint IDE, refer to Argo Lint IDE setup

Application status message: spec.source.repoURL and spec.source.path either

spec.source.chart are required

You specified an empty path in your application.

```
{ "healthStatus": "Unknown",
```

```
"message": "spec.source.repoURL and spec.source.path either
```

```
spec.source.chart are required", "reconciledAt": "Unknown", "source": { "path": "Unknown", "repoURL": "Unknown", "revision": "Unknown" }, "syncFinishedAt": "Unknown", "syncRessourcesStatus": [], "syncStartedAt": "Unknown", "syncStatus": "Unknown" }
```

Follow the solution:

Delete your application and create a new one, specifying a path. Check its status by using the endpoint:

{{apiurl}}/v2/admin/applications/{{appName}}/status

Sync an Application Manually

Applications sync with your GitHub repository automatically at intervals of ~3 minutes. Use the endpoint below to manually request a sync: {{apiurl}}/admin/applications/{{appName}}/refresh

Parent topic: Troubleshooting [page 345]

Related Information

Repository [page 346]

Configuration [page 347]

Artifacts [page 349]

Execution [page 356]

Docker [page 358]

Deployment [page 359]

Miscellaneous [page 360]

16.1.5  Execution

Execution status is DEAD or PENDING for a long time

Check the following:

 1. Check the execution logs.

 2. Check that the parameter name and execution name that you provided match those in your template. Note that the names are case-sensitive.

Images from private docker.io cannot be pulled for execution

Follow the solution:

 1. Specify the domain name of your Docker Hub. For example:

```
... none: {} container: image: "docker.io/tutorialrepo/movie-review-clfserve@sha256:123f4qwertyiop13432" imagePullPolicy: Always .. .
```

If you do not already know your domain name, refer to the image:

<!-- missing-text -->

<!-- missing-text -->

 2. Specify your Docker Image digest instead of the image version in your templates: image tutorialrepo/movie-review-clf-serve@sha256:123f4qwertyiop13432

Output artifacts in the execution list are empty and have status UNKNOWN for a long time

Check the following:

 1. Check that an object secret with the name default exists in the current resource group. For more information, see Register an Object Store Secret [page 87]

 2. Check that you have created the Docker Registry secrets required to pull your Docker Images. Logs will be available only after the execution has started.

GET execution has the status UNKNOWN for a long time

Check the following:

 1. Check if an object secret with the name default exists in the current resource group. For more information, see Register an Object Store Secret [page 87]

 2. Check that you have created the Docker Registry secrets required to pull your Docker Images. Logs will be available only after the execution has started.

Execution status changes to DEAD without any log

Check the following:

 1. Check that your templates meet the Argo specifications and that they can be executed by SAP AI Core.

 2. To identify errors automatically in your templates, use the Argo linter.

 3. Check that you have created Docker folders, to store artifacts that will be created during an execution.

Parent topic: Troubleshooting [page 345]

Related Information

Repository [page 346] Configuration [page 347] Artifacts [page 349] Application [page 351] Docker [page 358] Deployment [page 359] Miscellaneous [page 360]

16.1.6  Docker

Error pulling container main logs. Back-off pulling image

You push a template to SAP AI Core and you get the error "Error pulling container main logs". "Back-off pulling image" .

Complete the following steps:

 1. Make sure that your Docker Registry is public-facing and not protected behind the firewall of your organization.

 If your Docker Image isn't public, verify that you have created a Docker Registry secret in SAP AI Core.

 2. Check that you can pull your Docker Image on your local computer using the credentials from the previous step.

 3. Specify the same Docker Registry secret in your executable.

 4. Specify the path to your Docker Image in your executable in following format:

 &lt;DOCKER_REGISTRY&gt;/&lt;REPO_NAME&gt;/&lt;DOCKER_IMAGE&gt;:&lt;TAGNAME&gt;

```
 Example docker.io/tutorialrepo/text-clf-train:0.0.1
```

Parent topic: Troubleshooting [page 345]

Related Information

Repository [page 346]

Configuration [page 347]

Artifacts [page 349]

Application [page 351]

Execution [page 356]

Deployment [page 359]

Miscellaneous [page 360]

16.1.7  Deployment

You want to force a deployment with status UNKNOWN to stop

You want to stop and delete a deployment but you are unable to because the deployment status is 'Unknown'. You have tried to submit a PATCH request as follows:

```
PATCH {{apiurl}}/lm/deployments/d4fec9c24c54f87e
```

However, you receive the following response:

```
{ "error": { "code": "01010076" "message": "Invalid Request, Current status UNKNOWN cannot be changed..", "requestID": "e110820e-1cfe-456a-bb0e-77907b36422c", "target": "/apu/v2/deployments/d4fec9c24c54f87e" }
```

Complete the following steps:

 1. Find out why your deployment status is 'Unknown' by using the endpoint:

 GET {{apiurl}}/v2/lm/deployments/{{deploymentid}}

 2. Delete the deployment without trying to stop it (stopping a deployment is necessary only when it's running):

```
DELETE {{apiurl}}/v2/lm/deployments/{{deploymentid}}
```

Deployment remains in status PENDING

Check the following:

 1. Check that the Docker Registry secret exists when using your private Docker Image.

 2. Check that your Docker Image can be downloaded to your local system.

Deployment ID &lt;abc&gt; not found

This message appears when you have just started the deployment. Wait a few minutes and the message will resolve itself automatically.

Parent topic: Troubleshooting [page 345]

Related Information

Repository [page 346]

Configuration [page 347] Artifacts [page 349] Application [page 351] Execution [page 356] Docker [page 358] Miscellaneous [page 360]

16.1.8  Miscellaneous

403 - forbidden: RBAC Access denied

When you submit a POST request for an execution or a configuration, you get the error 403 - forbidden: RBAC Access denied .

Check the following:

 1. Check that you are passing the correct token and ai-resource-group header .

 2. Check your tenant provisioning.

You get a runtime adapter exception

You get the error: "Runtime Adapter Exception; Failed to post deployments :  \n{code\": \"400\",\n message\": Missing input parameter or artifacts, one or more placeholder values are not resolved in the serving spec and error is 'dict object' has no attribute ''.\"\n}\n"

Check the following:

Check that your input artifact key doesn't contain any separators, such as 'example-artifact-key'. If it does, rename the key (for example, 'exampleArtifactKey')

You have pushed your template to your GitHub repository but you can't see the executable created via the API

There may be an error in your template. Check the following:

 · Serving templates always expect an input parameter to be configured. If you don't have a parameter in your Serving template, add a dummy parameter and create a configuration for it.

 · Hyphens (-) are the only separator as permitted in the template name.

You have created a scenario in a template but you cannot see it in the AI API calls

Use the following solution:

Add a workflow template with the new scenario_id (not a serving template) to make the scenario visible.

Error: getaddrinfo ENOTFOUND

Check the following:

 1. Check that all environment variables match your SAP AI Core service keys. Specifically, check:

 · auth_url

 · client_id

 · client_secret

 · apiurl

 2. Submit a GET request to the endpoint {{apiurl}}/v2/admin/repositories

 3. If the issue persists, contact SAP support as described at Monitoring and Troubleshooting [page 345].

Git repo doesn't synchronize with SAP AI Core instances.

When you try to synchronize your git repository with SAP AI Core, the response shows empty fields.

Check the following:

 1. Check that you are using a GitHub personal access token and not your GitHub password.

If you are already using a personal access token, proceed as follows:

 1. Delete all UNKNOWN applications from your SAP AI Core instance using the endpoint: DELETE {{apiurl}}/v2/admin/applications/{{appName}}

 2. Offboard your GitHub repo from SAP AI Core by calling the endpoint:

DELETE

{{apiurl}}/v2/admin/repositories/{{repositoryName}}

 3. Generate a GitHub personal access token and use it to onboard your git repo to SAP AI Core as before. For more information, see Creating a personal access token .

 4. Sync your application again.

Parent topic:

Troubleshooting [page 345]

Related Information

Repository [page 346] Configuration [page 347] Artifacts [page 349] Application [page 351] Execution [page 356] Docker [page 358] Deployment [page 359]

17 Service Offboarding

Tenant offboarding occurs when a customer deletes a subaccount. SAP AI Core polls for the subaccount deletion event and performs the necessary deprovisioning and deletion activities.

 Note

Data and resources are not deleted when a service instance is deleted (because we don't isolate based on the service instance). If you want to keep the subaccount but still deprovision SAP AI Core, create a medium support ticket on component CA-ML-AIC with the title Service Offboarding and request that your data and resources be deleted manually.

Important Disclaimers and Legal Information

Hyperlinks

Some links are classified by an icon and/or a mouseover text. These links provide additional information. About the icons:

 · Links with the icon : Y ou are entering a Web site that is not hosted by SAP . By using such links, you agree (unless expressly stated otherwise in your agreements with SAP) to this:

 · The content of the linked-to site is not SAP documentation. You may not infer any product claims against SAP based on this information.

 · SAP does not agree or disagree with the content on the linked-to site, nor does SAP warrant the availability and correctness. SAP shall not be liable for any damages caused by the use of such content unless damages have been caused by SAP's gross negligence or willful misconduct.

 · Links with the icon : Y ou are leaving the documentation for that particular SAP product or service and are entering an SAP-hosted Web site. By using such links, you agree that (unless expressly stated otherwise in your agreements with SAP) you may not infer any product claims against SAP based on this information.

Videos Hosted on External Platforms

Some videos may point to third-party video hosting platforms. SAP cannot guarantee the future availability of videos stored on these platforms. Furthermore, any advertisements or other content hosted on these platforms (for example, suggested videos or by navigating to other videos hosted on the same site), are not within the control or responsibility of SAP .

Beta and Other Experimental Features

Experimental features are not part of the officially delivered scope that SAP guarantees for future releases. This means that experimental features may be changed by SAP at any time for any reason without notice. Experimental features are not for productive use. You may not demonstrate, test, examine, evaluate or otherwise use the experimental features in a live operating environment or with data that has not been sufficiently backed up.

The purpose of experimental features is to get feedback early on, allowing customers and partners to influence the future product accordingly. By providing your feedback (e.g. in the SAP Community), you accept that intellectual property rights of the contributions or derivative works shall remain the exclusive property of SAP .

Example Code

Any software coding and/or code snippets are examples. They are not for productive use. The example code is only intended to better explain and visualize the syntax and phrasing rules. SAP does not warrant the correctness and completeness of the example code. SAP shall not be liable for errors or damages caused by the use of example code unless damages have been caused by SAP's gross negligence or willful misconduct.

Bias-Free Language

SAP supports a culture of diversity and inclusion. Whenever possible, we use unbiased language in our documentation to refer to people of all cultures, ethnicities, genders, and abilities.

www.sap.com/contactsap

© 2024 SAP SE or an SAP affiliate company. All rights reserved.

No part of this publication may be reproduced or transmitted in any form or for any purpose without the express permission of SAP SE or an SAP affiliate company. The information contained herein may be changed without prior notice.

Some software products marketed by SAP SE and its distributors contain proprietary software components of other software vendors. National product specifications may vary.

These materials are provided by SAP SE or an SAP affiliate company for informational purposes only, without representation or warranty of any kind, and SAP or its affiliated companies shall not be liable for errors or omissions with respect to the materials. The only warranties for SAP or SAP affiliate company products and services are those that are set forth in the express warranty statements accompanying such products and services, if any. Nothing herein should be construed as constituting an additional warranty.

SAP and other SAP products and services mentioned herein as well as their respective logos are trademarks or registered trademarks of SAP SE (or an SAP affiliate company) in Germany and other countries. All other product and service names mentioned are the trademarks of their respective companies.

Please see https:/ /www.sap.com/about/legal/trademark.html for additional trademark information and notices.

<!-- missing-text -->